<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Dataset Distillation & Long-Tail</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Abstract" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="XXX" />
<meta property="og:url" content="XXX" />
<meta property="og:site_name" content="CymNet" />
<script type="application/ld+json">
{"@type":"WebSite","url":"XXX","headline":"Abstract","name":"CymNet","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="assets/css/style.css">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Dataset Distillation Enhancing Long-Tail Learning: A Two-Stage Dataset Strategy </h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/bytetriper/DD-LT" class="btn">View on GitHub</a>
      
    </section>

    <section class="main-content">
      <h3 id="abstract">Abstract</h3>
<p>This study addresses the challenge of long-tailed distributions in machine learning, where few categories are over-represented, and
  many are under-represented. We focus on datasets like CIFAR10-LT, CIFAR100-LT, and ImageNet-LT, which mimic these distributions.
  Traditional methods like re-sampling and re-weighting, though helpful, have limitations such as high computational costs. We propose
  a novel approach: applying dataset distillation to long-tailed datasets. This technique condenses large datasets into smaller, synthesized
  versions, ensuring efficient training with less computational demand. Our contributions are threefold. Firstly, we investigate the
  efficacy of dataset distillation in long-tail learning scenarios, establishing a foundational benchmark by evaluating performance on the
  CIFAR10-LT dataset. This research serves as a baseline for future explorations in this domain. Secondly, we delve into a comprehensive
  analysis of dataset distillation in long-tailed learning, proposing the ’grouping hypothesis’. This hypothesis sheds light on the nuances
  of data representation and model learning in imbalanced data settings, offering a new perspective on data interpretation within deep
  neural networks. Finally, we introduce a novel two-stage Classification strategy and a corresponding two-stage hierarchical Training
  method based on dataset distillation principles. Our approach not only demonstrates an improvement in classification accuracy but
  also significantly mitigates inter-class bias, addressing the challenges posed by long-tail distributions. Importantly, this methodology
  can be seamlessly integrated as a plugin to enhance future dataset distillation strategies in similar learning environments. The insights
  and methodologies presented in this paper hold substantial potential for advancing the state-of-the-art in dataset distillation and
  long-tailed learning.
  </p>

<h3 id="author">Author</h3>
<p>　　　　Boyang Zheng　　　　Lai Jiang　　　　Quanquan Peng　　　　Xuanchang Zhang</p>

<h3 id="paper">Paper</h3>
<p>Our paper is available <a href="https://github.com/bytetriper/DD-LT/blob/main/submission.pdf">here</a>.</p>
<h3 id="principle">A quick look into our method</h3>
<p><img style="margin: 0px 40px;" src="principle.png" width="800" /></p>



      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/bytetriper/DD-LT/">DD-LT</a> is maintained by <a href="YYY">Lai Jiang</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </section>

    
  </body>
</html>
