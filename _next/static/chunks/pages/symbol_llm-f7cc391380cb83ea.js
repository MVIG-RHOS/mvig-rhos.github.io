(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[94],{9149:function(e,n,t){(window.__NEXT_P=window.__NEXT_P||[]).push(["/symbol_llm",function(){return t(2883)}])},6526:function(e,n,t){"use strict";t.r(n),n.default={src:"/_next/static/media/insight.6e3a8110.png",height:4591,width:13422,blurDataURL:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAADCAIAAAAhqtkfAAAAUUlEQVR42gUAWwqAIMz7X6SfPgLpGHaBovrJuZpbkOEDk1Cl5K+1Vivx7fDynkUkp6TW/ZiMQbDxfSw4ImaiHKMaRq37Tog8wrbM1p1oIYXwA7fTQS/mypDwAAAAAElFTkSuQmCC",blurWidth:8,blurHeight:3}},7782:function(e,n,t){"use strict";t.d(n,{jE:function(){return f},yb:function(){return m},zM:function(){return x}});var i=t(5893),s=t(4829),a=t(728),r=t(1561),o=t(4184),l=t.n(o),c=t(1664),d=t.n(c),u=t(7294),m="headerNav",f=(0,u.memo)(function(e){var n=e.navSections,t="-m-1.5 p-1.5 rounded-md font-bold first-letter:uppercase hover:transition-colors hover:duration-300 focus:outline-none focus-visible:ring-2 focus-visible:ring-orange-500 sm:hover:text-orange-500 text-neutral-100",s=l()(t,"text-orange-500"),a=l()(t,"text-neutral-100");return(0,i.jsx)("header",{className:"fixed top-0 z-50 hidden w-full bg-neutral-900/50 p-4 backdrop-blur sm:block",id:m,children:(0,i.jsxs)("nav",{className:"flex justify-center gap-x-8",children:[(0,i.jsx)(d(),{href:"/",passHref:!0,children:(0,i.jsx)("a",{className:a,children:"Home"},"home")}),n.map(function(e){return(0,i.jsx)(h,{activeClass:s,current:!1,inactiveClass:a,section:e},e)})]})})}),x=(0,u.memo)(function(e){var n=e.navSections,t=(0,u.useState)(!1),o=t[0],c=t[1],m=(0,u.useCallback)(function(){c(!o)},[o]),f="p-2 rounded-md first-letter:uppercase transition-colors duration-300 focus:outline-none focus-visible:ring-2 focus-visible:ring-orange-500",x=l()(f,"bg-neutral-900 text-white font-bold"),g=l()(f,"text-neutral-200 font-medium");return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)("button",{"aria-label":"Menu Button",className:"fixed top-2 right-2 z-40 rounded-md bg-orange-500 p-2 ring-offset-gray-800/60 hover:bg-orange-400 focus:outline-none focus:ring-0 focus-visible:ring-2 focus-visible:ring-orange-500 focus-visible:ring-offset-2 sm:hidden",onClick:m,children:[(0,i.jsx)(r.Z,{className:"h-8 w-8 text-white"}),(0,i.jsx)("span",{className:"sr-only",children:"Open sidebar"})]}),(0,i.jsx)(s.u.Root,{as:u.Fragment,show:o,children:(0,i.jsxs)(a.V,{as:"div",className:"fixed inset-0 z-40 flex sm:hidden",onClose:m,children:[(0,i.jsx)(s.u.Child,{as:u.Fragment,enter:"transition-opacity ease-linear duration-300",enterFrom:"opacity-0",enterTo:"opacity-100",leave:"transition-opacity ease-linear duration-300",leaveFrom:"opacity-100",leaveTo:"opacity-0",children:(0,i.jsx)(a.V.Overlay,{className:"fixed inset-0 bg-stone-900 bg-opacity-75"})}),(0,i.jsx)(s.u.Child,{as:u.Fragment,enter:"transition ease-in-out duration-300 transform",enterFrom:"-translate-x-full",enterTo:"translate-x-0",leave:"transition ease-in-out duration-300 transform",leaveFrom:"translate-x-0",leaveTo:"-translate-x-full",children:(0,i.jsx)("div",{className:"relative w-4/5 bg-stone-800",children:(0,i.jsxs)("nav",{className:"mt-5 flex flex-col gap-y-2 px-2",children:[(0,i.jsx)(d(),{href:"/",passHref:!0,children:(0,i.jsx)("a",{className:g,onClick:m,children:"Home"},"home")}),n.map(function(e){return(0,i.jsx)(h,{activeClass:x,current:!1,inactiveClass:g,onClick:m,section:e},e)})]})})})]})})]})}),h=(0,u.memo)(function(e){var n=e.section,t=e.current,s=e.inactiveClass,a=e.activeClass,r=e.onClick;return(0,i.jsx)(d(),{href:"#".concat(n),passHref:!0,children:(0,i.jsx)("a",{className:l()(t?a:s),onClick:r,children:n},n)})})},2535:function(e,n,t){"use strict";t.d(n,{e:function(){return a}});var i=t(7294),s=t(7782),a=function(e,n){(0,i.useEffect)(function(){var t=document.querySelectorAll(e),i=Array.from(t),a=document.getElementById(s.yb),r=new IntersectionObserver(function(e){e.forEach(function(e){var t=e.boundingClientRect.y,s=e.target.getAttribute("id");if(a){var r,o={id:s,currentIndex:i.findIndex(function(e){return e.getAttribute("id")===s}),isIntersecting:e.isIntersecting,currentRatio:e.intersectionRatio,aboveToc:t<a.getBoundingClientRect().y,belowToc:!(t<a.getBoundingClientRect().y)};o.isIntersecting?n(o.id):!o.isIntersecting&&o.currentRatio<1&&o.currentRatio>0&&o.belowToc&&n(null===(r=i[o.currentIndex-1])||void 0===r?void 0:r.getAttribute("id"))}})},{root:null,threshold:.1,rootMargin:"0px 0px -70% 0px"});return t.forEach(function(e){r.observe(e)}),function(){r.disconnect()}},[])}},2883:function(e,n,t){"use strict";t.r(n),t.d(n,{HeaderSectionIdList:function(){return h},SectionId:function(){return x}});var i=t(603),s=t(5893),a=t(7294),r=t(9770),o=t(6571),l=t(3031),c=t(11),d=t(2535),u=t(7782),m=t(5675),f=t.n(m),x={About:"about",Demo:"demo",News:"news",Download:"download",Disclaimer:"disclaimer",Publications:"publications"},h=[x.About,x.News,x.Download,x.Publications,],g=[["2023.11",(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)("a",{className:"underline text-sky-600",href:"https://github.com/enlighten0707/Symbol-LLM",children:"Code"})," Released."]}),],["2023.10",(0,s.jsx)(s.Fragment,{children:"Symbol-LLM will appear at NeurIPS 2023."}),],],v=(0,a.memo)(function(){return(0,s.jsxs)(r.Z,{description:"SymbolLLM",title:"SymbolLLM",children:[(0,s.jsx)(b,{}),(0,s.jsx)("div",{className:"relative flex h-screen-no w-screen items-center justify-center bg-neutral-100",children:(0,s.jsxs)("div",{className:"flex flex-col z-10 w-full max-w-screen-lg p-4 lg:px-0 items-center text-center ",children:[(0,s.jsx)("div",{className:"h-20"}),(0,s.jsx)("h1",{className:"text-3xl font-bold text-gray-800 sm:text-4xl lg:text-5xl p-4",children:"Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human Activity Reasoning"}),(0,s.jsx)("p",{className:"text-gray-600 text-2xl",children:"MVIG-RHOS, SJTU"})]})}),(0,s.jsx)(l.Z,{className:"bg-neutral-100",sectionId:x.About,children:(0,s.jsxs)("div",{className:"flex flex-col",children:[(0,s.jsx)("div",{className:"grid justify-items-center pb-8",children:(0,s.jsx)("div",{className:"w-3/4",children:(0,s.jsx)(f(),{alt:"demo",src:t(6526),className:"place-self-center"})})}),(0,s.jsx)("div",{children:"Human reasoning can be understood as a cooperation between the intuitive, associative ``System-1'' and the deliberative, logical ``System-2''. For existing System-1-like methods in visual activity understanding, it is crucial to integrate System-2 processing to improve explainability, generalization, and data efficiency. One possible path of activity reasoning is building a symbolic system composed of symbols and rules, where one rule connects multiple symbols, implying human knowledge and reasoning abilities. Previous methods have made progress, but are defective with limited symbols from handcraft and limited rules from visual-based annotations, failing to cover the complex patterns of activities and lacking compositional generalization. To overcome the defects, we propose a new symbolic system with two ideal important properties: broad-coverage symbols and rational rules. Collecting massive human knowledge via manual annotations is expensive to instantiate this symbolic system. Instead, we leverage the recent advancement of LLMs (Large Language Models) as an approximation of the two ideal properties, i.e., Symbols from Large Language Models (Symbol-LLM). Then, given an image, visual contents from the images are extracted and checked as symbols and activity semantics are reasoned out based on rules via fuzzy logic calculation. Our method shows superiority in extensive activity understanding tasks."})]})}),(0,s.jsx)(l.Z,{className:"bg-neutral-100",sectionId:x.News,children:(0,s.jsx)(c.Z,{title:"News and Olds",children:(0,s.jsx)("div",{className:"flex flex-col",children:g.map(function(e,n){var t=(0,i.Z)(e,2),a=t[0],r=t[1];return(0,s.jsxs)("div",{className:"pb-2",children:[(0,s.jsxs)("span",{className:"flex-1 font-bold sm:flex-none",children:["[",a,"] "]}),(0,s.jsx)("span",{className:"flex-1 sm:flex-none",children:r})]},"".concat(a,"-").concat(n))})})})}),(0,s.jsx)(l.Z,{className:"bg-neutral-100",sectionId:x.Publications,children:(0,s.jsx)(c.Z,{title:"Publications",children:(0,s.jsxs)("div",{className:"flex flex-col divide-y-2",children:[(0,s.jsx)("div",{children:"If you find our paper, data or code usefull, please cite:"}),(0,s.jsx)("div",{className:"text-sm bg-neutral-300 p-2",children:(0,s.jsx)("pre",{children:(0,s.jsx)("code",{children:"@inproceedings{wu2023symbol,\n  title={Symbol-LLM: Leverage Language Models for Symbolic System in Visual \n  Human Activity Reasoning},\n  author={Wu, Xiaoqian and Li, Yong-Lu and Sun, Jianhua and Lu, Cewu},\n  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},\n  year={2023}\n}"})})})]})})}),(0,s.jsx)(l.Z,{className:"bg-neutral-100",sectionId:x.Disclaimer,children:(0,s.jsx)(c.Z,{title:"Disclaimer",children:(0,s.jsx)("div",{className:"flex flex-col divide-y-4",children:(0,s.jsxs)("p",{children:[(0,s.jsx)("a",{rel:"license",href:"http://creativecommons.org/licenses/by-nc/4.0/",children:(0,s.jsx)("img",{alt:"Creative Commons License",style:{borderWidth:0},src:"https://i.creativecommons.org/l/by-nc/4.0/88x31.png"})}),(0,s.jsx)("br",{}),"This work is licensed under a ",(0,s.jsx)("a",{className:"text-sky-600",rel:"license",href:"http://creativecommons.org/licenses/by-nc/4.0/",children:"Creative Commons Attribution-NonCommercial 4.0 International License"}),".",(0,s.jsx)("br",{})]})})})}),(0,s.jsx)(o.Z,{})]})}),b=(0,a.memo)(function(){var e=(0,a.useState)(null),n=e[0],t=e[1],i=(0,a.useMemo)(function(){return h},[]),r=(0,a.useCallback)(function(e){e&&t(e)},[]);return(0,d.e)(i.map(function(e){return"#".concat(e)}).join(","),r),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(u.zM,{currentSection:n,navSections:i}),(0,s.jsx)(u.jE,{currentSection:n,navSections:i})]})});n.default=v}},function(e){e.O(0,[526,342,715,345,774,888,179],function(){return e(e.s=9149)}),_N_E=e.O()}]);