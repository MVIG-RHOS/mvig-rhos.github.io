"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[345],{1469:function(t,i,e){var n=e(1438),a=e(2951),o=e(8029),r=e(6567),s=e(5893),h=e(7294),c=e(2719),u=function(t){(0,o.Z)(e,t);var i=(0,r.Z)(e);function e(){return(0,n.Z)(this,e),i.apply(this,arguments)}return(0,a.Z)(e,[{key:"render",value:function(){return this.props.href?(0,s.jsx)(c.Z,{"aria-label":"Star buttons/github-buttons on GitHub","data-show-count":"true",href:this.props.href,children:"Star"}):null}}]),e}(h.Component);i.Z=u},9770:function(t,i,e){var n=e(5893),a=e(9008),o=e.n(a),r=e(1163),s=(0,e(7294).memo)(function(t){var i=t.children,e=t.title,a=t.description,s=(0,r.useRouter)().asPath;return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(o(),{children:[(0,n.jsx)("title",{children:e}),(0,n.jsx)("meta",{content:a,name:"description"}),(0,n.jsx)("link",{href:"https://http://mvig-rhos.com".concat(s),rel:"canonical"},"canonical"),(0,n.jsx)("link",{href:"/favicon.ico",rel:"icon",sizes:"any"}),(0,n.jsx)("link",{href:"/icon.svg",rel:"icon",type:"image/svg+xml"}),(0,n.jsx)("link",{href:"/apple-touch-icon.png",rel:"apple-touch-icon"}),(0,n.jsx)("link",{href:"/site.webmanifest",rel:"manifest"}),(0,n.jsx)("meta",{content:e,property:"og:title"}),(0,n.jsx)("meta",{content:a,property:"og:description"}),(0,n.jsx)("meta",{content:"https://http://mvig-rhos.com".concat(s),property:"og:url"}),(0,n.jsx)("meta",{content:e,name:"twitter:title"}),(0,n.jsx)("meta",{content:a,name:"twitter:description"})]}),i]})});s.displayName="Page",i.Z=s},3031:function(t,i,e){var n=e(5893),a=e(4184),o=e.n(a),r=(0,e(7294).memo)(function(t){var i=t.children,e=t.sectionId,a=t.noPadding,r=void 0!==a&&a,s=t.className;return(0,n.jsx)("section",{className:o()(s,{"px-4 py-8 md:py-12 lg:px-8":!r}),id:e,children:(0,n.jsx)("div",{className:o()({"mx-auto max-w-screen-lg":!r}),children:i})})});r.displayName="Section",i.Z=r},11:function(t,i,e){var n=e(5893),a=(0,e(7294).memo)(function(t){var i=t.title,e=t.comment,a=t.children;return(0,n.jsxs)("div",{className:"grid grid-cols-1 gap-y-4 py-8 first:pt-0 last:pb-0 md:grid-cols-4",children:[(0,n.jsx)("div",{className:"col-span-1 flex justify-center md:justify-start",children:(0,n.jsxs)("div",{className:"relative h-max",children:[(0,n.jsx)("h2",{className:"text-xl font-bold uppercase text-neutral-800",children:i}),(0,n.jsx)("span",{className:"absolute inset-x-0 border-b-2 border-orange-400"}),(0,n.jsx)("div",{children:e&&(0,n.jsx)(n.Fragment,{children:e})})]})}),(0,n.jsx)("div",{className:"col-span-1 flex flex-col md:col-span-3",children:a})]})});a.displayName="TitledSection",i.Z=a},6571:function(t,i,e){var n=e(5893),a=e(822),o=e(7294),r=e(6346),s=(0,o.memo)(function(){return(0,n.jsxs)("div",{className:"relative bg-neutral-900 px-4 pb-6 pt-12 sm:px-8 sm:pt-14 sm:pb-8",children:[(0,n.jsx)("div",{className:"absolute inset-x-0 -top-4 flex justify-center sm:-top-6",children:(0,n.jsx)("a",{className:"rounded-full bg-neutral-100 p-1 ring-white ring-offset-2 ring-offset-gray-700/80 focus:outline-none focus:ring-2 sm:p-2",href:"/#".concat(r._h.Hero),children:(0,n.jsx)(a.Z,{className:"h-6 w-6 bg-transparent sm:h-8 sm:w-8"})})}),(0,n.jsxs)("div",{className:"flex flex-col items-center gap-y-6",children:[(0,n.jsxs)("div",{id:"pageview-script",className:"text-sm text-neutral-700",children:[(0,n.jsx)("a",{href:"https://www.revolvermaps.com/livestats/5r1om30zfoi/",children:(0,n.jsx)("img",{src:"//rf.revolvermaps.com/h/m/a/0/ff0000/128/0/5r1om30zfoi.png",width:"256",height:"128",alt:"Map",style:{border:0}})}),(0,n.jsx)("script",{type:"text/javascript",id:"clstr_globe",src:"//clustrmaps.com/globe.js?d=ko7teOw_sX7QKyWbHLxkMdyOA6BYkSEu0Fo1wnSs9QE"})]}),(0,n.jsxs)("span",{className:"text-sm text-neutral-700",children:["\xa9 Copyright 2022 MVIG-RHOS • Based on"," ",(0,n.jsx)("a",{href:"https://github.com/tbakerx/react-resume-template",children:"tbakerx"})]})]})]})});s.displayName="Footer",i.Z=s},6346:function(t,i,e){e.d(i,{J6:function(){return y},_h:function(){return L},QL:function(){return C},IX:function(){return k},oM:function(){return j},J8:function(){return v},uu:function(){return P},Vf:function(){return S},mZ:function(){return w},q:function(){return I},Po:function(){return H},c$:function(){return D}});var n=e(5893),a=e(5966),o=e(6339),r=e(7402),s=e(1469),h=e(1799),c=e(9396),u=e(7294),l=e(9534),g=(0,u.memo)(function(t){var i=t.children,e=t.className,a=t.svgRef,o=t.transform,r=(0,l.Z)(t,["children","className","svgRef","transform"]);return(0,n.jsx)("svg",(0,c.Z)((0,h.Z)({className:e,fill:"currentColor",ref:a,transform:o,viewBox:"0 0 128 128",width:"128",xmlns:"http://www.w3.org/2000/svg"},r),{children:i}))}),p=(0,u.memo)(function(t){return(0,n.jsxs)(g,(0,c.Z)((0,h.Z)({},t),{children:[(0,n.jsx)("path",{clipRule:"evenodd",d:"M64 5.103c-33.347 0-60.388 27.035-60.388 60.388 0 26.682 17.303 49.317 41.297 57.303 3.017.56 4.125-1.31 4.125-2.905 0-1.44-.056-6.197-.082-11.243-16.8 3.653-20.345-7.125-20.345-7.125-2.747-6.98-6.705-8.836-6.705-8.836-5.48-3.748.413-3.67.413-3.67 6.063.425 9.257 6.223 9.257 6.223 5.386 9.23 14.127 6.562 17.573 5.02.542-3.903 2.107-6.568 3.834-8.076-13.413-1.525-27.514-6.704-27.514-29.843 0-6.593 2.36-11.98 6.223-16.21-.628-1.52-2.695-7.662.584-15.98 0 0 5.07-1.623 16.61 6.19C53.7 35 58.867 34.327 64 34.304c5.13.023 10.3.694 15.127 2.033 11.526-7.813 16.59-6.19 16.59-6.19 3.287 8.317 1.22 14.46.593 15.98 3.872 4.23 6.215 9.617 6.215 16.21 0 23.194-14.127 28.3-27.574 29.796 2.167 1.874 4.097 5.55 4.097 11.183 0 8.08-.07 14.583-.07 16.572 0 1.607 1.088 3.49 4.148 2.897 23.98-7.994 41.263-30.622 41.263-57.294C124.388 32.14 97.35 5.104 64 5.104z",fillRule:"evenodd"}),(0,n.jsx)("path",{d:"M26.484 91.806c-.133.3-.605.39-1.035.185-.44-.196-.685-.605-.543-.906.13-.31.603-.395 1.04-.188.44.197.69.61.537.91zm2.446 2.729c-.287.267-.85.143-1.232-.28-.396-.42-.47-.983-.177-1.254.298-.266.844-.14 1.24.28.394.426.472.984.17 1.255zM31.312 98.012c-.37.258-.976.017-1.35-.52-.37-.538-.37-1.183.01-1.44.373-.258.97-.025 1.35.507.368.545.368 1.19-.01 1.452zm3.261 3.361c-.33.365-1.036.267-1.552-.23-.527-.487-.674-1.18-.343-1.544.336-.366 1.045-.264 1.564.23.527.486.686 1.18.333 1.543zm4.5 1.951c-.147.473-.825.688-1.51.486-.683-.207-1.13-.76-.99-1.238.14-.477.823-.7 1.512-.485.683.206 1.13.756.988 1.237zm4.943.361c.017.498-.563.91-1.28.92-.723.017-1.308-.387-1.315-.877 0-.503.568-.91 1.29-.924.717-.013 1.306.387 1.306.88zm4.598-.782c.086.485-.413.984-1.126 1.117-.7.13-1.35-.172-1.44-.653-.086-.498.422-.997 1.122-1.126.714-.123 1.354.17 1.444.663zm0 0"})]}))}),d=e(1438),m=e(2951),b=e(8029),A=e(6567),f=function(t){(0,b.Z)(e,t);var i=(0,A.Z)(e);function e(){return(0,d.Z)(this,e),i.apply(this,arguments)}return(0,m.Z)(e,[{key:"render",value:function(){return(0,n.jsxs)("a",{href:"/hake",children:[(0,n.jsxs)("b",{children:[(0,n.jsx)("span",{style:{color:"red"},children:"H"}),(0,n.jsx)("span",{style:{color:"blue"},children:"A"}),(0,n.jsx)("span",{style:{color:"red"},children:"KE"})]}),this.props.children]})}}]),e}(u.Component),x=(0,u.memo)(function(t){return(0,n.jsx)(g,(0,c.Z)((0,h.Z)({},t),{children:(0,n.jsx)("path",{d:"M116 3H12a8.91 8.91 0 00-9 8.8v104.42a8.91 8.91 0 009 8.78h104a8.93 8.93 0 009-8.81V11.77A8.93 8.93 0 00116 3zM39.17 107H21.06V48.73h18.11zm-9-66.21a10.5 10.5 0 1110.49-10.5 10.5 10.5 0 01-10.54 10.48zM107 107H88.89V78.65c0-6.75-.12-15.44-9.41-15.44s-10.87 7.36-10.87 15V107H50.53V48.73h17.36v8h.24c2.42-4.58 8.32-9.41 17.13-9.41C103.6 47.28 107 59.35 107 75z",fill:"currentColor"})}))}),v={title:"RHOS",description:"Homepage of RHOS"},L={Hero:"hero",About:"about",Recruit:"recruit",News:"news",Demos:"demos",Projects:"projects",Portfolio:"portfolio",Publications:"publications",People:"people"},y=[L.About,L.News,L.Demos,L.Projects,L.Publications,L.People],j={imageSrc:{src:"/_next/static/media/Robotics_Cyberpunk_2077_bg.f50d6e01.png",height:1125,width:2e3,blurDataURL:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAIAAAD38zoCAAAAhUlEQVR42mPw8DGPDLMz0ZMzs7OMTwpVNTAI8LayN1dikDXTUbUx9nDQD/K01jHSUzOyNrd1NjbUZVDQkbGw1zO1NXJ1srS2MjKysLK0tPEOiWCwMZVJ8FUvTbToKLSfUe9eHKXhbCrEIynL4GuvVJLiUp/jVZVqmxWqGuggpaciLCDACQABSR1UPIEuQgAAAABJRU5ErkJggg==",blurWidth:8,blurHeight:5},name:"RHOS",description:(0,n.jsx)(n.Fragment,{children:"Robot • Human • Object • Scene"}),actions:[{href:"#".concat(L.Recruit),text:"Recruit",primary:!0},{href:"https://github.com/mvig-rhos",text:"Github",primary:!1,Icon:p},{href:"https://mvig-rhos.com/clean-demos",text:"Demo",primary:!1},]},C={profileImageSrc:{src:"/_next/static/media/profilepic.80e868bc.jpg",height:294,width:294,blurDataURL:"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAgACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABAEBAQAAAAAAAAAAAAAAAAAAAwT/2gAMAwEAAhADEAAAAKxSH//EABsQAAIDAAMAAAAAAAAAAAAAAAECAwQRAAUh/9oACAEBAAE/AGs9hWWuDIzGTUkjw6rH0c//xAAZEQEAAgMAAAAAAAAAAAAAAAAhAAECAwT/2gAIAQIBAT8Aw6dlItk//8QAGhEBAAEFAAAAAAAAAAAAAAAAASEAAwQxMv/aAAgBAwEBPwBxraHURuv/2Q==",blurWidth:8,blurHeight:8},aboutWhat:"RHOS",description:(0,n.jsxs)("div",{children:[(0,n.jsxs)("p",{children:["Hi, this is the website of RHOS team at"," ",(0,n.jsx)("a",{className:"font-bold text-white",href:"https://www.mvig.org/",children:"MVIG"}),". We study"," ",(0,n.jsx)("i",{children:(0,n.jsx)("b",{children:"Embodied AI"})}),","," ",(0,n.jsx)("i",{children:(0,n.jsx)("b",{children:"Physical Reasoning"})}),", and"," ",(0,n.jsx)("i",{children:(0,n.jsx)("b",{children:"Human Activity Understanding"})}),". We are building a knowledge and reasoning driven system that enables intelligent agents/robots to perceive human activities, reason human behavior logics, learn skills from human activities and interact with the environment."]}),(0,n.jsx)("p",{children:(0,n.jsx)("b",{children:"Research Interests: "})}),(0,n.jsxs)("p",{children:["(S) ",(0,n.jsx)("b",{children:"Embodied AI"}),": how to make agents learn skills from humans and interact with humans & scenes & objects.",(0,n.jsx)("br",{}),"(S-1) ",(0,n.jsx)("b",{children:"Human Activity Understanding"}),": how to learn and ground complex/ambiguous human activity concepts (body motion, human-object/human/scene interaction) and object concepts from multi-modal information (2D-3D-4D).",(0,n.jsx)("br",{}),"(S-2) ",(0,n.jsx)("b",{children:"Visual Reasoning"}),": how to mine, capture, and embed the logic and causal relations from human activities.",(0,n.jsx)("br",{}),"(S-3) ",(0,n.jsx)("b",{children:"General Multi-Modal Foundation Models"}),": especially for human-centric perception tasks.",(0,n.jsx)("br",{}),"(S-4) ",(0,n.jsx)("b",{children:"Activity Understanding from A Cognitive Perspective"}),": work with multidisciplinary researchers to study how the brain perceives activities.",(0,n.jsx)("br",{}),"(E) ",(0,n.jsx)("b",{children:"Human-Robot Interaction for hospital, home, factory, etc."}),": work with experts in different domains to develop intelligent robots to help people."]})]}),contactDetail:(0,n.jsxs)("div",{children:[(0,n.jsx)("b",{children:"Yong-Lu Li"}),(0,n.jsx)("br",{}),"Email: yonglu_li[at]sjtu[dot]edu[dot]cn Shanghai Jiao Tong University Shanghai Innovation Institute"]}),contactLinkItems:[{label:"Personal Website",link:"https://dirtyharrylyl.github.io/",Icon:a.Z,newline:!0},{label:"Google Scholar ",link:"https://scholar.google.com.hk/citations?user=UExAaVgAAAAJ",Icon:o.Z},{label:"Github ",link:"https://github.com/DirtyHarryLYL",Icon:p},{label:"LinkedIn",link:"https://www.linkedin.com/in/%E6%B0%B8%E9%9C%B2-%E6%9D%8E-991b99139/",Icon:x},{label:"dblp",link:"https://dblp.org/pid/198/9345.html",Icon:r.Z},{label:"Semantic Scholar",link:"https://www.semanticscholar.org/author/Yong-Lu-Li/10384643",Icon:r.Z},{label:"Robot Demos",link:"https://mvig-rhos.com/clean-demos",Icon:a.Z,newline:!0},]},D=(0,n.jsxs)("span",{children:[(0,n.jsxs)("p",{children:["We are actively looking for self-motivated"," ",(0,n.jsx)("strong",{children:"students (Master/PhD, 2026 spring & fall), interns/engineers/visitors"})," ","(CV/ML/ROB/NLP/Math/Phys background, always welcome) to join us in"," ",(0,n.jsx)("a",{className:"text-red600",href:"https://www.mvig.org/",children:"Machine Vision and Intelligence Group (MVIG)"}),". If you have the same/similar interests, feel free to email me your resume."]}),(0,n.jsxs)("p",{children:["Click: "," ",(0,n.jsx)("a",{className:"text-red-600",href:"https://dirtyharrylyl.github.io/recruit.html",children:(0,n.jsx)("b",{children:"Eng"})})," "," or",(0,n.jsx)("a",{className:"text-red-600",href:"https://mvig-rhos.com/recruit",children:(0,n.jsx)("b",{children:" 中"})})," ","for more details."]})]}),P=[["2025.5",(0,n.jsxs)(n.Fragment,{children:["Our paper on human-robot joint learning has won the ",(0,n.jsx)("a",{className:"text-red-600",children:"ICRA 2025 Best Paper Award on Human-Robot Interaction"}),"!"]}),],["2025.4",(0,n.jsx)(n.Fragment,{children:"Our paper on human-robot joint learning has been selected as an ICRA 2025 Best Paper Award Finalist!"}),],["2025.2",(0,n.jsx)(n.Fragment,{children:"Our work on 3D HOI reconstruction, motion dynamics, garment generation/reconstruction, and dynamic object segmentation will appear at CVPR 2025!"}),],["2025.1",(0,n.jsx)(n.Fragment,{children:"Our work on efficient robot teleoperation will appear at ICRA 2025."}),],["2025.1",(0,n.jsx)(n.Fragment,{children:"Two works on the association ability of LLM and human motion will appear at ICLR 2025."}),],["2024.12",(0,n.jsx)(n.Fragment,{children:"Our work on video human-object interaction learning will appear at AAAI 2025."}),],["2024.9",(0,n.jsx)(n.Fragment,{children:"Two works on articulated object image manipulation and humanoid-object interaction will appear at NeurIPS 2024!"}),],["2024.7",(0,n.jsx)(n.Fragment,{children:"Five works on visual reasoning, 4D human motions, embodied AI, and dataset distillation will appear at ECCV 2024!"}),],["2024.2",(0,n.jsxs)(n.Fragment,{children:["Our work ",(0,n.jsx)("b",{children:(0,n.jsx)("a",{className:"underline text-sky-600",href:"https://mvig-rhos.com/pangea",children:"Pangea"})})," and ",(0,n.jsx)("b",{children:(0,n.jsx)("a",{className:"underline text-sky-600",href:"https://mvig-rhos.com/video-distill",children:"Video Distillation"})})," will appear at CVPR 2024."]}),],["2023.12",(0,n.jsxs)(n.Fragment,{children:["Our work on ",(0,n.jsxs)("b",{children:["primitive-based HOI reconstruction (",(0,n.jsx)("a",{className:"underline text-sky-600",href:"https://mvig-rhos.com/p3haoi",children:"P3HAOI"}),")"]})," will appear at AAAI 2024!"]}),],["2023.9",(0,n.jsxs)(n.Fragment,{children:["The advanced HAKE reasoning engine based on LLM (",(0,n.jsx)("a",{className:"underline text-sky-600",href:"https://mvig-rhos.com/symbol_llm",children:"Symbol-LLM"}),") will appear at NeurIPS'23!"]}),],["2023.7",(0,n.jsxs)(n.Fragment,{children:["Our works on ",(0,n.jsx)("a",{className:"underline text-sky-600",href:"https://mvig-rhos.com/ego_pca",children:"ego-centric video understanding"})," and ",(0,n.jsx)("a",{className:"underline text-sky-600",href:"https://mvig-rhos.com/ocl",children:"object concept learning"})," will appear at ICCV'23!"]}),],["2023.7",(0,n.jsxs)(n.Fragment,{children:["The upgrade version of ",(0,n.jsx)("a",{className:"underline text-sky-600",href:"https://github.com/AllenXuuu/DCR",children:"DCR"})," will appear at IJCV!"]}),],["2022.12",(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)("a",{className:"font-bold",href:"https://arxiv.org/abs/2202.06851",children:"HAKE 2.0"})," will appear at TPAMI!"]}),],["2022.12",(0,n.jsxs)(n.Fragment,{children:["OCL (Object Concept Learning) is released on ",(0,n.jsx)("a",{className:"underline text-sky-600",href:"https://arxiv.org/abs/2212.02710",children:"arXiv"}),". Please visit the ",(0,n.jsx)("a",{className:"underline text-sky-600",href:"/ocl",children:"project page"})," for details."]}),],["2022.11",(0,n.jsxs)(n.Fragment,{children:["We release the human body part states and interactive object bounding box annotations upon AVA (2.1 & 2.2): ",(0,n.jsx)("a",{className:"font-bold",href:"https://github.com/DirtyHarryLYL/HAKE-AVA",children:"[HAKE-AVA]"}),", and a CLIP-based human part state & verb recognizer: ",(0,n.jsx)("a",{className:"font-bold",href:"https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/CLIP-Activity2Vec",children:"[CLIP-Activity2Vec]"}),"."]}),],["2022.11",(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)("a",{className:"font-bold",href:"https://github.com/MVIG-SJTU/AlphaPose",children:"AlphaPose"})," will appear at TPAMI!"]}),],["2022.07",(0,n.jsxs)(n.Fragment,{children:["Two papers on ",(0,n.jsx)("b",{children:"longtailed learning, HOI detection"})," are accepted by ECCV'22, arXivs and code are coming soon"]}),],["2022.03",(0,n.jsxs)(n.Fragment,{children:["Five papers on ",(0,n.jsx)("b",{children:"HOI detection/prediction, trajection prediction, 3D detection/keypoints"})," are accepted by CVPR'22, papers and code are coming soon."]}),],["2022.02",(0,n.jsxs)(n.Fragment,{children:["We release the human body part state labels based on AVA:"," ",(0,n.jsx)("a",{className:"font-bold",href:"https://github.com/DirtyHarryLYL/HAKE-AVA",children:"HAKE-AVA"})," and ",(0,n.jsx)("a",{className:"font-bold",href:"https://arxiv.org/abs/2202.06851",children:"HAKE 2.0"}),"."]}),],["2021.12",(0,n.jsxs)(n.Fragment,{children:["Our work on ",(0,n.jsx)("b",{children:"HOI generalization"})," will appear at AAAI'22."]}),],["2021.10",(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)("b",{children:"Learning Single/Multi-Attribute of Object with Symmetry and Group"})," is accepted by TPAMI."]}),],["2021.09",(0,n.jsxs)(n.Fragment,{children:["Our work ",(0,n.jsx)("b",{children:"Localization with Sampling-Argmax"})," will appear at NeurIPS'21."]}),],["2021.02",(0,n.jsxs)(n.Fragment,{children:["Upgraded"," ",(0,n.jsx)("a",{className:"font-bold",href:"https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/Activity2Vec",children:"HAKE-Activity2Vec"})," ","is released! Images/Videos --> human box + ID + skeleton + part states + action + representation."," ",(0,n.jsx)("a",{className:"underline",href:"https://youtu.be/ty-bXDInLMQ",children:"[Demo]"})," ",(0,n.jsx)("a",{className:"underline",href:"https://drive.google.com/file/d/1iZ57hKjus2lKbv1MAB-TLFrChSoWGD5e/view?usp=sharing",children:"[Description]"})]}),],["2021.01",(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)("b",{children:[(0,n.jsx)("a",{href:"https://arxiv.org/abs/2101.10292",children:"TIN"})," (Transferable Interactiveness Network)"]})," ","is accepted by TPAMI."]}),],["2020.12",(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)("a",{className:"font-bold underline",href:"https://arxiv.org/abs/2010.01007",children:"DecAug"})," ","is accepted by AAAI'21."]}),],["2020.09",(0,n.jsxs)(n.Fragment,{children:["Our work ",(0,n.jsx)("b",{children:"HOI Analysis"})," will appear at NeurIPS 2020."]}),],["2020.06",(0,n.jsxs)(n.Fragment,{children:["The larger"," ",(0,n.jsx)("a",{className:"font-bold",href:"https://github.com/DirtyHarryLYL/HAKE#hake-large-for-instance-level-hoi-detection",children:"HAKE-Large"})," ","(>120K images with activity and part state labels) is released."]}),],["2020.02",(0,n.jsxs)(n.Fragment,{children:["Three papers ",(0,n.jsx)("b",{children:"Image-based HAKE: PaSta-Net"}),", ",(0,n.jsx)("b",{children:"2D-3D Joint HOI Learning"}),","," ",(0,n.jsx)("b",{children:"Symmetry-based Attribute-Object Learning"})," are accepted in ",(0,n.jsx)("a",{href:"http://cvpr2020.thecvf.com/",children:"CVPR'20"}),"! Papers and corresponding resources (code, data) will be released soon."]}),],["2019.07",(0,n.jsxs)(n.Fragment,{children:["Our paper ",(0,n.jsx)("b",{children:"InstaBoost"})," is accepted in ",(0,n.jsx)("a",{href:"http://iccv2019.thecvf.com/",children:"ICCV'19"}),"."]}),],["2019.06",(0,n.jsxs)(n.Fragment,{children:["The Part I of our ",(0,n.jsx)(f,{}),":"," ",(0,n.jsx)("b",{children:(0,n.jsx)("a",{href:"http://hake-mvig.cn/download/",children:"HAKE-HICO"})})," ","which contains the image-level part-state annotations is released."]}),],["2019.04",(0,n.jsxs)(n.Fragment,{children:["Our project ",(0,n.jsx)(f,{})," (Human Activity Knowledge Engine) begins trial operation."]}),],["2019.02",(0,n.jsxs)(n.Fragment,{children:["Our paper on"," ",(0,n.jsx)("b",{children:(0,n.jsx)("a",{href:"https://arxiv.org/abs/1811.08264",children:"Interactiveness"})})," ","is accepted in ",(0,n.jsx)("a",{href:"http://cvpr2019.thecvf.com/",children:"CVPR'19"}),"."]}),],["2018.07",(0,n.jsxs)(n.Fragment,{children:["Our paper on"," ",(0,n.jsx)("b",{children:(0,n.jsx)("a",{href:"https://arxiv.org/abs/1801.08839",children:"GAN & Annotation Generation"})})," ","is accepted in ",(0,n.jsx)("a",{href:"https://eccv2018.org/",children:"ECCV'18"}),"."]}),],["2018.05",(0,n.jsxs)(n.Fragment,{children:["Presentation (Kaibot Team) in"," ",(0,n.jsx)("a",{href:"https://icra2018.org/tidy-up-my-room-challenge/",children:"TIDY UP MY ROOM CHALLENGE | ICRA'18"}),"."]}),],["2018.02",(0,n.jsxs)(n.Fragment,{children:["Our paper on"," ",(0,n.jsx)("b",{children:(0,n.jsx)("a",{href:"http://ai.ucsd.edu/~haosu/papers/cvpr18_partstate.pdf",children:"Object Part States"})})," ","is accepted in ",(0,n.jsx)("a",{href:"http://cvpr2018.thecvf.com/program/main_conference",children:"CVPR'18"}),"."]}),],];n.Fragment;var w=[{title:"HAKE 2.0",description:"The upgraded Human Activity Knowledge Engine",url:"http://hake-mvig.cn/home",image:"2022_hake2.0.jpg"},{title:"PartMap",description:"Interactiveness learning from the global, scene-level perspective",url:"https://github.com/enlighten0707/Body-Part-Map-for-Interactiveness",image:"2022_ECCV_partmap.jpg"},{title:"DLSA",description:"Plug-and-play long-tail learning module by reorganizing label space",url:"https://github.com/silicx/DLSA",image:"2022_ECCV_longtail.jpg"},{title:"Interactiveness-Field",description:"Model HOI with the interactiveness bimodal prior",url:"https://github.com/Foruck/Interactiveness-Field",image:"2022_CVPR_InteractivenessField.jpg"},{title:"DCR",description:"A training strategy on action predictions with a dynamic learning pipeline",url:"https://github.com/AllenXuuu/DCR",image:"2022_CVPR_anticipate.jpg"},{title:"OC-Immunity",description:"Advance HOI generalization by mitigating object category bias",url:"https://github.com/Foruck/OC-Immunity",image:"2022_AAAI_hoi.jpg"},{title:"TIN & TIN++",description:"Interactiveness learning for Human-Object Interaction learning.",url:"https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network",image:"2021_TPAMI_TIN.jpg"},{title:"SymNet",description:"Attribute detector based on symmetry and group.",url:"https://github.com/DirtyHarryLYL/SymNet",image:"2021_TPAMI_SymNet.jpg"},{title:"HOI Analysis",description:"A way to decompose and integrate human-object interaction pairs.",url:"https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network)",image:"2020_NeurIPS_analysis.jpg"},{title:"PaStaNet",description:"A human body Part States library and learner.",url:"https://github.com/DirtyHarryLYL/HAKE-Action",image:"2020_CVPR_pastanet.jpg"},{title:"DJ-RN",description:"A 2D-3D joint learning framework for Human-Object Interaction detection.",url:"https://github.com/DirtyHarryLYL/DJ-RN",image:"2020_CVPR_djrn.jpg"},{title:"HAKE 1.0",description:"Human Activity Knowledge Engine.",url:"http://hake-mvig.cn/home",image:"2019_hake1.jpg"},],I=[{date:"2018",location:"Human Activity Knowledge Engine",title:"HAKE",link:"http://hake-mvig.cn/home/",content:(0,n.jsxs)("p",{children:["Human Activity Knowledge Engine (",(0,n.jsx)(f,{}),") is a knowledge-driven system that enables intelligent agents to perceive human activities, reason human behavior logics, learn skills from human activities, and interact with objects and environments."]})},{date:"2022",location:"Object Concept Learning",title:"OCL",link:"/ocl",content:(0,n.jsx)("p",{children:"We propose a challenging Object Concept Learning (OCL) task to push the envelope of object understanding. It requires machines to reason out object affordances and simultaneously give the reason: what attributes make an object possess these affordances."})},{date:"2023",location:"Unified Action Semantic Space",title:"Pangea",link:"/pangea",content:(0,n.jsx)("p",{children:"We design an action semantic space given verb taxonomy hierarchy and covering massive actions. Thus, we can gather multi-modal datasets into a unified database in a unified label system, i.e., bridging “isolated islands” into a “Pangea”. So then, we propose a bidirectional mapping model between physical and semantic space to use Pangea fully."})},{date:"2023",location:"EgoPCA: A New Framework for EgoHOI",title:"EgoPCA",link:"/ego_pca",content:(0,n.jsx)("p",{children:"We rethink and propose a new framework as an infrastructure to advance Ego-HOI recognition by Probing, Curation and Adaption (EgoPCA). We contribute comprehensive pre-train sets, balanced test sets and a new baseline, which are complete with a training-finetuning strategy and several new and effective mechanisms and settings to advance further research."})},{date:"2023",location:"Robotic Teleoperation, Robot Manipulation, Imitation Learning",title:"Human-Agent Joint Learning",link:"/joint_learning",content:(0,n.jsx)("p",{children:"A human-agent joint learning teleoperation system for faster data collection, less human effort, and efficient robot manipulation skill acquisition."})},{date:"2023",location:"Video Distillation via Static-Dynamic Disentanglement",title:"Video-Distillation",link:"/video-distill",content:(0,n.jsx)("p",{children:"We provide the first systematic study of video distillation and introduce a taxonomy to categorize temporal compression. This taxonomy motivates our unified framework for disentangling the dynamic and static information in videos. It first distills the videos into still images as static memory and then compensates the dynamic and motion information with a learnable dynamic memory block."})},],H=[{title:"Motion Before Action: Diffusing Object Motion as Manipulation Condition",author:"Yue Su, Xinyu Zhan, Hongjie Fang, Yong-Lu Li, Cewu Lu, Lixin Yang",conf:"RA-L 2025",links:[["arXiv","https://arxiv.org/abs/2411.09658"],["PDF","https://arxiv.org/pdf/2411.09658.pdf"],["Project","https://selen-suyue.github.io/MBApage/"],["Code","https://github.com/Selen-Suyue/MBA"],]},{title:"SIME: Enhancing Policy Self-Improvement with Modal-level Exploration",author:"Yang Jin, Jun Lv, Wenye Yu, Hongjie Fang, Yong-Lu Li, Cewu Lu",conf:"arXiv 2025",links:[["arXiv","https://arxiv.org/abs/2505.01396"],["PDF","https://arxiv.org/pdf/2505.01396.pdf"],["Project","https://ericjin2002.github.io/SIME/"],["Code","https://github.com/EricJin2002/SIME"],]},{title:"Dense Policy: Bidirectional Autoregressive Learning of Actions",author:"Yue Su, Xinyu Zhan, Hongjie Fang, Han Xue, Hao-Shu Fang, Yong-Lu Li, Cewu Lu, Lixin Yang",conf:"arXiv 2025",links:[["arXiv","https://arxiv.org/abs/2503.13217"],["PDF","https://arxiv.org/pdf/2503.13217.pdf"],["Project","https://selen-suyue.github.io/DspNet/"],["Code","https://github.com/Selen-Suyue/DensePolicy"],]},{title:"Reconstructing In-the-Wild Open-Vocabulary Human-Object Interactions",author:"Boran Wen, Dingbang Huang, Zichen Zhang, Jiahong Zhou, Jianbin Deng, Jingyu Gong, Yulong Chen#, Lizhuang Ma#, Yong-Lu Li#",conf:"CVPR 2025",links:[["arXiv","https://arxiv.org/abs/2503.15898"],["PDF","https://arxiv.org/pdf/2503.15898.pdf"],["Project","https://wenboran2002.github.io/3dhoi/"],["Code","https://github.com/wenboran2002/open-3dhoi"],]},{title:"Homogeneous Dynamics Space for Heterogeneous Humans",author:"Xinpeng Liu, Junxuan Liang, Chenshuo Zhang, Zixuan Cai, Cewu Lu#, Yong-Lu Li#",conf:"CVPR 2025",links:[["arXiv","https://arxiv.org/abs/2412.06146"],["PDF","https://arxiv.org/pdf/2412.06146.pdf"],["Project","https://foruck.github.io/HDyS/"],["Code","xx"],]},{title:"GaPT-DAR: Category-level Garments Pose Tracking via Integrated 2D Deformation and 3D Reconstruction",author:"Li Zhang, Mingliang Xu, Jianan Wang, Qiaojun Yu, Lixin Yang, Yong-Lu Li, Cewu Lu, RujingWang, Liu Liu",conf:"CVPR 2025",links:[["arXiv","https://arxiv.org/abs/xx"],["PDF","https://arxiv.org/pdf/xx.pdf"],["Project","xx"],["Code","xx"],]},{title:"M3-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object Segmentation",author:"Zixuan Chen*, Jiaxin Li*, Liming Tan, Yejie Guo, Junxuan Liang, Cewu Lu, Yong-Lu Li#",conf:"CVPR 2025",links:[["arXiv","https://arxiv.org/abs/2412.13803"],["PDF","https://arxiv.org/pdf/2412.13803.pdf"],["Project","https://zixuan-chen.github.io/M-cube-VOS.github.io/"],["Code","https://github.com/zixuan-chen/DeformVOS"],]},{title:"Design2GarmentCode: Turning Design Concepts to Tangible Garments Through Program Synthesis",author:"Feng Zhou, Ruiyang Liu, Chen Liu, Gaofeng He, Yong-Lu Li, Xiaogang Jin, Huamin Wang",conf:"CVPR 2025",links:[["arXiv","https://arxiv.org/abs/2412.08603"],["PDF","https://arxiv.org/pdf/2412.08603.pdf"],["Project","https://style3d.github.io/design2garmentcode/"],["Code","https://github.com/Style3D/SXDGarmentCode"],]},{title:"Human-Agent Joint Learning for Efficient Robot Manipulation Skill Acquisition",author:"Shengcheng Luo*, Quanquan Peng*, Jun Lv*, Kaiwen Hong, Katherine Rose Driggs-Campbell, Cewu Lu, Yong-Lu Li#",conf:"ICRA 2025, Best Paper Award on Human-Robot Interaction",links:[["arXiv","https://arxiv.org/abs/2407.00299"],["PDF","https://arxiv.org/pdf/2407.00299.pdf"],["Project","https://norweig1an.github.io/HAJL.github.io/"],["Code","xx"],]},{title:"ImDy: Human Inverse Dynamics from Imitated Observations",author:"Xinpeng Liu, Junxuan Liang, Zili Lin, Haowen Hou, Yong-Lu Li#, Cewu Lu#",conf:"ICLR 2025",links:[["arXiv","https://arxiv.org/abs/2410.17610"],["PDF","https://arxiv.org/pdf/2410.17610.pdf"],["Project","https://foruck.github.io/ImDy/"],["Code","https://github.com/Foruck/ImDy"],]},{title:"The Labyrinth of Links: Navigating the Associative Maze of Multi-modal LLMs",author:"Hong Li, Nanxi Li, Yuanjie Chen, Jianbin Zhu, Qinlu Guo, Cewu Lu, Yong-Lu Li#",conf:"ICLR 2025",links:[["arXiv","https://arxiv.org/abs/2410.01417"],["PDF","https://arxiv.org/pdf/2410.01417.pdf"],["Project","https://mvig-rhos.com/llm_inception"],["Code","https://github.com/lihong2303/LLM_Inception"],]},{title:"exUMI: Extensible System for Robot Teaching with Precise Proprioception and Multi-Modalities",author:"Yue Xu, Litao Wei, Pengyu An, Yong-Lu Li#",conf:"arXiv 2025",links:[["arXiv","https://arxiv.org/abs/"],["PDF","https://arxiv.org/pdf/.pdf"],["Project","xx"],["Code","https://github.com/silicx/exUMI"],]},{title:"Interacted Object Grounding in Spatio-Temporal Human-Object Interactions",author:"Xiaoyang Liu*, Boran Wen*, Xinpeng Liu*, Zizheng Zhou, Hongwei Fan, Cewu Lu, Lizhuang Ma, Yulong Chen#, Yong-Lu Li#",conf:"AAAI 2025",links:[["arXiv","https://arxiv.org/abs/2412.19542"],["PDF","https://arxiv.org/pdf/2412.19542.pdf"],["Code","https://github.com/DirtyHarryLYL/HAKE-AVA/tree/DIO"],]},{title:"Verb Mirage: Unveiling and Assessing Verb Concept Hallucinations in Multimodal Large Language Models",author:"Zehao Wang, Xinpeng Liu, Xiaoqian Wu, Yudonglin Zhang, Zhou Fang, Yifan Fang, Junfu Pu, Cewu Lu#, Yong-Lu Li#",conf:"arXiv 2024",links:[["arXiv","https://arxiv.org/abs/2412.04939"],["PDF","https://arxiv.org/pdf/2412.04939.pdf"],["Project",""],["Code",""],]},{title:"General Articulated Objects Manipulation in Real Images via Part-Aware Diffusion Process",author:"Zhou Fang, Yong-Lu Li#, Lixin Yang, Cewu Lu#",conf:"NeurIPS 2024",links:[["arXiv","https://arxiv.org/abs/xx"],["PDF","https://openreview.net/pdf?id=WRd9LCbvxN"],["Project","https://mvig-rhos.com/pa_diffusion"],]},{title:"HumanVLA: Towards Vision-Language Directed Object Rearrangement by Physical Humanoid",author:"Xinyu Xu, Yizheng Zhang, Yong-Lu Li, Lei Han, Cewu Lu",conf:"NeurIPS 2024",links:[["arXiv","https://arxiv.org/abs/2406.19972"],["PDF","https://arxiv.org/pdf/2406.19972.pdf"],["Code","https://github.com/AllenXuuu/HumanVLA"],]},{title:"Take A Step Back: Rethinking the Two Stages in Visual Reasoning",author:"Mingyu Zhang*, Jiting Cai*, Mingyu Liu, Yue Xu, Cewu Lu, Yong-Lu Li#",conf:"ECCV 2024",links:[["arXiv","https://arxiv.org/abs/2407.19666"],["PDF","https://arxiv.org/pdf/2407.19666.pdf"],["Project","https://mybearyzhang.github.io/projects/TwoStageReason/"],["Code","https://github.com/mybearyZhang/TwoStageReason"],]},{title:"Revisit Human-Scene Interaction via Space Occupancy",author:"Xinpeng Liu*, Haowen Hou*, Yanchao Yang, Yong-Lu Li#, Cewu Lu",conf:"ECCV 2024",links:[["arXiv","https://arxiv.org/abs/2312.02700"],["PDF","https://arxiv.org/pdf/2312.02700.pdf"],["Project","https://foruck.github.io/occu-page/"],["Code","https://github.com/HaowenHou/Motion-Occupancy-Base"],]},{title:"Bridging the Gap between Human Motion and Action Semantics via Kinematic Phrases",author:"Xinpeng Liu, Yong-Lu Li#, Ailing Zeng, Zizheng Zhou, Yang You, Cewu Lu#",conf:"ECCV 2024",links:[["arXiv","https://arxiv.org/abs/2310.04189"],["PDF","https://arxiv.org/pdf/2310.04189.pdf"],["Project","https://foruck.github.io/KP/"],["Code","xx"],]},{title:"Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection",author:"Yue Xu, Yong-Lu Li#, Kaitong Cui, Ziyu Wang, Cewu Lu, Yu-Wing Tai, Chi-Keung Tang",conf:"ECCV 2024",links:[["arXiv","https://arxiv.org/abs/2305.18381"],["PDF","https://arxiv.org/pdf/2305.18381.pdf"],["Code","https://github.com/silicx/GoldFromOres"],]},{title:"DISCO: Embodied Navigation and Interaction via Differentiable Scene Semantics and Dual-level Control",author:"Xinyu Xu, Shengcheng Luo, Yanchao Yang, Yong-Lu Li#, Cewu Lu#",conf:"ECCV 2024",links:[["arXiv","https://arxiv.org/abs/2407.14758"],["PDF","https://arxiv.org/pdf/2407.14758.pdf"],["Code","https://github.com/AllenXuuu/DISCO"],]},{title:"Low-Rank Similarity Mining for Multimodal Dataset Distillation",author:"Yue Xu, Zhilin Lin, Yusong Qiu, Cewu Lu, Yong-Lu Li#",conf:"ICML 2024",links:[["arXiv","https://arxiv.org/abs/2406.03793"],["PDF","https://arxiv.org/pdf/2406.03793.pdf"],["Project","https://mvig-rhos.com/xx"],["Code","https://github.com/silicx/LoRS_Distill"],]},{title:"Dancing with Still Images: Video Distillation via Static-Dynamic Disentanglement",author:"Ziyu Wang*, Yue Xu*, Cewu Lu, Yong-Lu Li#",conf:"CVPR 2024",links:[["arXiv","https://arxiv.org/abs/2312.00362"],["PDF","https://arxiv.org/pdf/2312.00362.pdf"],["Project","https://mvig-rhos.com/video-distill"],["Code","https://github.com/yuz1wan/video_distillation"],]},{title:"From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding",author:"Yong-Lu Li*, Xiaoqian Wu*, Xinpeng Liu, Yiming Dou, Yikun Ji, Junyi Zhang, Yixing Li, Xudong Lu, Jingru Tan, Cewu Lu",conf:"CVPR 2024, Highlight",links:[["arXiv","https://arxiv.org/abs/2304.00553"],["PDF","https://arxiv.org/pdf/2304.00553.pdf"],["Project","https://mvig-rhos.com/pangea"],["Code","https://github.com/DirtyHarryLYL/Sandwich"],]},{title:"Primitive-based 3D Human-Object Interaction Modelling and Programming",author:"Siqi Liu, Yong-Lu Li#, Zhou Fang, Xinpeng Liu, Yang You, Cewu Lu#",conf:"AAAI 2024",links:[["arXiv","https://arxiv.org/abs/2312.10714"],["PDF","https://arxiv.org/pdf/2312.10714.pdf"],["Project","https://mvig-rhos.com/p3haoi"],["Code","https://github.com/MayuOshima/P3HAOI"],]},{title:"Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human Activity Reasoning",author:"Xiaoqian Wu, Yong-Lu Li#, Jianhua Sun, Cewu Lu#",conf:"NeurIPS 2023",links:[["arXiv","https://arxiv.org/abs/2311.17365"],["PDF","https://arxiv.org/pdf/2311.17365.pdf"],["Project","https://mvig-rhos.com/symbol_llm"],["Code","https://github.com/enlighten0707/Symbol-LLM"],]},{title:"Beyond Object Recognition: A New Benchmark towards Object Concept Learning",author:"Yong-Lu Li, Yue Xu, Xinyu Xu, Xiaohan Mao, Yuan Yao, Siqi Liu, Cewu Lu",conf:"ICCV 2023",links:[["arXiv","https://arxiv.org/abs/2212.02710"],["PDF","https://arxiv.org/pdf/2212.02710.pdf"],["Project","https://mvig-rhos.com/ocl"],["Code & Data","https://github.com/silicx/ObjectConceptLearning"],]},{title:"EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding",author:"Yue Xu, Yong-Lu Li#, Zhemin Huang, Michael Xu LIU, Cewu Lu, Yu-Wing Tai, Chi Keung Tang.",conf:"ICCV 2023",links:[["arXiv","https://arxiv.org/abs/2309.02423"],["PDF","https://arxiv.org/pdf/2309.02423.pdf"],["Project","https://mvig-rhos.com/ego_pca"],["Code","https://github.com/silicx/EgoPCA"],]},{title:"Dynamic Context Removal: A General Training Strategy for Robust Models on Video Action Predictive Tasks",author:"Xinyu Xu, Yong-Lu Li#, Cewu Lu#.",conf:"IJCV 2023",links:[["arXiv","https://arxiv.org/abs/"],["PDF","https://arxiv.org/pdf/.pdf"],["Code","https://github.com/AllenXuuu/DCR"],]},{title:"Discovering A Variety of Objects in Spatio-Temporal Human-Object Interactions",author:"Yong-Lu Li*, Hongwei Fan*, Zuoyu Qiu, Yiming Dou, Liang Xu, Hao-Shu Fang, Peiyang Guo, Haisheng Su, Dongliang Wang, Wei Wu, Cewu Lu",conf:"Tech Report",links:[["arXiv","https://arxiv.org/abs/2211.07501"],["PDF","https://arxiv.org/pdf/2211.07501.pdf"],["Code & Data","https://github.com/DirtyHarryLYL/HAKE-AVA"],],github:"https://github.com/DirtyHarryLYL/HAKE-AVA",content:"A part of the HAKE Project"},{title:"HAKE: Human Activity Knowledge Engine",author:"Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu, Mingyang Chen, Ze Ma, Shiyi Wang, Hao-Shu Fang, Cewu Lu",conf:"Tech Report",shortname:(0,n.jsxs)("span",{children:[(0,n.jsx)(f,{}),"1.0"]}),links:[["arXiv","https://arxiv.org/abs/1904.06539"],["PDF","https://arxiv.org/pdf/1904.06539.pdf"],["Project","http://hake-mvig.cn"],["Code","https://github.com/DirtyHarryLYL/HAKE"],],content:(0,n.jsx)("table",{children:(0,n.jsxs)("tbody",{children:[(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{children:"Main Repo: "}),(0,n.jsxs)("td",{children:["HAKE ",(0,n.jsx)(s.Z,{href:"https://github.com/DirtyHarryLYL/HAKE"})]}),(0,n.jsx)("td",{})]}),(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{children:"Sub-repos: "}),(0,n.jsxs)("td",{children:["Torch ",(0,n.jsx)(s.Z,{href:"https://github.com/DirtyHarryLYL/HAKE-Action-Torch"})]}),(0,n.jsxs)("td",{children:["TF ",(0,n.jsx)(s.Z,{href:"https://github.com/DirtyHarryLYL/HAKE-Action"})]}),(0,n.jsxs)("td",{children:["HAKE-AVA ",(0,n.jsx)(s.Z,{href:"https://github.com/DirtyHarryLYL/HAKE-AVA"})]})]}),(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{}),(0,n.jsxs)("td",{children:["Halpe ",(0,n.jsx)(s.Z,{href:"https://github.com/Fang-Haoshu/Halpe-FullBody"})]}),(0,n.jsxs)("td",{children:["HOI List ",(0,n.jsx)(s.Z,{href:"https://github.com/DirtyHarryLYL/HOI-Learning-List"})]})]})]})})},{title:"HAKE: A Knowledge Engine Foundation for Human Activity Understanding",author:"Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Yizhuo Li, Zuoyu Qiu, Liang Xu, Yue Xu, Hao-Shu Fang, Cewu Lu",conf:"TPAMI 2023",shortname:(0,n.jsxs)("span",{children:[(0,n.jsx)(f,{}),"2.0"]}),links:[["arXiv","https://arxiv.org/abs/2202.06851"],["PDF","https://arxiv.org/pdf/2202.06851.pdf"],["Project","http://hake-mvig.cn"],["Code","https://github.com/DirtyHarryLYL/HAKE"],["Press","https://mp.weixin.qq.com/s/0KoPD7SAaaFKycmTUDBPOg"],]},{title:"AlphaPose: Whole-Body Regional Multi-Person Pose Estimation and Tracking in Real-Time",author:"Hao-Shu Fang*, Jiefeng Li*, Hongyang Tang, Chao Xu, Haoyi Zhu, Yuliang Xiu, Yong-Lu Li, Cewu Lu",conf:"TPAMI 2022",links:[["arXiv","https://arxiv.org/abs/2211.03375"],["PDF","https://arxiv.org/pdf/2211.03375.pdf"],["Code","https://github.com/MVIG-SJTU/AlphaPose"],],github:"https://github.com/MVIG-SJTU/AlphaPose"},{title:"Constructing Balance from Imbalance for Long-tailed Image Recognition",author:"Yue Xu*, Yong-Lu Li*, Jiefeng Li, Cewu Lu",conf:"ECCV 2022",shortname:"DLSA",links:[["arXiv","https://arxiv.org/abs/2208.02567"],["PDF","https://arxiv.org/pdf/2208.02567.pdf"],["Code","https://github.com/silicx/DLSA"],],github:"https://github.com/silicx/DLSA"},{title:"Mining Cross-Person Cues for Body-Part Interactiveness Learning in HOI Detection",author:"Xiaoqian Wu*, Yong-Lu Li*, Xinpeng Liu, Junyi Zhang, Yuzhe Wu, Cewu Lu",conf:"ECCV 2022",shortname:"PartMap",links:[["arXiv","https://arxiv.org/abs/2207.14192"],["PDF","https://arxiv.org/pdf/2207.14192v1.pdf"],["Code","https://github.com/enlighten0707/Body-Part-Map-for-Interactiveness"],],github:"https://github.com/enlighten0707/Body-Part-Map-for-Interactiveness"},{title:"Interactiveness Field of Human-Object Interactions",author:"Xinpeng Liu*, Yong-Lu Li*, Xiaoqian Wu, Yu-Wing Tai, Cewu Lu, Chi Keung Tang",conf:"CVPR 2022",links:[["arXiv","https://arxiv.org/abs/2204.07718"],["PDF","https://arxiv.org/pdf/2204.07718.pdf"],["Code","https://github.com/Foruck/Interactiveness-Field"],],github:"https://github.com/Foruck/Interactiveness-Field"},{title:"Human Trajectory Prediction with Momentary Observation",author:"Jianhua Sun, Yuxuan Li, Liang Chai, Hao-Shu Fang, Yong-Lu Li, Cewu Lu",conf:"CVPR 2022",links:[["PDF","https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_Human_Trajectory_Prediction_With_Momentary_Observation_CVPR_2022_paper.pdf",],]},{title:"Learn to Anticipate Future with Dynamic Context Removal",author:"Xinyu Xu, Yong-Lu Li, Cewu Lu",conf:"CVPR 2022",shortname:"DCR",links:[["arXiv","https://arxiv.org/abs/2204.02587"],["PDF","https://arxiv.org/pdf/2204.02587.pdf"],["Code","https://github.com/AllenXuuu/DCR"],],github:"https://github.com/AllenXuuu/DCR"},{title:"Canonical Voting: Towards Robust Oriented Bounding Box Detection in 3D Scenes",author:"Yang You, Zelin Ye, Yujing Lou, Chengkun Li, Yong-Lu Li, Lizhuang Ma, Weiming Wang, Cewu Lu",conf:"CVPR 2022",links:[["arXiv","https://arxiv.org/abs/2011.12001"],["PDF","https://arxiv.org/pdf/2011.12001.pdf"],["Code","https://github.com/qq456cvb/CanonicalVoting"],],github:"https://github.com/qq456cvb/CanonicalVoting"},{title:"UKPGAN: Unsupervised KeyPoint GANeration",author:"Yang You, Wenhai Liu, Yong-Lu Li, Weiming Wang, Cewu Lu",conf:"CVPR 2022",links:[["arXiv","https://arxiv.org/abs/2011.11974"],["PDF","https://arxiv.org/pdf/2011.11974.pdf"],["Code","https://github.com/qq456cvb/UKPGAN"],],github:"https://github.com/qq456cvb/UKPGAN"},{title:"Highlighting Object Category Immunity for the Generalization of Human-Object Interaction Detection",author:"Xinpeng Liu*, Yong-Lu Li*, Cewu Lu",conf:"AAAI 2022",links:[["arXiv","https://arxiv.org/abs/2202.09492"],["PDF","https://arxiv.org/pdf/2202.09492.pdf"],["Code","https://github.com/Foruck/OC-Immunity"],],github:"https://github.com/Foruck/OC-Immunity"},{title:"Learning Single/Multi-Attribute of Object with Symmetry and Group",author:"Yong-Lu Li, Yue Xu, Xinyu Xu, Xiaohan Mao, Cewu Lu",conf:"TPAMI 2021",shortname:"SymNet",links:[["arXiv","https://arxiv.org/abs/2110.04603"],["PDF","https://arxiv.org/pdf/2110.04603.pdf"],["Code","https://github.com/DirtyHarryLYL/SymNet"],],github:"https://github.com/DirtyHarryLYL/SymNet",content:"An extension of our CVPR 2020 work (Symmetry and Group in Attribute-Object Compositions, SymNet)."},{title:"Localization with Sampling-Argmax",author:"Jiefeng Li, Tong Chen, Ruiqi Shi, Yujing Lou, Yong-Lu Li, Cewu Lu",conf:"NeurIPS 2021",links:[["arXiv","https://arxiv.org/abs/2110.08825"],["PDF","https://arxiv.org/pdf/2110.08825.pdf"],["Code","https://github.com/Jeff-sjtu/sampling-argmax"],],github:"https://github.com/Jeff-sjtu/sampling-argmax"},{title:"Transferable Interactiveness Knowledge for Human-Object Interaction Detection",author:"Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Xijie Huang, Liang Xu, Cewu Lu",conf:"TPAMI 2021",shortname:"TIN++",links:[["arXiv","https://arxiv.org/abs/2101.10292"],["PDF","https://arxiv.org/pdf/2101.10292.pdf"],["Code","https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network"],],github:"https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network",content:"An extension of our CVPR 2019 work (Transferable Interactiveness Network, TIN)."},{title:"DecAug: Augmenting HOI Detection via Decomposition",author:"Yichen Xie, Hao-Shu Fang, Dian Shao, Yong-Lu Li, Cewu Lu",conf:"AAAI 2021",links:[["arXiv","https://arxiv.org/abs/2010.01007"],["PDF","https://arxiv.org/pdf/2010.01007.pdf"],]},{title:"HOI Analysis: Integrating and Decomposing Human-Object Interaction",author:"Yong-Lu Li*, Xinpeng Liu*, Xiaoqian Wu, Yizhuo Li, Cewu Lu",conf:"NeurIPS 2020",shortname:"IDN",links:[["arXiv","https://arxiv.org/abs/2010.16219"],["PDF","https://arxiv.org/pdf/2010.16219.pdf"],["Code","https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network)"],["Project: HAKE-Action-Torch","https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/master"],],github:"https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network)"},{title:"PaStaNet: Toward Human Activity Knowledge Engine",author:"Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu, Shiyi Wang, Hao-Shu Fang, Ze Ma, Mingyang Chen, Cewu Lu.",conf:"CVPR 2020",links:[["arXiv","https://arxiv.org/abs/2004.00945"],["PDF","https://arxiv.org/pdf/2004.00945.pdf"],["Video","https://drive.google.com/file/d/16PCCK_flK2qW4QJVWoYXQwG3wgXd6yvT/view?usp=sharing"],["Slides","https://drive.google.com/file/d/19J9uz3epBo3o9CIU85mzgLblRVNawl87/view?usp=sharing"],["Data","https://github.com/DirtyHarryLYL/HAKE"],["Code","https://github.com/DirtyHarryLYL/HAKE-Action"],],github:"https://github.com/DirtyHarryLYL/HAKE-Action",content:(0,n.jsxs)("p",{children:[(0,n.jsx)("strong",{className:"text-red-600",children:"Oral Talk:"})," ",(0,n.jsx)("a",{href:"http://ai.stanford.edu/~jingweij/cicv/#schedule",children:"Compositionality in Computer Vision"})," in CVPR 2020"]})},{title:"Detailed 2D-3D Joint Representation for Human-Object Interaction",author:"Yong-Lu Li, Xinpeng Liu, Han Lu, Shiyi Wang, Junqi Liu, Jiefeng Li, Cewu Lu",conf:"CVPR 2020",shortname:"DJ-RN",links:[["arXiv","https://arxiv.org/abs/2004.08154"],["PDF","https://arxiv.org/pdf/2004.08154.pdf"],["Video","https://drive.google.com/file/d/14dK1tBLe3xHXyO_5WsRO2JcjJ95meaDB/view?usp=sharing"],["Slides","https://drive.google.com/file/d/1A5bQZFsBOahj7dJgSnWR7jdyvMG58NkT/view?usp=sharing"],["Benchmark: Ambiguous-HOI","https://github.com/DirtyHarryLYL/DJ-RN#ambiguous-hoi"],["Code","https://github.com/DirtyHarryLYL/DJ-RN"],],github:"https://github.com/DirtyHarryLYL/DJ-RN"},{title:"Symmetry and Group in Attribute-Object Compositions",author:"Yong-Lu Li, Yue Xu, Xiaohan Mao, Cewu Lu",conf:"CVPR 2020",shortname:"SymNet",links:[["arXiv","https://arxiv.org/abs/2004.00587"],["PDF","https://arxiv.org/pdf/2004.00587.pdf"],["Video","https://drive.google.com/file/d/1ZTSB2lJbDTH7D-7GdEJQGmszvEc2Vuwd/view?usp=sharing"],["Slides","https://drive.google.com/file/d/1aqYeSIQkoTp1hYOJokDgoucdZcufN2iG/view?usp=sharing"],["Code","https://github.com/DirtyHarryLYL/SymNet"],],github:"https://github.com/DirtyHarryLYL/SymNet"},{title:"InstaBoost: Boosting Instance Segmentation Via Probability Map Guided Copy-Pasting",author:"Hao-Shu Fang*, Jianhua Sun*, Runzhong Wang*, Minghao Gou, Yong-Lu Li, Cewu Lu",conf:"ICCV 2019",links:[["arXiv","https://arxiv.org/abs/1908.07801"],["PDF","https://arxiv.org/pdf/1908.07801.pdf"],["Code","https://github.com/GothicAi/Instaboost"],],github:"https://github.com/GothicAi/Instaboost"},{title:"Transferable Interactiveness Knowledge for Human-Object Interaction Detection",author:"Yong-Lu Li, Siyuan Zhou, Xijie Huang, Liang Xu, Ze Ma, Hao-Shu Fang, Yan-Feng Wang, Cewu Lu",conf:"CVPR 2019",shortname:"TIN",links:[["arXiv","https://arxiv.org/abs/1811.08264"],["PDF","https://arxiv.org/pdf/1811.08264.pdf"],["Code","https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network"],],github:"https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network"},{title:"SRDA: Generating Instance Segmentation Annotation via Scanning, Reasoning and Domain Adaptation",author:"Wenqiang Xu*, Yong-Lu Li*, Cewu Lu",conf:"ECCV 2018",links:[["arXiv","https://arxiv.org/abs/1801.08839"],["PDF","https://arxiv.org/pdf/1801.08839.pdf"],["Dataset (Instance-60k & 3D Object Models)","https://drive.google.com/drive/folders/1t941oiLk40XQX2Q9a2HPmiDRUpiwazJO?usp=sharing",],["Code","https://github.com/DirtyHarryLYL/SRDA-ECCV2018"],],github:"https://github.com/DirtyHarryLYL/SRDA-ECCV2018"},{title:"Beyond Holistic Object Recognition: Enriching Image Understanding with Part States",author:"Cewu Lu, Hao Su, Yong-Lu Li, Yongyi Lu, Li Yi, Chi-Keung Tang, Leonidas J. Guibas",conf:"CVPR 2018",links:[["PDF","http://ai.ucsd.edu/~haosu/papers/cvpr18_partstate.pdf"]]},{title:"Optimization of Radial Distortion Self-Calibration for Structure from Motion from Uncalibrated UAV Images",author:"Yong-Lu Li, Yinghao Cai, Dayong Wen, Yiping Yang",conf:"ICPR 2016",links:[["PDF","https://projet.liris.cnrs.fr/imagine/pub/proceedings/ICPR-2016/media/files/1010.pdf"]]},],S=[{name:"Yong-Lu Li",position:"Assistant Professor",url:"https://dirtyharrylyl.github.io/"},{name:"Cewu Lu",position:"Professor",url:"https://www.mvig.org/"},{name:"Xinpeng Liu",position:"PhD. Student",url:"https://foruck.github.io/"},{name:"Yue Xu",position:"PhD. Student",url:"https://silicx.github.io/"},{name:"Xiaoqian Wu",position:"PhD. Student",url:"https://scholar.google.com/citations?user=-PHR96oAAAAJ"},{name:"Siqi Liu",position:"PhD. Student",url:"https://github.com/MayuOshima/"},{name:"Hong Li",position:"PhD. Student",url:"https://scholar.google.com/citations?user=fhDk2-wAAAAJ&hl=zh-CN"},{name:"Zehao Wang",position:"Ph.D. Student",altimage:"placeholder.png",url:""},{name:"Zixuan Chen",position:"Ph.D. Student",url:""},{name:"Boran Wen",position:"Ph.D. Student",url:"https://scholar.google.com/citations?user=8KnzXWUAAAAJ&hl=zh-CN"},{name:"Mingyu Zhang",position:"Ph.D. Student",url:"https://scholar.google.com/citations?user=YyWddPsAAAAJ&hl=en"},{name:"Jiaxin Li",position:"Ph.D. Student",url:""},{name:"Xianchao Zeng",position:"Ph.D. Student (SII)",url:""},{name:"Weixi Song",position:"Ph.D. Student (SII)",url:"https://scholar.google.com.hk/citations?user=fvP8SGcAAAAJ&hl=zh-CN"},{name:"Xinyu Zhou",position:"Ph.D. Student (SII)",url:""},{name:"Yusong Qiu",position:"Master Student",altimage:"placeholder.png",url:""},{name:"Yushun Xiang",position:"Master Student",url:"https://yushunxiang.github.io"},{name:"Chenyang Yu",position:"Master Student",altimage:"placeholder.png"},{name:"Ziyu Wang",position:"Master Student",url:"https://scholar.google.com/citations?user=BsM3ggwAAAAJ&hl=en"},],k=[{name:"Junxuan Liang",position:"CMU, Intern",url:""},{name:"Dingbang Huang",position:"UT Austin, Intern",url:""},{name:"Xin Li",position:"Shanghai AI Lab, Researcher",url:""},{name:"Jingru Tan",position:"Central South University, Associate Professor",url:"https://scholar.google.com/citations?user=l18d7kcAAAAJ&hl=en"},{name:"Yude Zou",position:"Westlake University, Ph.D.",url:""},{name:"Yejie Guo",position:"UCMerced, Intern",url:""},{name:"Quanquan Peng",position:"UW, Intern -> USCD, Ph.D.",url:"https://bariona.github.io/"},{name:"Zizheng Zhou",position:"UC Merced, Intern -> CMU, MS",url:"https://scholar.google.com/citations?user=h_UN0qUAAAAJ&hl=en"},{name:"Jiting Cai",position:"UMass, Intern -> CMU, MS",url:"https://scholar.google.com/citations?user=vhagIp4AAAAJ&hl=zh-CN"},{name:"Yuyang Zhang",position:"EIAS & SJTU, Ph.D.",url:""},{name:"Zhilin Lin",position:"SJTU, Ph.D.",url:""},{name:"Zili Lin",position:"EIAS & SJTU, Ph.D.",url:""},{name:"Yifan Shi",position:"EIAS & SJTU, Ph.D.",url:""},{name:"Haowen Hou",position:"UCSD, Intern -> SJTU, Ph.D.",url:"https://haowenhou.github.io/"},{name:"Yixing Li",position:"CUHK, Ph.D.",url:"https://scholar.google.com/citations?user=V8FcspEAAAAJ&hl=en"},{name:"Mingyu Liu",position:"ZJU, Ph.D.",url:"https://mingyulau.github.io/"},{name:"Kaitong Cui",position:"HKU, Intern",url:""},{name:"Junyi Zhang",position:"UC Merced+Google Intern -> UCB, Ph.D.",url:"https://www.junyi42.com/"},{name:"Yiming Dou",position:"Stanford, Intern -> UMich, Ph.D. -> Cornell, Ph.D.",url:"https://dou-yiming.github.io/"},{name:"Xiaohan Mao",position:"Shanghai AI Lab & SJTU, Ph.D.",url:"https://scholar.google.com/citations?user=-zT1NKwAAAAJ"},{name:"Zhemin Huang",position:"Stanford, MS"},{name:"Yuzhe Wu",position:""},{name:"Shaopeng Guo",position:"HKUST, Intern -> UCSD, Ph.D.",url:"https://coeusguo.github.io/"},{name:"Xudong Lu",position:"CUHK, Ph.D.",url:"https://lucky-lance.github.io"},{name:"Hongwei Fan",position:"SenseTime, Research Engineer -> Peking University, Ph.D.",url:"https://hwfan.io/about-me/"},{name:"Yuan Yao",position:"Johns Hopkins, Intern -> U of Rochester, Ph.D.",url:"https://www.cs.rochester.edu/u/yyao39/#research"},{name:"Zuoyu Qiu",position:"SJTU, MS"},{name:"Shiyi Wang",position:"Flexiv"},{name:"Junqi Liu",position:""},{name:"Han Lu",position:"SJTU, Ph.D."},{name:"Zhanke Zhou",position:"HKBU, Ph.D."},{name:"Mingyang Chen",position:"UCSD, MS"},{name:"Liang Xu",position:"EIAS & SJTU, Ph.D.",url:"https://liangxuy.github.io/"},{name:"Ze Ma",position:"UCB, Intern -> Columbia University, MS",url:"https://maqingyang.github.io/"},{name:"Xijie Huang",position:"HKUST, Ph.D.",url:"https://huangowen.github.io/"},]}}]);