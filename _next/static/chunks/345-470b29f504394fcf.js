"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[345],{9770:function(e,t,i){var n=i(5893),r=i(9008),a=i.n(r),o=i(1163),s=(0,i(7294).memo)(function(e){var t=e.children,i=e.title,r=e.description,s=(0,o.useRouter)().asPath;return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(a(),{children:[(0,n.jsx)("title",{children:i}),(0,n.jsx)("meta",{content:r,name:"description"}),(0,n.jsx)("link",{href:"https://http://mvig-rhos.com".concat(s),rel:"canonical"},"canonical"),(0,n.jsx)("link",{href:"/favicon.ico",rel:"icon",sizes:"any"}),(0,n.jsx)("link",{href:"/icon.svg",rel:"icon",type:"image/svg+xml"}),(0,n.jsx)("link",{href:"/apple-touch-icon.png",rel:"apple-touch-icon"}),(0,n.jsx)("link",{href:"/site.webmanifest",rel:"manifest"}),(0,n.jsx)("meta",{content:i,property:"og:title"}),(0,n.jsx)("meta",{content:r,property:"og:description"}),(0,n.jsx)("meta",{content:"https://http://mvig-rhos.com".concat(s),property:"og:url"}),(0,n.jsx)("meta",{content:i,name:"twitter:title"}),(0,n.jsx)("meta",{content:r,name:"twitter:description"})]}),t]})});s.displayName="Page",t.Z=s},3031:function(e,t,i){var n=i(5893),r=i(4184),a=i.n(r),o=(0,i(7294).memo)(function(e){var t=e.children,i=e.sectionId,r=e.noPadding,o=void 0!==r&&r,s=e.className;return(0,n.jsx)("section",{className:a()(s,{"px-4 py-8 md:py-12 lg:px-8":!o}),id:i,children:(0,n.jsx)("div",{className:a()({"mx-auto max-w-screen-lg":!o}),children:t})})});o.displayName="Section",t.Z=o},11:function(e,t,i){var n=i(5893),r=(0,i(7294).memo)(function(e){var t=e.title,i=e.children;return(0,n.jsxs)("div",{className:"grid grid-cols-1 gap-y-4 py-8 first:pt-0 last:pb-0 md:grid-cols-4",children:[(0,n.jsx)("div",{className:"col-span-1 flex justify-center md:justify-start",children:(0,n.jsxs)("div",{className:"relative h-max",children:[(0,n.jsx)("h2",{className:"text-xl font-bold uppercase text-neutral-800",children:t}),(0,n.jsx)("span",{className:"absolute inset-x-0 -bottom-1 border-b-2 border-orange-400"})]})}),(0,n.jsx)("div",{className:"col-span-1 flex flex-col md:col-span-3",children:i})]})});r.displayName="TitledSection",t.Z=r},6571:function(e,t,i){var n=i(5893),r=i(822),a=i(7294),o=i(5414),s=(0,a.memo)(function(){return(0,n.jsxs)("div",{className:"relative bg-neutral-900 px-4 pb-6 pt-12 sm:px-8 sm:pt-14 sm:pb-8",children:[(0,n.jsx)("div",{className:"absolute inset-x-0 -top-4 flex justify-center sm:-top-6",children:(0,n.jsx)("a",{className:"rounded-full bg-neutral-100 p-1 ring-white ring-offset-2 ring-offset-gray-700/80 focus:outline-none focus:ring-2 sm:p-2",href:"/#".concat(o._h.Hero),children:(0,n.jsx)(r.Z,{className:"h-6 w-6 bg-transparent sm:h-8 sm:w-8"})})}),(0,n.jsxs)("div",{className:"flex flex-col items-center gap-y-6",children:[(0,n.jsx)("div",{id:"pageview-script",className:"text-sm text-neutral-700"}),(0,n.jsxs)("span",{className:"text-sm text-neutral-700",children:["\xa9 Copyright 2022 MVIG-RHOS • Based on"," ",(0,n.jsx)("a",{href:"https://github.com/tbakerx/react-resume-template",children:"tbakerx"})]})]})]})});s.displayName="Footer",t.Z=s},5414:function(e,t,i){i.d(t,{J6:function(){return y},_h:function(){return L},QL:function(){return w},IX:function(){return Y},oM:function(){return C},J8:function(){return v},uu:function(){return H},Vf:function(){return k},mZ:function(){return I},q:function(){return P},Po:function(){return S},c$:function(){return D}});var n=i(5893),r=i(5966),a=i(6339),o=i(7402),s=i(1438),c=i(2951),h=i(8029),l=i(6567),u=i(7294),g=i(2719),d=function(e){(0,h.Z)(i,e);var t=(0,l.Z)(i);function i(){return(0,s.Z)(this,i),t.apply(this,arguments)}return(0,c.Z)(i,[{key:"render",value:function(){return this.props.href?(0,n.jsx)(g.Z,{"aria-label":"Star buttons/github-buttons on GitHub","data-show-count":"true",href:this.props.href,children:"Star"}):null}}]),i}(u.Component),p=i(1799),m=i(9396),A=i(9534),b=(0,u.memo)(function(e){var t=e.children,i=e.className,r=e.svgRef,a=e.transform,o=(0,A.Z)(e,["children","className","svgRef","transform"]);return(0,n.jsx)("svg",(0,m.Z)((0,p.Z)({className:i,fill:"currentColor",ref:r,transform:a,viewBox:"0 0 128 128",width:"128",xmlns:"http://www.w3.org/2000/svg"},o),{children:t}))}),x=(0,u.memo)(function(e){return(0,n.jsxs)(b,(0,m.Z)((0,p.Z)({},e),{children:[(0,n.jsx)("path",{clipRule:"evenodd",d:"M64 5.103c-33.347 0-60.388 27.035-60.388 60.388 0 26.682 17.303 49.317 41.297 57.303 3.017.56 4.125-1.31 4.125-2.905 0-1.44-.056-6.197-.082-11.243-16.8 3.653-20.345-7.125-20.345-7.125-2.747-6.98-6.705-8.836-6.705-8.836-5.48-3.748.413-3.67.413-3.67 6.063.425 9.257 6.223 9.257 6.223 5.386 9.23 14.127 6.562 17.573 5.02.542-3.903 2.107-6.568 3.834-8.076-13.413-1.525-27.514-6.704-27.514-29.843 0-6.593 2.36-11.98 6.223-16.21-.628-1.52-2.695-7.662.584-15.98 0 0 5.07-1.623 16.61 6.19C53.7 35 58.867 34.327 64 34.304c5.13.023 10.3.694 15.127 2.033 11.526-7.813 16.59-6.19 16.59-6.19 3.287 8.317 1.22 14.46.593 15.98 3.872 4.23 6.215 9.617 6.215 16.21 0 23.194-14.127 28.3-27.574 29.796 2.167 1.874 4.097 5.55 4.097 11.183 0 8.08-.07 14.583-.07 16.572 0 1.607 1.088 3.49 4.148 2.897 23.98-7.994 41.263-30.622 41.263-57.294C124.388 32.14 97.35 5.104 64 5.104z",fillRule:"evenodd"}),(0,n.jsx)("path",{d:"M26.484 91.806c-.133.3-.605.39-1.035.185-.44-.196-.685-.605-.543-.906.13-.31.603-.395 1.04-.188.44.197.69.61.537.91zm2.446 2.729c-.287.267-.85.143-1.232-.28-.396-.42-.47-.983-.177-1.254.298-.266.844-.14 1.24.28.394.426.472.984.17 1.255zM31.312 98.012c-.37.258-.976.017-1.35-.52-.37-.538-.37-1.183.01-1.44.373-.258.97-.025 1.35.507.368.545.368 1.19-.01 1.452zm3.261 3.361c-.33.365-1.036.267-1.552-.23-.527-.487-.674-1.18-.343-1.544.336-.366 1.045-.264 1.564.23.527.486.686 1.18.333 1.543zm4.5 1.951c-.147.473-.825.688-1.51.486-.683-.207-1.13-.76-.99-1.238.14-.477.823-.7 1.512-.485.683.206 1.13.756.988 1.237zm4.943.361c.017.498-.563.91-1.28.92-.723.017-1.308-.387-1.315-.877 0-.503.568-.91 1.29-.924.717-.013 1.306.387 1.306.88zm4.598-.782c.086.485-.413.984-1.126 1.117-.7.13-1.35-.172-1.44-.653-.086-.498.422-.997 1.122-1.126.714-.123 1.354.17 1.444.663zm0 0"})]}))}),f=function(e){(0,h.Z)(i,e);var t=(0,l.Z)(i);function i(){return(0,s.Z)(this,i),t.apply(this,arguments)}return(0,c.Z)(i,[{key:"render",value:function(){return(0,n.jsxs)("a",{href:"/hake",children:[(0,n.jsxs)("b",{children:[(0,n.jsx)("span",{style:{color:"red"},children:"H"}),(0,n.jsx)("span",{style:{color:"blue"},children:"A"}),(0,n.jsx)("span",{style:{color:"red"},children:"KE"})]}),this.props.children]})}}]),i}(u.Component),j=(0,u.memo)(function(e){return(0,n.jsx)(b,(0,m.Z)((0,p.Z)({},e),{children:(0,n.jsx)("path",{d:"M116 3H12a8.91 8.91 0 00-9 8.8v104.42a8.91 8.91 0 009 8.78h104a8.93 8.93 0 009-8.81V11.77A8.93 8.93 0 00116 3zM39.17 107H21.06V48.73h18.11zm-9-66.21a10.5 10.5 0 1110.49-10.5 10.5 10.5 0 01-10.54 10.48zM107 107H88.89V78.65c0-6.75-.12-15.44-9.41-15.44s-10.87 7.36-10.87 15V107H50.53V48.73h17.36v8h.24c2.42-4.58 8.32-9.41 17.13-9.41C103.6 47.28 107 59.35 107 75z",fill:"currentColor"})}))}),v={title:"RHOS",description:"Homepage of RHOS"},L={Hero:"hero",About:"about",Recruit:"recruit",News:"news",Projects:"projects",Portfolio:"portfolio",Publications:"publications",People:"people"},y=[L.About,L.News,L.Projects,L.Publications,L.People],C={imageSrc:{src:"/_next/static/media/Robotics_Cyberpunk_2077_bg.f50d6e01.png",height:1125,width:2e3,blurDataURL:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAIAAAD38zoCAAAAhUlEQVR42mPw8DGPDLMz0ZMzs7OMTwpVNTAI8LayN1dikDXTUbUx9nDQD/K01jHSUzOyNrd1NjbUZVDQkbGw1zO1NXJ1srS2MjKysLK0tPEOiWCwMZVJ8FUvTbToKLSfUe9eHKXhbCrEIynL4GuvVJLiUp/jVZVqmxWqGuggpaciLCDACQABSR1UPIEuQgAAAABJRU5ErkJggg==",blurWidth:8,blurHeight:5},name:"RHOS",description:(0,n.jsx)(n.Fragment,{children:"Robot • Human • Object • Scene"}),actions:[{href:"#".concat(L.Recruit),text:"Recruit",primary:!0},{href:"https://github.com/mvig-rhos",text:"Github",primary:!1,Icon:x},]},w={profileImageSrc:{src:"/_next/static/media/profilepic.80e868bc.jpg",height:294,width:294,blurDataURL:"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAgACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABAEBAQAAAAAAAAAAAAAAAAAAAwT/2gAMAwEAAhADEAAAAKxSH//EABsQAAIDAAMAAAAAAAAAAAAAAAECAwQRAAUh/9oACAEBAAE/AGs9hWWuDIzGTUkjw6rH0c//xAAZEQEAAgMAAAAAAAAAAAAAAAAhAAECAwT/2gAIAQIBAT8Aw6dlItk//8QAGhEBAAEFAAAAAAAAAAAAAAAAASEAAwQxMv/aAAgBAwEBPwBxraHURuv/2Q==",blurWidth:8,blurHeight:8},aboutWhat:"RHOS",description:(0,n.jsxs)("div",{children:[(0,n.jsxs)("p",{children:["Hi, this is the website of RHOS team at"," ",(0,n.jsx)("a",{className:"font-bold text-white",href:"https://www.mvig.org/",children:"MVIG"}),". We study"," ",(0,n.jsx)("i",{children:(0,n.jsx)("b",{children:"Human Activity Understanding"})}),","," ",(0,n.jsx)("i",{children:(0,n.jsx)("b",{children:"Visual Reasoning"})}),", and"," ",(0,n.jsx)("i",{children:(0,n.jsx)("b",{children:"Embodied AI"})}),". We are building a knowledge-driven system that enables intelligent agents to perceive human activities, reason human behavior logics, learn skills from human activities, and interact with environment."]}),(0,n.jsx)("p",{children:(0,n.jsx)("b",{children:"Research Interests: "})}),(0,n.jsxs)("p",{children:["(S) ",(0,n.jsx)("b",{children:"Embodied AI"}),": how to make agents learn skills from humans and interact with humans.",(0,n.jsx)("br",{}),"(S-1) ",(0,n.jsx)("b",{children:"Human Activity Understanding"}),": how to learn and ground complex/ambiguous human activity concepts (body motion, human-object/human/scene interaction) and object concepts from multi-modal information (2D-3D-4D).",(0,n.jsx)("br",{}),"(S-2) ",(0,n.jsx)("b",{children:"Visual Reasoning"}),": how to mine, capture, and embed the logics and causal relations from human activities.",(0,n.jsx)("br",{}),"(S-3) ",(0,n.jsx)("b",{children:"General Multi-Modal Foundation Models"}),": especially for human-centric perception tasks.",(0,n.jsx)("br",{}),"(S-4) ",(0,n.jsx)("b",{children:"Activity Understanding from A Cognitive Perspective"}),": work with multidisciplinary researchers to study how the brain perceives activities.",(0,n.jsx)("br",{}),"(E) ",(0,n.jsx)("b",{children:"Human-Robot Interaction for Smart Hospital"}),": work with the healthcare team (doctors and engineers) in SJTU to develop intelligent robots to help people."]})]}),contactDetail:(0,n.jsxs)("div",{children:[(0,n.jsx)("b",{children:"Yong-Lu Li"}),(0,n.jsx)("br",{}),"Email: yonglu_li[at]sjtu[dot]edu[dot]cn",(0,n.jsx)("br",{}),"Office: SEIEE-3-301",(0,n.jsx)("br",{}),"Shanghai Jiao Tong University"]}),contactLinkItems:[{label:"Personal Website",link:"https://dirtyharrylyl.github.io/",Icon:r.Z,newline:!0},{label:"Google Scholar ",link:"https://scholar.google.com.hk/citations?user=UExAaVgAAAAJ",Icon:a.Z},{label:"Github ",link:"https://github.com/DirtyHarryLYL",Icon:x},{label:"LinkedIn",link:"https://www.linkedin.com/in/%E6%B0%B8%E9%9C%B2-%E6%9D%8E-991b99139/",Icon:j},{label:"dblp",link:"https://dblp.org/pid/198/9345.html",Icon:o.Z},{label:"Semantic Scholar",link:"https://www.semanticscholar.org/author/Yong-Lu-Li/10384643",Icon:o.Z},]},D=(0,n.jsxs)("span",{children:[(0,n.jsxs)("p",{children:["We are actively looking for self-motivated"," ",(0,n.jsxs)("strong",{children:["students (master/",(0,n.jsx)("del",{children:"PhD"}),", 2023 fall), interns / engineers / visitors"]})," ","(CV/ML/ROB/NLP background, always welcome) to join us in"," ",(0,n.jsx)("a",{className:"text-red600",href:"https://www.mvig.org/",children:"Machine Vision and Intelligence Group (MVIG)"}),". If you share same/similar interests, feel free to drop me an email with your resume."]}),(0,n.jsxs)("p",{children:["Click"," ",(0,n.jsx)("a",{className:"text-red-600",href:"https://dirtyharrylyl.github.io/recruit.html",children:(0,n.jsx)("b",{children:"here"})})," ","for more details."]})]}),H=[["2022.11",(0,n.jsxs)(n.Fragment,{children:["We release the human body part states and interactive object bounding box annotations upon AVA (2.1 & 2.2): ",(0,n.jsx)("a",{className:"font-bold",href:"https://github.com/DirtyHarryLYL/HAKE-AVA",children:"[HAKE-AVA]"}),", and a CLIP-based human part state & verb recognizer: ",(0,n.jsx)("a",{className:"font-bold",href:"https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/CLIP-Activity2Vec",children:"[CLIP-Activity2Vec]"}),"."]}),],["2022.11",(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)("a",{className:"font-bold",href:"https://github.com/MVIG-SJTU/AlphaPose",children:"AlphaPose"})," will appear at TPAMI!"]}),],["2022.10",(0,n.jsxs)(n.Fragment,{children:["Honored to be a ",(0,n.jsx)("a",{className:"font-bold",href:"https://neurips.cc/Conferences/2022/ProgramCommittee",children:"top reviewer in NeurIPS'22"}),"!"]}),],["2022.09",(0,n.jsxs)(n.Fragment,{children:["Joined ",(0,n.jsx)("a",{className:"font-bold",href:"https://www.sjtu.edu.cn/",children:"SJTU"})," as a tenure-track assistant professor."]}),],["2022.07",(0,n.jsxs)(n.Fragment,{children:["Two papers on ",(0,n.jsx)("b",{children:"longtailed learning, HOI detection"})," are accepted by ECCV'22, arXivs and code are coming soon"]}),],["2022.03",(0,n.jsxs)(n.Fragment,{children:["Five papers on ",(0,n.jsx)("b",{children:"HOI detection/prediction, trajection prediction, 3D detection/keypoints"})," are accepted by CVPR'22, papers and code are coming soon."]}),],["2022.02",(0,n.jsxs)(n.Fragment,{children:["We release the human body part state labels based on AVA:"," ",(0,n.jsx)("a",{className:"font-bold",href:"https://github.com/DirtyHarryLYL/HAKE-AVA",children:"HAKE-AVA"})," and ",(0,n.jsx)("a",{className:"font-bold",href:"https://arxiv.org/abs/2202.06851",children:"HAKE 2.0"}),"."]}),],["2021.12",(0,n.jsxs)(n.Fragment,{children:["Our work on ",(0,n.jsx)("b",{children:"HOI generalization"})," will appear at AAAI'22."]}),],["2021.10",(0,n.jsxs)(n.Fragment,{children:["Yong-Lu recieves ",(0,n.jsx)("b",{children:"Outstanding Reviewer Award"})," from NeurIPS'21."]}),],["2021.10",(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)("b",{children:"Learning Single/Multi-Attribute of Object with Symmetry and Group"})," is accepted by TPAMI."]}),],["2021.09",(0,n.jsxs)(n.Fragment,{children:["Our work ",(0,n.jsx)("b",{children:"Localization with Sampling-Argmax"})," will appear at NeurIPS'21."]}),],["2021.05",(0,n.jsxs)(n.Fragment,{children:["Yong-Lu is selected as the"," ",(0,n.jsx)("a",{className:"font-bold",href:"https://mp.weixin.qq.com/s/v7ITiZXOJiDUbPlRlcqQRA",children:"Chinese AI New Star Top-100 (Machine Learning)"})]}),],["2021.02",(0,n.jsxs)(n.Fragment,{children:["Upgraded"," ",(0,n.jsx)("a",{className:"font-bold",href:"https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/Activity2Vec",children:"HAKE-Activity2Vec"})," ","is released! Images/Videos --> human box + ID + skeleton + part states + action + representation."," ",(0,n.jsx)("a",{className:"underline",href:"https://youtu.be/ty-bXDInLMQ",children:"[Demo]"})," ",(0,n.jsx)("a",{className:"underline",href:"https://drive.google.com/file/d/1iZ57hKjus2lKbv1MAB-TLFrChSoWGD5e/view?usp=sharing",children:"[Description]"})]}),],["2021.01",(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)("b",{children:[(0,n.jsx)("a",{href:"https://arxiv.org/abs/2101.10292",children:"TIN"})," (Transferable Interactiveness Network)"]})," ","is accepted by TPAMI."]}),],["2021.01",(0,n.jsxs)(n.Fragment,{children:["Recieved"," ",(0,n.jsx)("a",{className:"font-bold",href:"http://scholarship.baidu.com/",children:"Baidu Scholarship"})," ","(10 recipients globally)."]}),],["2020.12",(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)("a",{className:"font-bold underline",href:"https://arxiv.org/abs/2010.01007",children:"DecAug"})," ","is accepted by AAAI'21."]}),],["2020.09",(0,n.jsxs)(n.Fragment,{children:["Our work ",(0,n.jsx)("b",{children:"HOI Analysis"})," will appear at NeurIPS 2020."]}),],["2020.07",(0,n.jsxs)(n.Fragment,{children:["Yong-Lu recieves"," ",(0,n.jsx)("a",{className:"font-bold",href:"https://www.thepaper.cn/newsDetail_forward_8240318",children:"WAIC YunFan Award"})," ","and be among the 2nd A-Class Project."]}),],["2020.06",(0,n.jsxs)(n.Fragment,{children:["The larger"," ",(0,n.jsx)("a",{className:"font-bold",href:"https://github.com/DirtyHarryLYL/HAKE#hake-large-for-instance-level-hoi-detection",children:"HAKE-Large"})," ","(>120K images with activity and part state labels) is released."]}),],["2020.02",(0,n.jsxs)(n.Fragment,{children:["Three papers ",(0,n.jsx)("b",{children:"Image-based HAKE: PaSta-Net"}),", ",(0,n.jsx)("b",{children:"2D-3D Joint HOI Learning"}),","," ",(0,n.jsx)("b",{children:"Symmetry-based Attribute-Object Learning"})," are accepted in ",(0,n.jsx)("a",{href:"http://cvpr2020.thecvf.com/",children:"CVPR'20"}),"! Papers and corresponding resources (code, data) will be released soon."]}),],["2019.07",(0,n.jsxs)(n.Fragment,{children:["Our paper ",(0,n.jsx)("b",{children:"InstaBoost"})," is accepted in ",(0,n.jsx)("a",{href:"http://iccv2019.thecvf.com/",children:"ICCV'19"}),"."]}),],["2019.06",(0,n.jsxs)(n.Fragment,{children:["The Part I of our ",(0,n.jsx)(f,{}),":"," ",(0,n.jsx)("b",{children:(0,n.jsx)("a",{href:"http://hake-mvig.cn/download/",children:"HAKE-HICO"})})," ","which contains the image-level part-state annotations is released."]}),],["2019.04",(0,n.jsxs)(n.Fragment,{children:["Our project ",(0,n.jsx)(f,{})," (Human Activity Knowledge Engine) begins trial operation."]}),],["2019.02",(0,n.jsxs)(n.Fragment,{children:["Our paper on"," ",(0,n.jsx)("b",{children:(0,n.jsx)("a",{href:"https://arxiv.org/abs/1811.08264",children:"Interactiveness"})})," ","is accepted in ",(0,n.jsx)("a",{href:"http://cvpr2019.thecvf.com/",children:"CVPR'19"}),"."]}),],["2018.07",(0,n.jsxs)(n.Fragment,{children:["Our paper on"," ",(0,n.jsx)("b",{children:(0,n.jsx)("a",{href:"https://arxiv.org/abs/1801.08839",children:"GAN & Annotation Generation"})})," ","is accepted in ",(0,n.jsx)("a",{href:"https://eccv2018.org/",children:"ECCV'18"}),"."]}),],["2018.05",(0,n.jsxs)(n.Fragment,{children:["Presentation (Kaibot Team) in"," ",(0,n.jsx)("a",{href:"https://icra2018.org/tidy-up-my-room-challenge/",children:"TIDY UP MY ROOM CHALLENGE | ICRA'18"}),"."]}),],["2018.02",(0,n.jsxs)(n.Fragment,{children:["Our paper on"," ",(0,n.jsx)("b",{children:(0,n.jsx)("a",{href:"http://ai.ucsd.edu/~haosu/papers/cvpr18_partstate.pdf",children:"Object Part States"})})," ","is accepted in ",(0,n.jsx)("a",{href:"http://cvpr2018.thecvf.com/program/main_conference",children:"CVPR'18"}),"."]}),],],I=[{title:"HAKE 2.0",description:"",url:"http://hake-mvig.cn/home",image:"2022_hake2.0.jpg"},{title:"PartMap",description:"",url:"https://github.com/enlighten0707/Body-Part-Map-for-Interactiveness",image:"2022_ECCV_partmap.jpg"},{title:"DLSA",description:"",url:"https://github.com/silicx/DLSA",image:"2022_ECCV_longtail.jpg"},{title:"Interactiveness-Field",description:"",url:"https://github.com/Foruck/Interactiveness-Field",image:"2022_CVPR_InteractivenessField.jpg"},{title:"DCR",description:"",url:"https://github.com/AllenXuuu/DCR",image:"2022_CVPR_anticipate.jpg"},{title:"OC-Immunity",description:"",url:"https://github.com/Foruck/OC-Immunity",image:"2022_AAAI_hoi.jpg"},{title:"TIN & TIN++",description:"",url:"https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network",image:"2021_TPAMI_TIN.jpg"},{title:"SymNet",description:"Attribute detector based on symmetry and group",url:"https://github.com/DirtyHarryLYL/SymNet",image:"2021_TPAMI_SymNet.jpg"},{title:"HOI Analysis",description:"",url:"https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network)",image:"2020_NeurIPS_analysis.jpg"},{title:"PaStaNet",description:"",url:"https://github.com/DirtyHarryLYL/HAKE-Action",image:"2020_CVPR_pastanet.jpg"},{title:"DJ-RN",description:"",url:"https://github.com/DirtyHarryLYL/DJ-RN",image:"2020_CVPR_djrn.jpg"},{title:"HAKE 1.0",description:"",url:"http://hake-mvig.cn/home",image:"2019_hake1.jpg"},],P=[{date:"2018",location:"Human Activity Knowledge Engine",title:"HAKE",link:"http://hake-mvig.cn/home/",content:(0,n.jsxs)("p",{children:["Human Activity Knowledge Engine (",(0,n.jsx)(f,{}),") is a knowledge-driven system that aims at enabling intelligent agents to perceive human activities, reason human behavior logics, learn skills from human activities, and interact with objects and environments."]})},{date:"2022",location:"Object Concept Learning",title:"OCL",link:"/ocl",content:(0,n.jsx)("p",{children:"We propose a challenging Object Concept Learning (OCL) task to push the envelope of object understanding. It requires machines to reason out object affordances and simultaneously give the reason: what attributes make an object possesses these affordances."})},],S=[{title:"Discovering A Variety of Objects in Spatio-Temporal Human-Object Interactions",author:"Yong-Lu Li*, Hongwei Fan*, Zuoyu Qiu, Yiming Dou, Liang Xu, Hao-Shu Fang, Peiyang Guo, Haisheng Su, Dongliang Wang, Wei Wu, Cewu Lu (*=equal contribution)",conf:"Tech Report",links:[["arXiv","https://arxiv.org/abs/2211.07501"],["PDF","https://arxiv.org/pdf/2211.07501.pdf"],["Code & Data","https://github.com/DirtyHarryLYL/HAKE-AVA"],],content:"A part of the HAKE Project"},{title:"HAKE: A Knowledge Engine Foundation for Human Activity Understanding",author:"Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Yizhuo Li, Zuoyu Qiu, Liang Xu, Yue Xu, Hao-Shu Fang, Cewu Lu",conf:"Preprint",shortname:(0,n.jsxs)("span",{children:[(0,n.jsx)(f,{}),"2.0"]}),links:[["arXiv","https://arxiv.org/abs/2202.06851"],["PDF","https://arxiv.org/pdf/2202.06851.pdf"],["Project","http://hake-mvig.cn"],["Press","https://mp.weixin.qq.com/s/0KoPD7SAaaFKycmTUDBPOg"],]},{title:"HAKE: Human Activity Knowledge Engine",author:"Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu, Mingyang Chen, Ze Ma, Shiyi Wang, Hao-Shu Fang, Cewu Lu",conf:"Tech Report",shortname:(0,n.jsxs)("span",{children:[(0,n.jsx)(f,{}),"1.0"]}),links:[["arXiv","https://arxiv.org/abs/1904.06539"],["PDF","https://arxiv.org/pdf/1904.06539.pdf"],["Project","http://hake-mvig.cn"],["Code","https://github.com/DirtyHarryLYL/HAKE"],],content:(0,n.jsx)("table",{children:(0,n.jsxs)("tbody",{children:[(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{children:"Main Repo: "}),(0,n.jsxs)("td",{children:["HAKE ",(0,n.jsx)(d,{href:"https://github.com/DirtyHarryLYL/HAKE"})]}),(0,n.jsx)("td",{})]}),(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{children:"Sub-repos: "}),(0,n.jsxs)("td",{children:["Torch ",(0,n.jsx)(d,{href:"https://github.com/DirtyHarryLYL/HAKE-Action-Torch"})]}),(0,n.jsxs)("td",{children:["TF ",(0,n.jsx)(d,{href:"https://github.com/DirtyHarryLYL/HAKE-Action"})]}),(0,n.jsxs)("td",{children:["HAKE-AVA ",(0,n.jsx)(d,{href:"https://github.com/DirtyHarryLYL/HAKE-AVA"})]})]}),(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{}),(0,n.jsxs)("td",{children:["Halpe ",(0,n.jsx)(d,{href:"https://github.com/Fang-Haoshu//Halpe-FullBody"})]}),(0,n.jsxs)("td",{children:["HOI List ",(0,n.jsx)(d,{href:"https://github.com/DirtyHarryLYL/HOI-Learning-List"})]})]})]})})},{title:"Constructing Balance from Imbalance for Long-tailed Image Recognition",author:"Yue Xu*, Yong-Lu Li*, Jiefeng Li, Cewu Lu (*=equal contribution)",conf:"ECCV 2022",shortname:"DLSA",links:[["arXiv","https://arxiv.org/abs/2208.02567"],["PDF","https://arxiv.org/pdf/2208.02567.pdf"],["Code","https://github.com/silicx/DLSA"],]},{title:"Mining Cross-Person Cues for Body-Part Interactiveness Learning in HOI Detection",author:"Xiaoqian Wu*, Yong-Lu Li*, Xinpeng Liu, Junyi Zhang, Yuzhe Wu, Cewu Lu (*=equal contribution)",conf:"ECCV 2022",links:[["arXiv","https://arxiv.org/abs/2207.14192"],["PDF","https://arxiv.org/pdf/2207.14192v1.pdf"],["Code","https://github.com/enlighten0707/Body-Part-Map-for-Interactiveness"],]},{title:"Interactiveness Field of Human-Object Interactions",author:"Xinpeng Liu*, Yong-Lu Li*, Xiaoqian Wu, Yu-Wing Tai, Cewu Lu, Chi Keung Tang (*=equal contribution)",conf:"CVPR 2022",links:[["arXiv","https://arxiv.org/abs/2204.07718"],["PDF","https://arxiv.org/pdf/2204.07718.pdf"],["Code","https://github.com/Foruck/Interactiveness-Field"],]},{title:"Human Trajectory Prediction with Momentary Observation",author:"Jianhua Sun, Yuxuan Li, Liang Chai, Hao-Shu Fang, Yong-Lu Li, Cewu Lu",conf:"CVPR 2022",links:[["PDF","https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_Human_Trajectory_Prediction_With_Momentary_Observation_CVPR_2022_paper.pdf",],]},{title:"Learn to Anticipate Future with Dynamic Context Removal",author:"Xinyu Xu, Yong-Lu Li, Cewu Lu",conf:"CVPR 2022",shortname:"DCR",links:[["arXiv","https://arxiv.org/abs/2204.02587"],["PDF","https://arxiv.org/pdf/2204.02587.pdf"],["Code","https://github.com/AllenXuuu/DCR"],]},{title:"Canonical Voting: Towards Robust Oriented Bounding Box Detection in 3D Scenes",author:"Yang You, Zelin Ye, Yujing Lou, Chengkun Li, Yong-Lu Li, Lizhuang Ma, Weiming Wang, Cewu Lu",conf:"CVPR 2022",links:[["arXiv","https://arxiv.org/abs/2011.12001"],["PDF","https://arxiv.org/pdf/2011.12001.pdf"],["Code","https://github.com/qq456cvb/CanonicalVoting"],]},{title:"UKPGAN: Unsupervised KeyPoint GANeration",author:"Yang You, Wenhai Liu, Yong-Lu Li, Weiming Wang, Cewu Lu",conf:"CVPR 2022",links:[["arXiv","https://arxiv.org/abs/2011.11974"],["PDF","https://arxiv.org/pdf/2011.11974.pdf"],["Code","https://github.com/qq456cvb/UKPGAN"],]},{title:"Highlighting Object Category Immunity for the Generalization of Human-Object Interaction Detection",author:"Xinpeng Liu*, Yong-Lu Li*, Cewu Lu (*=equal contribution)",conf:"AAAI 2022",links:[["arXiv","https://arxiv.org/abs/2202.09492"],["PDF","https://arxiv.org/pdf/2202.09492.pdf"],["Code","https://github.com/Foruck/OC-Immunity"],["Github","https://github.com/Foruck/OC-Immunity"],]},{title:"Learning Single/Multi-Attribute of Object with Symmetry and Group",author:"Yong-Lu Li, Yue Xu, Xinyu Xu, Xiaohan Mao, Cewu Lu",conf:"TPAMI 2021",shortname:"SymNet",links:[["arXiv","https://arxiv.org/abs/2110.04603"],["PDF","https://arxiv.org/pdf/2110.04603.pdf"],["Code","https://github.com/DirtyHarryLYL/SymNet"],],content:"An extension of our CVPR 2020 work (Symmetry and Group in Attribute-Object Compositions, SymNet)."},{title:"Localization with Sampling-Argmax",author:"Jiefeng Li, Tong Chen, Ruiqi Shi, Yujing Lou, Yong-Lu Li, Cewu Lu",conf:"NeurIPS 2021",links:[["arXiv","https://arxiv.org/abs/2110.08825"],["PDF","https://arxiv.org/pdf/2110.08825.pdf"],["Code","https://github.com/Jeff-sjtu/sampling-argmax"],]},{title:"Transferable Interactiveness Knowledge for Human-Object Interaction Detection",author:"Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Xijie Huang, Liang Xu, Cewu Lu",conf:"TPAMI 2021",shortname:"TIN",links:[["arXiv","https://arxiv.org/abs/2101.10292"],["PDF","https://arxiv.org/pdf/2101.10292.pdf"],["Code","https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network"],],content:"An extension of our CVPR 2019 work (Transferable Interactiveness Network, TIN)."},{title:"DecAug: Augmenting HOI Detection via Decomposition",author:"Yichen Xie, Hao-Shu Fang, Dian Shao, Yong-Lu Li, Cewu Lu",conf:"AAAI 2021",links:[["arXiv","https://arxiv.org/abs/2010.01007"],["PDF","https://arxiv.org/pdf/2010.01007.pdf"],]},{title:"HOI Analysis: Integrating and Decomposing Human-Object Interaction",author:"Yong-Lu Li*, Xinpeng Liu*, Xiaoqian Wu, Yizhuo Li, Cewu Lu (*=equal contribution)",conf:"NeurIPS 2020",links:[["arXiv","https://arxiv.org/abs/2010.16219"],["PDF","https://arxiv.org/pdf/2010.16219.pdf"],["Code","https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network)"],["Project: HAKE-Action-Torch","https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/master"],]},{title:"PaStaNet: Toward Human Activity Knowledge Engine",author:"Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu, Shiyi Wang, Hao-Shu Fang, Ze Ma, Mingyang Chen, Cewu Lu.",conf:"CVPR 2020",links:[["arXiv","https://arxiv.org/abs/2004.00945"],["PDF","https://arxiv.org/pdf/2004.00945.pdf"],["Video","https://drive.google.com/file/d/16PCCK_flK2qW4QJVWoYXQwG3wgXd6yvT/view?usp=sharing"],["Slides","https://drive.google.com/file/d/19J9uz3epBo3o9CIU85mzgLblRVNawl87/view?usp=sharing"],["Data","https://github.com/DirtyHarryLYL/HAKE"],["Code","https://github.com/DirtyHarryLYL/HAKE-Action"],],content:(0,n.jsxs)("p",{children:[(0,n.jsx)("strong",{className:"text-red-600",children:"Oral Talk:"})," ",(0,n.jsx)("a",{href:"http://ai.stanford.edu/~jingweij/cicv/#schedule",children:"Compositionality in Computer Vision"})," in CVPR 2020"]})},{title:"Detailed 2D-3D Joint Representation for Human-Object Interaction",author:"Yong-Lu Li, Xinpeng Liu, Han Lu, Shiyi Wang, Junqi Liu, Jiefeng Li, Cewu Lu",conf:"CVPR 2020",shortname:"DJ-RN",links:[["arXiv","https://arxiv.org/abs/2004.08154"],["PDF","https://arxiv.org/pdf/2004.08154.pdf"],["Video","https://drive.google.com/file/d/14dK1tBLe3xHXyO_5WsRO2JcjJ95meaDB/view?usp=sharing"],["Slides","https://drive.google.com/file/d/1A5bQZFsBOahj7dJgSnWR7jdyvMG58NkT/view?usp=sharing"],["Benchmark: Ambiguous-HOI","https://github.com/DirtyHarryLYL/DJ-RN#ambiguous-hoi"],["Code","https://github.com/DirtyHarryLYL/DJ-RN"],]},{title:"Symmetry and Group in Attribute-Object Compositions",author:"Yong-Lu Li, Yue Xu, Xiaohan Mao, Cewu Lu",conf:"CVPR 2020",shortname:"SymNet",links:[["arXiv","https://arxiv.org/abs/2004.00587"],["PDF","https://arxiv.org/pdf/2004.00587.pdf"],["Video","https://drive.google.com/file/d/1ZTSB2lJbDTH7D-7GdEJQGmszvEc2Vuwd/view?usp=sharing"],["Slides","https://drive.google.com/file/d/1aqYeSIQkoTp1hYOJokDgoucdZcufN2iG/view?usp=sharing"],["Code","https://github.com/DirtyHarryLYL/SymNet"],]},{title:"InstaBoost: Boosting Instance Segmentation Via Probability Map Guided Copy-Pasting",author:"Hao-Shu Fang*, Jianhua Sun*, Runzhong Wang*, Minghao Gou, Yong-Lu Li, Cewu Lu (*=equal contribution)",conf:"ICCV 2019",links:[["arXiv","https://arxiv.org/abs/1908.07801"],["PDF","https://arxiv.org/pdf/1908.07801.pdf"],["Code","https://github.com/GothicAi/Instaboost"],]},{title:"Transferable Interactiveness Knowledge for Human-Object Interaction Detection",author:"Yong-Lu Li, Siyuan Zhou, Xijie Huang, Liang Xu, Ze Ma, Hao-Shu Fang, Yan-Feng Wang, Cewu Lu",conf:"CVPR 2019",shortname:"TIN",links:[["arXiv","https://arxiv.org/abs/1811.08264"],["PDF","https://arxiv.org/pdf/1811.08264.pdf"],["Code","https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network"],]},{title:"SRDA: Generating Instance Segmentation Annotation via Scanning, Reasoning and Domain Adaptation",author:"Wenqiang Xu*, Yong-Lu Li*, Cewu Lu (*=equal contribution)",conf:"ECCV 2018",links:[["arXiv","https://arxiv.org/abs/1801.08839"],["PDF","https://arxiv.org/pdf/1801.08839.pdf"],["Dataset (Instance-60k & 3D Object Models)","https://drive.google.com/drive/folders/1t941oiLk40XQX2Q9a2HPmiDRUpiwazJO?usp=sharing",],["Code","https://github.com/DirtyHarryLYL/SRDA-ECCV2018"],]},{title:"Beyond Holistic Object Recognition: Enriching Image Understanding with Part States",author:"Cewu Lu, Hao Su, Yong-Lu Li, Yongyi Lu, Li Yi, Chi-Keung Tang, Leonidas J. Guibas",conf:"CVPR 2018",links:[["PDF","http://ai.ucsd.edu/~haosu/papers/cvpr18_partstate.pdf"]]},{title:"Optimization of Radial Distortion Self-Calibration for Structure from Motion from Uncalibrated UAV Images",author:"Yong-Lu Li, Yinghao Cai, Dayong Wen, Yiping Yang",conf:"ICPR 2016",links:[["PDF","https://projet.liris.cnrs.fr/imagine/pub/proceedings/ICPR-2016/media/files/1010.pdf"]]},],k=[{name:"Cewu Lu",position:"Professor",url:"https://www.mvig.org/"},{name:"Yong-Lu Li",position:"Assistant Professor",url:"https://dirtyharrylyl.github.io/"},{name:"Jingru Tan",position:"Postdoc",url:"https://scholar.google.com/citations?user=l18d7kcAAAAJ"},{name:"Xinpeng Liu",position:"PhD. Student",url:"https://foruck.github.io/"},{name:"Yue Xu",position:"PhD. Student",url:"https://silicx.github.io/"},{name:"Xiaoqian Wu",position:"PhD. Student",url:"https://scholar.google.com/citations?user=-PHR96oAAAAJ"},{name:"Xiaohan Mao",position:"PhD. Student",url:"https://scholar.google.com/citations?user=-zT1NKwAAAAJ"},{name:"Siqi Liu",position:"PhD. Student",url:"https://github.com/MayuOshima/"},{name:"Xinyu Xu",position:"Master Student",url:"https://xuxinyu.website"},{name:"Junyi Zhang",position:"Undergraduate",url:"https://scholar.google.com/citations?user=LTi1tYsAAAAJ"},{name:"Yiming Dou",position:"Undergraduate",url:"https://dou-yiming.github.io/"},{name:"Zhemin Huang",position:"Undergraduate",altimage:"placeholder.png"},{name:"Kaitong Cui",position:"Undergraduate",altimage:"placeholder.png"},{name:"Yixing Li",position:"Undergraduate",url:"https://github.com/HEYyox/"},{name:"Yikun Ji",position:"Undergraduate",url:"http://github.com/Gennadiyev/"},],Y=[{name:"Yuzhe Wu",position:"Undergraduate"},{name:"Shaopeng Guo",position:"UCSD PhD",url:"https://coeusguo.github.io/"},{name:"Xudong Lu",position:"CUHK, PhD"},{name:"Hongwei Fan",position:"Sensetime"},{name:"Yuan Yao",position:"University of Rochester, PhD"},{name:"Zuoyu Qiu",position:"SJTU"},{name:"Junqi Liu",position:""},{name:"Han Lu",position:"SJTU, PhD"},{name:"Shiyi Wang",position:""},{name:"Zhanke Zhou",position:"Hong Kong Baptist University"},{name:"Mingyang Chen",position:"UCSD MS"},{name:"Siyuan Zhou",position:"SJTU MS"},{name:"Liang Xu",position:"EIAS & SJTU, PhD",url:"https://liangxuy.github.io/"},{name:"Ze Ma",position:"Columbia University, MSCS",url:"https://maqingyang.github.io/"},{name:"Xijie Huang",url:"https://huangowen.github.io/",position:"HKUST, PhD"},]}}]);