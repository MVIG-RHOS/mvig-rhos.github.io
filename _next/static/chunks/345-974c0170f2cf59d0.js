"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[345],{1469:function(t,e,i){var n=i(1438),a=i(2951),r=i(8029),o=i(6567),s=i(5893),c=i(7294),h=i(2719),u=function(t){(0,r.Z)(i,t);var e=(0,o.Z)(i);function i(){return(0,n.Z)(this,i),e.apply(this,arguments)}return(0,a.Z)(i,[{key:"render",value:function(){return this.props.href?(0,s.jsx)(h.Z,{"aria-label":"Star buttons/github-buttons on GitHub","data-show-count":"true",href:this.props.href,children:"Star"}):null}}]),i}(c.Component);e.Z=u},9770:function(t,e,i){var n=i(5893),a=i(9008),r=i.n(a),o=i(1163),s=(0,i(7294).memo)(function(t){var e=t.children,i=t.title,a=t.description,s=(0,o.useRouter)().asPath;return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(r(),{children:[(0,n.jsx)("title",{children:i}),(0,n.jsx)("meta",{content:a,name:"description"}),(0,n.jsx)("link",{href:"https://http://mvig-rhos.com".concat(s),rel:"canonical"},"canonical"),(0,n.jsx)("link",{href:"/favicon.ico",rel:"icon",sizes:"any"}),(0,n.jsx)("link",{href:"/icon.svg",rel:"icon",type:"image/svg+xml"}),(0,n.jsx)("link",{href:"/apple-touch-icon.png",rel:"apple-touch-icon"}),(0,n.jsx)("link",{href:"/site.webmanifest",rel:"manifest"}),(0,n.jsx)("meta",{content:i,property:"og:title"}),(0,n.jsx)("meta",{content:a,property:"og:description"}),(0,n.jsx)("meta",{content:"https://http://mvig-rhos.com".concat(s),property:"og:url"}),(0,n.jsx)("meta",{content:i,name:"twitter:title"}),(0,n.jsx)("meta",{content:a,name:"twitter:description"})]}),e]})});s.displayName="Page",e.Z=s},3031:function(t,e,i){var n=i(5893),a=i(4184),r=i.n(a),o=(0,i(7294).memo)(function(t){var e=t.children,i=t.sectionId,a=t.noPadding,o=void 0!==a&&a,s=t.className;return(0,n.jsx)("section",{className:r()(s,{"px-4 py-8 md:py-12 lg:px-8":!o}),id:i,children:(0,n.jsx)("div",{className:r()({"mx-auto max-w-screen-lg":!o}),children:e})})});o.displayName="Section",e.Z=o},11:function(t,e,i){var n=i(5893),a=(0,i(7294).memo)(function(t){var e=t.title,i=t.comment,a=t.children;return(0,n.jsxs)("div",{className:"grid grid-cols-1 gap-y-4 py-8 first:pt-0 last:pb-0 md:grid-cols-4",children:[(0,n.jsx)("div",{className:"col-span-1 flex justify-center md:justify-start",children:(0,n.jsxs)("div",{className:"relative h-max",children:[(0,n.jsx)("h2",{className:"text-xl font-bold uppercase text-neutral-800",children:e}),(0,n.jsx)("span",{className:"absolute inset-x-0 border-b-2 border-orange-400"}),(0,n.jsx)("div",{children:i&&(0,n.jsx)(n.Fragment,{children:i})})]})}),(0,n.jsx)("div",{className:"col-span-1 flex flex-col md:col-span-3",children:a})]})});a.displayName="TitledSection",e.Z=a},6571:function(t,e,i){var n=i(5893),a=i(822),r=i(7294),o=i(6346),s=(0,r.memo)(function(){return(0,n.jsxs)("div",{className:"relative bg-neutral-900 px-4 pb-6 pt-12 sm:px-8 sm:pt-14 sm:pb-8",children:[(0,n.jsx)("div",{className:"absolute inset-x-0 -top-4 flex justify-center sm:-top-6",children:(0,n.jsx)("a",{className:"rounded-full bg-neutral-100 p-1 ring-white ring-offset-2 ring-offset-gray-700/80 focus:outline-none focus:ring-2 sm:p-2",href:"/#".concat(o._h.Hero),children:(0,n.jsx)(a.Z,{className:"h-6 w-6 bg-transparent sm:h-8 sm:w-8"})})}),(0,n.jsxs)("div",{className:"flex flex-col items-center gap-y-6",children:[(0,n.jsx)("div",{id:"pageview-script",className:"text-sm text-neutral-700"}),(0,n.jsxs)("span",{className:"text-sm text-neutral-700",children:["\xa9 Copyright 2022 MVIG-RHOS • Based on"," ",(0,n.jsx)("a",{href:"https://github.com/tbakerx/react-resume-template",children:"tbakerx"})]})]})]})});s.displayName="Footer",e.Z=s},6346:function(t,e,i){i.d(e,{J6:function(){return y},_h:function(){return j},QL:function(){return C},IX:function(){return k},oM:function(){return L},J8:function(){return v},uu:function(){return w},Vf:function(){return S},mZ:function(){return H},q:function(){return I},Po:function(){return P},c$:function(){return D}});var n=i(5893),a=i(5966),r=i(6339),o=i(7402),s=i(1469),c=i(1799),h=i(9396),u=i(7294),l=i(9534),g=(0,u.memo)(function(t){var e=t.children,i=t.className,a=t.svgRef,r=t.transform,o=(0,l.Z)(t,["children","className","svgRef","transform"]);return(0,n.jsx)("svg",(0,h.Z)((0,c.Z)({className:i,fill:"currentColor",ref:a,transform:r,viewBox:"0 0 128 128",width:"128",xmlns:"http://www.w3.org/2000/svg"},o),{children:e}))}),p=(0,u.memo)(function(t){return(0,n.jsxs)(g,(0,h.Z)((0,c.Z)({},t),{children:[(0,n.jsx)("path",{clipRule:"evenodd",d:"M64 5.103c-33.347 0-60.388 27.035-60.388 60.388 0 26.682 17.303 49.317 41.297 57.303 3.017.56 4.125-1.31 4.125-2.905 0-1.44-.056-6.197-.082-11.243-16.8 3.653-20.345-7.125-20.345-7.125-2.747-6.98-6.705-8.836-6.705-8.836-5.48-3.748.413-3.67.413-3.67 6.063.425 9.257 6.223 9.257 6.223 5.386 9.23 14.127 6.562 17.573 5.02.542-3.903 2.107-6.568 3.834-8.076-13.413-1.525-27.514-6.704-27.514-29.843 0-6.593 2.36-11.98 6.223-16.21-.628-1.52-2.695-7.662.584-15.98 0 0 5.07-1.623 16.61 6.19C53.7 35 58.867 34.327 64 34.304c5.13.023 10.3.694 15.127 2.033 11.526-7.813 16.59-6.19 16.59-6.19 3.287 8.317 1.22 14.46.593 15.98 3.872 4.23 6.215 9.617 6.215 16.21 0 23.194-14.127 28.3-27.574 29.796 2.167 1.874 4.097 5.55 4.097 11.183 0 8.08-.07 14.583-.07 16.572 0 1.607 1.088 3.49 4.148 2.897 23.98-7.994 41.263-30.622 41.263-57.294C124.388 32.14 97.35 5.104 64 5.104z",fillRule:"evenodd"}),(0,n.jsx)("path",{d:"M26.484 91.806c-.133.3-.605.39-1.035.185-.44-.196-.685-.605-.543-.906.13-.31.603-.395 1.04-.188.44.197.69.61.537.91zm2.446 2.729c-.287.267-.85.143-1.232-.28-.396-.42-.47-.983-.177-1.254.298-.266.844-.14 1.24.28.394.426.472.984.17 1.255zM31.312 98.012c-.37.258-.976.017-1.35-.52-.37-.538-.37-1.183.01-1.44.373-.258.97-.025 1.35.507.368.545.368 1.19-.01 1.452zm3.261 3.361c-.33.365-1.036.267-1.552-.23-.527-.487-.674-1.18-.343-1.544.336-.366 1.045-.264 1.564.23.527.486.686 1.18.333 1.543zm4.5 1.951c-.147.473-.825.688-1.51.486-.683-.207-1.13-.76-.99-1.238.14-.477.823-.7 1.512-.485.683.206 1.13.756.988 1.237zm4.943.361c.017.498-.563.91-1.28.92-.723.017-1.308-.387-1.315-.877 0-.503.568-.91 1.29-.924.717-.013 1.306.387 1.306.88zm4.598-.782c.086.485-.413.984-1.126 1.117-.7.13-1.35-.172-1.44-.653-.086-.498.422-.997 1.122-1.126.714-.123 1.354.17 1.444.663zm0 0"})]}))}),d=i(1438),m=i(2951),A=i(8029),b=i(6567),f=function(t){(0,A.Z)(i,t);var e=(0,b.Z)(i);function i(){return(0,d.Z)(this,i),e.apply(this,arguments)}return(0,m.Z)(i,[{key:"render",value:function(){return(0,n.jsxs)("a",{href:"/hake",children:[(0,n.jsxs)("b",{children:[(0,n.jsx)("span",{style:{color:"red"},children:"H"}),(0,n.jsx)("span",{style:{color:"blue"},children:"A"}),(0,n.jsx)("span",{style:{color:"red"},children:"KE"})]}),this.props.children]})}}]),i}(u.Component),x=(0,u.memo)(function(t){return(0,n.jsx)(g,(0,h.Z)((0,c.Z)({},t),{children:(0,n.jsx)("path",{d:"M116 3H12a8.91 8.91 0 00-9 8.8v104.42a8.91 8.91 0 009 8.78h104a8.93 8.93 0 009-8.81V11.77A8.93 8.93 0 00116 3zM39.17 107H21.06V48.73h18.11zm-9-66.21a10.5 10.5 0 1110.49-10.5 10.5 10.5 0 01-10.54 10.48zM107 107H88.89V78.65c0-6.75-.12-15.44-9.41-15.44s-10.87 7.36-10.87 15V107H50.53V48.73h17.36v8h.24c2.42-4.58 8.32-9.41 17.13-9.41C103.6 47.28 107 59.35 107 75z",fill:"currentColor"})}))}),v={title:"RHOS",description:"Homepage of RHOS"},j={Hero:"hero",About:"about",Recruit:"recruit",News:"news",Projects:"projects",Portfolio:"portfolio",Publications:"publications",People:"people"},y=[j.About,j.News,j.Projects,j.Publications,j.People],L={imageSrc:{src:"/_next/static/media/Robotics_Cyberpunk_2077_bg.f50d6e01.png",height:1125,width:2e3,blurDataURL:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAIAAAD38zoCAAAAhUlEQVR42mPw8DGPDLMz0ZMzs7OMTwpVNTAI8LayN1dikDXTUbUx9nDQD/K01jHSUzOyNrd1NjbUZVDQkbGw1zO1NXJ1srS2MjKysLK0tPEOiWCwMZVJ8FUvTbToKLSfUe9eHKXhbCrEIynL4GuvVJLiUp/jVZVqmxWqGuggpaciLCDACQABSR1UPIEuQgAAAABJRU5ErkJggg==",blurWidth:8,blurHeight:5},name:"RHOS",description:(0,n.jsx)(n.Fragment,{children:"Robot • Human • Object • Scene"}),actions:[{href:"#".concat(j.Recruit),text:"Recruit",primary:!0},{href:"https://github.com/mvig-rhos",text:"Github",primary:!1,Icon:p},]},C={profileImageSrc:{src:"/_next/static/media/profilepic.80e868bc.jpg",height:294,width:294,blurDataURL:"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAgACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABAEBAQAAAAAAAAAAAAAAAAAAAwT/2gAMAwEAAhADEAAAAKxSH//EABsQAAIDAAMAAAAAAAAAAAAAAAECAwQRAAUh/9oACAEBAAE/AGs9hWWuDIzGTUkjw6rH0c//xAAZEQEAAgMAAAAAAAAAAAAAAAAhAAECAwT/2gAIAQIBAT8Aw6dlItk//8QAGhEBAAEFAAAAAAAAAAAAAAAAASEAAwQxMv/aAAgBAwEBPwBxraHURuv/2Q==",blurWidth:8,blurHeight:8},aboutWhat:"RHOS",description:(0,n.jsxs)("div",{children:[(0,n.jsxs)("p",{children:["Hi, this is the website of RHOS team at"," ",(0,n.jsx)("a",{className:"font-bold text-white",href:"https://www.mvig.org/",children:"MVIG"}),". We study"," ",(0,n.jsx)("i",{children:(0,n.jsx)("b",{children:"Human Activity Understanding"})}),","," ",(0,n.jsx)("i",{children:(0,n.jsx)("b",{children:"Visual Reasoning"})}),", and"," ",(0,n.jsx)("i",{children:(0,n.jsx)("b",{children:"Embodied AI"})}),". We are building a knowledge-driven system that enables intelligent agents to perceive human activities, reason human behavior logics, learn skills from human activities, and interact with environment."]}),(0,n.jsx)("p",{children:(0,n.jsx)("b",{children:"Research Interests: "})}),(0,n.jsxs)("p",{children:["(S) ",(0,n.jsx)("b",{children:"Embodied AI"}),": how to make agents learn skills from humans and interact with human & scene & object.",(0,n.jsx)("br",{}),"(S-1) ",(0,n.jsx)("b",{children:"Human Activity Understanding"}),": how to learn and ground complex/ambiguous human activity concepts (body motion, human-object/human/scene interaction) and object concepts from multi-modal information (2D-3D-4D).",(0,n.jsx)("br",{}),"(S-2) ",(0,n.jsx)("b",{children:"Visual Reasoning"}),": how to mine, capture, and embed the logics and causal relations from human activities.",(0,n.jsx)("br",{}),"(S-3) ",(0,n.jsx)("b",{children:"General Multi-Modal Foundation Models"}),": especially for human-centric perception tasks.",(0,n.jsx)("br",{}),"(S-4) ",(0,n.jsx)("b",{children:"Activity Understanding from A Cognitive Perspective"}),": work with multidisciplinary researchers to study how the brain perceives activities.",(0,n.jsx)("br",{}),"(E) ",(0,n.jsx)("b",{children:"Human-Robot Interaction (e.g. for Smart Hospital)"}),": work with the healthcare team (doctors and engineers) in SJTU to develop intelligent robots to help people."]})]}),contactDetail:(0,n.jsxs)("div",{children:[(0,n.jsx)("b",{children:"Yong-Lu Li"}),(0,n.jsx)("br",{}),"Email: yonglu_li[at]sjtu[dot]edu[dot]cn",(0,n.jsx)("br",{}),"Office: SEIEE-3-301",(0,n.jsx)("br",{}),"Shanghai Jiao Tong University"]}),contactLinkItems:[{label:"Personal Website",link:"https://dirtyharrylyl.github.io/",Icon:a.Z,newline:!0},{label:"Google Scholar ",link:"https://scholar.google.com.hk/citations?user=UExAaVgAAAAJ",Icon:r.Z},{label:"Github ",link:"https://github.com/DirtyHarryLYL",Icon:p},{label:"LinkedIn",link:"https://www.linkedin.com/in/%E6%B0%B8%E9%9C%B2-%E6%9D%8E-991b99139/",Icon:x},{label:"dblp",link:"https://dblp.org/pid/198/9345.html",Icon:o.Z},{label:"Semantic Scholar",link:"https://www.semanticscholar.org/author/Yong-Lu-Li/10384643",Icon:o.Z},]},D=(0,n.jsxs)("span",{children:[(0,n.jsxs)("p",{children:["We are actively looking for self-motivated"," ",(0,n.jsx)("strong",{children:"students (Master/PhD, 2024 spring & fall), interns / engineers / visitors"})," ","(CV/ML/ROB/NLP/Math/Phys background, always welcome) to join us in"," ",(0,n.jsx)("a",{className:"text-red600",href:"https://www.mvig.org/",children:"Machine Vision and Intelligence Group (MVIG)"}),". If you share same/similar interests, feel free to drop me an email with your resume."]}),(0,n.jsxs)("p",{children:["Click"," ",(0,n.jsx)("a",{className:"text-red-600",href:"https://dirtyharrylyl.github.io/recruit.html",children:(0,n.jsx)("b",{children:"here"})})," ","for more details."]})]}),w=[["2023.7",(0,n.jsxs)(n.Fragment,{children:["Our works on ego-centric video understanding and ",(0,n.jsx)("a",{className:"underline text-sky-600",href:"https://mvig-rhos.com/ocl",children:"object concept learning"})," will appear at ICCV 2023!"]}),],["2023.7",(0,n.jsxs)(n.Fragment,{children:["The upgrade version of ",(0,n.jsx)("a",{className:"underline text-sky-600",href:"https://github.com/AllenXuuu/DCR",children:"DCR"})," will appear at IJCV!"]}),],["2022.12",(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)("a",{className:"font-bold",href:"https://arxiv.org/abs/2202.06851",children:"HAKE 2.0"})," will appear at TPAMI!"]}),],["2022.12",(0,n.jsxs)(n.Fragment,{children:["OCL (Object Concept Leanring) is released on ",(0,n.jsx)("a",{className:"underline text-sky-600",href:"https://arxiv.org/abs/2212.02710",children:"arXiv"}),". Please visit the ",(0,n.jsx)("a",{className:"underline text-sky-600",href:"/ocl",children:"project page"})," for details."]}),],["2022.11",(0,n.jsxs)(n.Fragment,{children:["We release the human body part states and interactive object bounding box annotations upon AVA (2.1 & 2.2): ",(0,n.jsx)("a",{className:"font-bold",href:"https://github.com/DirtyHarryLYL/HAKE-AVA",children:"[HAKE-AVA]"}),", and a CLIP-based human part state & verb recognizer: ",(0,n.jsx)("a",{className:"font-bold",href:"https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/CLIP-Activity2Vec",children:"[CLIP-Activity2Vec]"}),"."]}),],["2022.11",(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)("a",{className:"font-bold",href:"https://github.com/MVIG-SJTU/AlphaPose",children:"AlphaPose"})," will appear at TPAMI!"]}),],["2022.07",(0,n.jsxs)(n.Fragment,{children:["Two papers on ",(0,n.jsx)("b",{children:"longtailed learning, HOI detection"})," are accepted by ECCV'22, arXivs and code are coming soon"]}),],["2022.03",(0,n.jsxs)(n.Fragment,{children:["Five papers on ",(0,n.jsx)("b",{children:"HOI detection/prediction, trajection prediction, 3D detection/keypoints"})," are accepted by CVPR'22, papers and code are coming soon."]}),],["2022.02",(0,n.jsxs)(n.Fragment,{children:["We release the human body part state labels based on AVA:"," ",(0,n.jsx)("a",{className:"font-bold",href:"https://github.com/DirtyHarryLYL/HAKE-AVA",children:"HAKE-AVA"})," and ",(0,n.jsx)("a",{className:"font-bold",href:"https://arxiv.org/abs/2202.06851",children:"HAKE 2.0"}),"."]}),],["2021.12",(0,n.jsxs)(n.Fragment,{children:["Our work on ",(0,n.jsx)("b",{children:"HOI generalization"})," will appear at AAAI'22."]}),],["2021.10",(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)("b",{children:"Learning Single/Multi-Attribute of Object with Symmetry and Group"})," is accepted by TPAMI."]}),],["2021.09",(0,n.jsxs)(n.Fragment,{children:["Our work ",(0,n.jsx)("b",{children:"Localization with Sampling-Argmax"})," will appear at NeurIPS'21."]}),],["2021.02",(0,n.jsxs)(n.Fragment,{children:["Upgraded"," ",(0,n.jsx)("a",{className:"font-bold",href:"https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/Activity2Vec",children:"HAKE-Activity2Vec"})," ","is released! Images/Videos --> human box + ID + skeleton + part states + action + representation."," ",(0,n.jsx)("a",{className:"underline",href:"https://youtu.be/ty-bXDInLMQ",children:"[Demo]"})," ",(0,n.jsx)("a",{className:"underline",href:"https://drive.google.com/file/d/1iZ57hKjus2lKbv1MAB-TLFrChSoWGD5e/view?usp=sharing",children:"[Description]"})]}),],["2021.01",(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)("b",{children:[(0,n.jsx)("a",{href:"https://arxiv.org/abs/2101.10292",children:"TIN"})," (Transferable Interactiveness Network)"]})," ","is accepted by TPAMI."]}),],["2020.12",(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)("a",{className:"font-bold underline",href:"https://arxiv.org/abs/2010.01007",children:"DecAug"})," ","is accepted by AAAI'21."]}),],["2020.09",(0,n.jsxs)(n.Fragment,{children:["Our work ",(0,n.jsx)("b",{children:"HOI Analysis"})," will appear at NeurIPS 2020."]}),],["2020.06",(0,n.jsxs)(n.Fragment,{children:["The larger"," ",(0,n.jsx)("a",{className:"font-bold",href:"https://github.com/DirtyHarryLYL/HAKE#hake-large-for-instance-level-hoi-detection",children:"HAKE-Large"})," ","(>120K images with activity and part state labels) is released."]}),],["2020.02",(0,n.jsxs)(n.Fragment,{children:["Three papers ",(0,n.jsx)("b",{children:"Image-based HAKE: PaSta-Net"}),", ",(0,n.jsx)("b",{children:"2D-3D Joint HOI Learning"}),","," ",(0,n.jsx)("b",{children:"Symmetry-based Attribute-Object Learning"})," are accepted in ",(0,n.jsx)("a",{href:"http://cvpr2020.thecvf.com/",children:"CVPR'20"}),"! Papers and corresponding resources (code, data) will be released soon."]}),],["2019.07",(0,n.jsxs)(n.Fragment,{children:["Our paper ",(0,n.jsx)("b",{children:"InstaBoost"})," is accepted in ",(0,n.jsx)("a",{href:"http://iccv2019.thecvf.com/",children:"ICCV'19"}),"."]}),],["2019.06",(0,n.jsxs)(n.Fragment,{children:["The Part I of our ",(0,n.jsx)(f,{}),":"," ",(0,n.jsx)("b",{children:(0,n.jsx)("a",{href:"http://hake-mvig.cn/download/",children:"HAKE-HICO"})})," ","which contains the image-level part-state annotations is released."]}),],["2019.04",(0,n.jsxs)(n.Fragment,{children:["Our project ",(0,n.jsx)(f,{})," (Human Activity Knowledge Engine) begins trial operation."]}),],["2019.02",(0,n.jsxs)(n.Fragment,{children:["Our paper on"," ",(0,n.jsx)("b",{children:(0,n.jsx)("a",{href:"https://arxiv.org/abs/1811.08264",children:"Interactiveness"})})," ","is accepted in ",(0,n.jsx)("a",{href:"http://cvpr2019.thecvf.com/",children:"CVPR'19"}),"."]}),],["2018.07",(0,n.jsxs)(n.Fragment,{children:["Our paper on"," ",(0,n.jsx)("b",{children:(0,n.jsx)("a",{href:"https://arxiv.org/abs/1801.08839",children:"GAN & Annotation Generation"})})," ","is accepted in ",(0,n.jsx)("a",{href:"https://eccv2018.org/",children:"ECCV'18"}),"."]}),],["2018.05",(0,n.jsxs)(n.Fragment,{children:["Presentation (Kaibot Team) in"," ",(0,n.jsx)("a",{href:"https://icra2018.org/tidy-up-my-room-challenge/",children:"TIDY UP MY ROOM CHALLENGE | ICRA'18"}),"."]}),],["2018.02",(0,n.jsxs)(n.Fragment,{children:["Our paper on"," ",(0,n.jsx)("b",{children:(0,n.jsx)("a",{href:"http://ai.ucsd.edu/~haosu/papers/cvpr18_partstate.pdf",children:"Object Part States"})})," ","is accepted in ",(0,n.jsx)("a",{href:"http://cvpr2018.thecvf.com/program/main_conference",children:"CVPR'18"}),"."]}),],],H=[{title:"HAKE 2.0",description:"The upgraded Human Activity Knowledge Engine",url:"http://hake-mvig.cn/home",image:"2022_hake2.0.jpg"},{title:"PartMap",description:"Interactiveness learning from the global, scene-level perspective",url:"https://github.com/enlighten0707/Body-Part-Map-for-Interactiveness",image:"2022_ECCV_partmap.jpg"},{title:"DLSA",description:"Plug-and-play long-tail learning module by reorganizing label space",url:"https://github.com/silicx/DLSA",image:"2022_ECCV_longtail.jpg"},{title:"Interactiveness-Field",description:"Model HOI with the interactiveness bimodal prior",url:"https://github.com/Foruck/Interactiveness-Field",image:"2022_CVPR_InteractivenessField.jpg"},{title:"DCR",description:"A training strategy on action predictions with a dynamic learning pipeline",url:"https://github.com/AllenXuuu/DCR",image:"2022_CVPR_anticipate.jpg"},{title:"OC-Immunity",description:"Advance HOI generalization by mitigating object category bias",url:"https://github.com/Foruck/OC-Immunity",image:"2022_AAAI_hoi.jpg"},{title:"TIN & TIN++",description:"Interactiveness learning for Human-Object Interaction learning.",url:"https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network",image:"2021_TPAMI_TIN.jpg"},{title:"SymNet",description:"Attribute detector based on symmetry and group.",url:"https://github.com/DirtyHarryLYL/SymNet",image:"2021_TPAMI_SymNet.jpg"},{title:"HOI Analysis",description:"A way to decompse and integrate human-object interaction pairs.",url:"https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network)",image:"2020_NeurIPS_analysis.jpg"},{title:"PaStaNet",description:"A human body Part States library and learner.",url:"https://github.com/DirtyHarryLYL/HAKE-Action",image:"2020_CVPR_pastanet.jpg"},{title:"DJ-RN",description:"A 2D-3D jointlt learning framework for Human-Object Interaction detection.",url:"https://github.com/DirtyHarryLYL/DJ-RN",image:"2020_CVPR_djrn.jpg"},{title:"HAKE 1.0",description:"Human Activity Knowledge Engine.",url:"http://hake-mvig.cn/home",image:"2019_hake1.jpg"},],I=[{date:"2018",location:"Human Activity Knowledge Engine",title:"HAKE",link:"http://hake-mvig.cn/home/",content:(0,n.jsxs)("p",{children:["Human Activity Knowledge Engine (",(0,n.jsx)(f,{}),") is a knowledge-driven system that aims at enabling intelligent agents to perceive human activities, reason human behavior logics, learn skills from human activities, and interact with objects and environments."]})},{date:"2022",location:"Object Concept Learning",title:"OCL",link:"/ocl",content:(0,n.jsx)("p",{children:"We propose a challenging Object Concept Learning (OCL) task to push the envelope of object understanding. It requires machines to reason out object affordances and simultaneously give the reason: what attributes make an object possesses these affordances."})},{date:"2023",location:"Unified Action Semantic Space",title:"Pangea",link:"/pangea",content:(0,n.jsx)("p",{children:"We design an action semantic space in view of verb taxonomy hierarchy and covering massive actions. Thus, we can gather multi-modal datasets into a unified database in a unified label system, i.e., bridging “isolated islands” into a “Pangea”. Accordingly, we propose a bidirectional mapping model between physical and semantic space to fully use Pangea."})},],P=[{title:"Beyond Object Recognition: A New Benchmark towards Object Concept Learning",author:"Yong-Lu Li, Yue Xu, Xinyu Xu, Xiaohan Mao, Yuan Yao, Siqi Liu, Cewu Lu",conf:"ICCV 2023",links:[["arXiv","https://arxiv.org/abs/2212.02710"],["PDF","https://arxiv.org/pdf/2212.02710.pdf"],["Project","https://mvig-rhos.com/ocl"],]},{title:"EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding",author:"Yue Xu, Yong-Lu Li#, Zhemin Huang, Michael Xu LIU, Cewu Lu, Yu-Wing Tai, Chi Keung Tang.",conf:"ICCV 2023",links:[["arXiv","https://arxiv.org/abs/"],["PDF","https://arxiv.org/pdf/.pdf"],["Project","https://mvig-rhos.com/"],]},{title:"Dynamic Context Removal: A General Training Strategy for Robust Models on Video Action Predictive Tasks",author:"Xinyu Xu, Yong-Lu Li#, Cewu Lu#.",conf:"IJCV 2023",links:[["arXiv","https://arxiv.org/abs/"],["PDF","https://arxiv.org/pdf/.pdf"],["Code","https://github.com/AllenXuuu/DCR"],]},{title:"Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection",author:"Yue Xu, Yong-Lu Li#, Kaitong Cui, Ziyu Wang, Cewu Lu, Yu-Wing Tai, Chi-Keung Tang",conf:"Preprint",links:[["arXiv","https://arxiv.org/abs/2305.18381"],["PDF","https://arxiv.org/pdf/2305.18381.pdf"],]},{title:"From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding",author:"Yong-Lu Li*, Xiaoqian Wu*, Xinpeng Liu, Yiming Dou, Yikun Ji, Junyi Zhang, Yixing Li, Jingru Tan, Xudong Lu, Cewu Lu",conf:"Preprint",links:[["arXiv","https://arxiv.org/abs/2304.00553"],["PDF","https://arxiv.org/pdf/2304.00553.pdf"],["Project","https://mvig-rhos.com/pangea"],]},{title:"Discovering A Variety of Objects in Spatio-Temporal Human-Object Interactions",author:"Yong-Lu Li*, Hongwei Fan*, Zuoyu Qiu, Yiming Dou, Liang Xu, Hao-Shu Fang, Peiyang Guo, Haisheng Su, Dongliang Wang, Wei Wu, Cewu Lu",conf:"Tech Report",links:[["arXiv","https://arxiv.org/abs/2211.07501"],["PDF","https://arxiv.org/pdf/2211.07501.pdf"],["Code & Data","https://github.com/DirtyHarryLYL/HAKE-AVA"],],github:"https://github.com/DirtyHarryLYL/HAKE-AVA",content:"A part of the HAKE Project"},{title:"HAKE: Human Activity Knowledge Engine",author:"Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu, Mingyang Chen, Ze Ma, Shiyi Wang, Hao-Shu Fang, Cewu Lu",conf:"Tech Report",shortname:(0,n.jsxs)("span",{children:[(0,n.jsx)(f,{}),"1.0"]}),links:[["arXiv","https://arxiv.org/abs/1904.06539"],["PDF","https://arxiv.org/pdf/1904.06539.pdf"],["Project","http://hake-mvig.cn"],["Code","https://github.com/DirtyHarryLYL/HAKE"],],content:(0,n.jsx)("table",{children:(0,n.jsxs)("tbody",{children:[(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{children:"Main Repo: "}),(0,n.jsxs)("td",{children:["HAKE ",(0,n.jsx)(s.Z,{href:"https://github.com/DirtyHarryLYL/HAKE"})]}),(0,n.jsx)("td",{})]}),(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{children:"Sub-repos: "}),(0,n.jsxs)("td",{children:["Torch ",(0,n.jsx)(s.Z,{href:"https://github.com/DirtyHarryLYL/HAKE-Action-Torch"})]}),(0,n.jsxs)("td",{children:["TF ",(0,n.jsx)(s.Z,{href:"https://github.com/DirtyHarryLYL/HAKE-Action"})]}),(0,n.jsxs)("td",{children:["HAKE-AVA ",(0,n.jsx)(s.Z,{href:"https://github.com/DirtyHarryLYL/HAKE-AVA"})]})]}),(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{}),(0,n.jsxs)("td",{children:["Halpe ",(0,n.jsx)(s.Z,{href:"https://github.com/Fang-Haoshu/Halpe-FullBody"})]}),(0,n.jsxs)("td",{children:["HOI List ",(0,n.jsx)(s.Z,{href:"https://github.com/DirtyHarryLYL/HOI-Learning-List"})]})]})]})})},{title:"HAKE: A Knowledge Engine Foundation for Human Activity Understanding",author:"Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Yizhuo Li, Zuoyu Qiu, Liang Xu, Yue Xu, Hao-Shu Fang, Cewu Lu",conf:"TPAMI 2023",shortname:(0,n.jsxs)("span",{children:[(0,n.jsx)(f,{}),"2.0"]}),links:[["arXiv","https://arxiv.org/abs/2202.06851"],["PDF","https://arxiv.org/pdf/2202.06851.pdf"],["Project","http://hake-mvig.cn"],["Press","https://mp.weixin.qq.com/s/0KoPD7SAaaFKycmTUDBPOg"],]},{title:"AlphaPose: Whole-Body Regional Multi-Person Pose Estimation and Tracking in Real-Time",author:"Hao-Shu Fang*, Jiefeng Li*, Hongyang Tang, Chao Xu, Haoyi Zhu, Yuliang Xiu, Yong-Lu Li, Cewu Lu",conf:"TPAMI 2022",links:[["arXiv","https://arxiv.org/abs/2211.03375"],["PDF","https://arxiv.org/pdf/2211.03375.pdf"],["Code","https://github.com/MVIG-SJTU/AlphaPose"],],github:"https://github.com/MVIG-SJTU/AlphaPose"},{title:"Constructing Balance from Imbalance for Long-tailed Image Recognition",author:"Yue Xu*, Yong-Lu Li*, Jiefeng Li, Cewu Lu",conf:"ECCV 2022",shortname:"DLSA",links:[["arXiv","https://arxiv.org/abs/2208.02567"],["PDF","https://arxiv.org/pdf/2208.02567.pdf"],["Code","https://github.com/silicx/DLSA"],],github:"https://github.com/silicx/DLSA"},{title:"Mining Cross-Person Cues for Body-Part Interactiveness Learning in HOI Detection",author:"Xiaoqian Wu*, Yong-Lu Li*, Xinpeng Liu, Junyi Zhang, Yuzhe Wu, Cewu Lu",conf:"ECCV 2022",shortname:"PartMap",links:[["arXiv","https://arxiv.org/abs/2207.14192"],["PDF","https://arxiv.org/pdf/2207.14192v1.pdf"],["Code","https://github.com/enlighten0707/Body-Part-Map-for-Interactiveness"],],github:"https://github.com/enlighten0707/Body-Part-Map-for-Interactiveness"},{title:"Interactiveness Field of Human-Object Interactions",author:"Xinpeng Liu*, Yong-Lu Li*, Xiaoqian Wu, Yu-Wing Tai, Cewu Lu, Chi Keung Tang",conf:"CVPR 2022",links:[["arXiv","https://arxiv.org/abs/2204.07718"],["PDF","https://arxiv.org/pdf/2204.07718.pdf"],["Code","https://github.com/Foruck/Interactiveness-Field"],],github:"https://github.com/Foruck/Interactiveness-Field"},{title:"Human Trajectory Prediction with Momentary Observation",author:"Jianhua Sun, Yuxuan Li, Liang Chai, Hao-Shu Fang, Yong-Lu Li, Cewu Lu",conf:"CVPR 2022",links:[["PDF","https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_Human_Trajectory_Prediction_With_Momentary_Observation_CVPR_2022_paper.pdf",],]},{title:"Learn to Anticipate Future with Dynamic Context Removal",author:"Xinyu Xu, Yong-Lu Li, Cewu Lu",conf:"CVPR 2022",shortname:"DCR",links:[["arXiv","https://arxiv.org/abs/2204.02587"],["PDF","https://arxiv.org/pdf/2204.02587.pdf"],["Code","https://github.com/AllenXuuu/DCR"],],github:"https://github.com/AllenXuuu/DCR"},{title:"Canonical Voting: Towards Robust Oriented Bounding Box Detection in 3D Scenes",author:"Yang You, Zelin Ye, Yujing Lou, Chengkun Li, Yong-Lu Li, Lizhuang Ma, Weiming Wang, Cewu Lu",conf:"CVPR 2022",links:[["arXiv","https://arxiv.org/abs/2011.12001"],["PDF","https://arxiv.org/pdf/2011.12001.pdf"],["Code","https://github.com/qq456cvb/CanonicalVoting"],],github:"https://github.com/qq456cvb/CanonicalVoting"},{title:"UKPGAN: Unsupervised KeyPoint GANeration",author:"Yang You, Wenhai Liu, Yong-Lu Li, Weiming Wang, Cewu Lu",conf:"CVPR 2022",links:[["arXiv","https://arxiv.org/abs/2011.11974"],["PDF","https://arxiv.org/pdf/2011.11974.pdf"],["Code","https://github.com/qq456cvb/UKPGAN"],],github:"https://github.com/qq456cvb/UKPGAN"},{title:"Highlighting Object Category Immunity for the Generalization of Human-Object Interaction Detection",author:"Xinpeng Liu*, Yong-Lu Li*, Cewu Lu",conf:"AAAI 2022",links:[["arXiv","https://arxiv.org/abs/2202.09492"],["PDF","https://arxiv.org/pdf/2202.09492.pdf"],["Code","https://github.com/Foruck/OC-Immunity"],],github:"https://github.com/Foruck/OC-Immunity"},{title:"Learning Single/Multi-Attribute of Object with Symmetry and Group",author:"Yong-Lu Li, Yue Xu, Xinyu Xu, Xiaohan Mao, Cewu Lu",conf:"TPAMI 2021",shortname:"SymNet",links:[["arXiv","https://arxiv.org/abs/2110.04603"],["PDF","https://arxiv.org/pdf/2110.04603.pdf"],["Code","https://github.com/DirtyHarryLYL/SymNet"],],github:"https://github.com/DirtyHarryLYL/SymNet",content:"An extension of our CVPR 2020 work (Symmetry and Group in Attribute-Object Compositions, SymNet)."},{title:"Localization with Sampling-Argmax",author:"Jiefeng Li, Tong Chen, Ruiqi Shi, Yujing Lou, Yong-Lu Li, Cewu Lu",conf:"NeurIPS 2021",links:[["arXiv","https://arxiv.org/abs/2110.08825"],["PDF","https://arxiv.org/pdf/2110.08825.pdf"],["Code","https://github.com/Jeff-sjtu/sampling-argmax"],],github:"https://github.com/Jeff-sjtu/sampling-argmax"},{title:"Transferable Interactiveness Knowledge for Human-Object Interaction Detection",author:"Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Xijie Huang, Liang Xu, Cewu Lu",conf:"TPAMI 2021",shortname:"TIN++",links:[["arXiv","https://arxiv.org/abs/2101.10292"],["PDF","https://arxiv.org/pdf/2101.10292.pdf"],["Code","https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network"],],github:"https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network",content:"An extension of our CVPR 2019 work (Transferable Interactiveness Network, TIN)."},{title:"DecAug: Augmenting HOI Detection via Decomposition",author:"Yichen Xie, Hao-Shu Fang, Dian Shao, Yong-Lu Li, Cewu Lu",conf:"AAAI 2021",links:[["arXiv","https://arxiv.org/abs/2010.01007"],["PDF","https://arxiv.org/pdf/2010.01007.pdf"],]},{title:"HOI Analysis: Integrating and Decomposing Human-Object Interaction",author:"Yong-Lu Li*, Xinpeng Liu*, Xiaoqian Wu, Yizhuo Li, Cewu Lu",conf:"NeurIPS 2020",shortname:"IDN",links:[["arXiv","https://arxiv.org/abs/2010.16219"],["PDF","https://arxiv.org/pdf/2010.16219.pdf"],["Code","https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network)"],["Project: HAKE-Action-Torch","https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/master"],],github:"https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network)"},{title:"PaStaNet: Toward Human Activity Knowledge Engine",author:"Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu, Shiyi Wang, Hao-Shu Fang, Ze Ma, Mingyang Chen, Cewu Lu.",conf:"CVPR 2020",links:[["arXiv","https://arxiv.org/abs/2004.00945"],["PDF","https://arxiv.org/pdf/2004.00945.pdf"],["Video","https://drive.google.com/file/d/16PCCK_flK2qW4QJVWoYXQwG3wgXd6yvT/view?usp=sharing"],["Slides","https://drive.google.com/file/d/19J9uz3epBo3o9CIU85mzgLblRVNawl87/view?usp=sharing"],["Data","https://github.com/DirtyHarryLYL/HAKE"],["Code","https://github.com/DirtyHarryLYL/HAKE-Action"],],github:"https://github.com/DirtyHarryLYL/HAKE-Action",content:(0,n.jsxs)("p",{children:[(0,n.jsx)("strong",{className:"text-red-600",children:"Oral Talk:"})," ",(0,n.jsx)("a",{href:"http://ai.stanford.edu/~jingweij/cicv/#schedule",children:"Compositionality in Computer Vision"})," in CVPR 2020"]})},{title:"Detailed 2D-3D Joint Representation for Human-Object Interaction",author:"Yong-Lu Li, Xinpeng Liu, Han Lu, Shiyi Wang, Junqi Liu, Jiefeng Li, Cewu Lu",conf:"CVPR 2020",shortname:"DJ-RN",links:[["arXiv","https://arxiv.org/abs/2004.08154"],["PDF","https://arxiv.org/pdf/2004.08154.pdf"],["Video","https://drive.google.com/file/d/14dK1tBLe3xHXyO_5WsRO2JcjJ95meaDB/view?usp=sharing"],["Slides","https://drive.google.com/file/d/1A5bQZFsBOahj7dJgSnWR7jdyvMG58NkT/view?usp=sharing"],["Benchmark: Ambiguous-HOI","https://github.com/DirtyHarryLYL/DJ-RN#ambiguous-hoi"],["Code","https://github.com/DirtyHarryLYL/DJ-RN"],],github:"https://github.com/DirtyHarryLYL/DJ-RN"},{title:"Symmetry and Group in Attribute-Object Compositions",author:"Yong-Lu Li, Yue Xu, Xiaohan Mao, Cewu Lu",conf:"CVPR 2020",shortname:"SymNet",links:[["arXiv","https://arxiv.org/abs/2004.00587"],["PDF","https://arxiv.org/pdf/2004.00587.pdf"],["Video","https://drive.google.com/file/d/1ZTSB2lJbDTH7D-7GdEJQGmszvEc2Vuwd/view?usp=sharing"],["Slides","https://drive.google.com/file/d/1aqYeSIQkoTp1hYOJokDgoucdZcufN2iG/view?usp=sharing"],["Code","https://github.com/DirtyHarryLYL/SymNet"],],github:"https://github.com/DirtyHarryLYL/SymNet"},{title:"InstaBoost: Boosting Instance Segmentation Via Probability Map Guided Copy-Pasting",author:"Hao-Shu Fang*, Jianhua Sun*, Runzhong Wang*, Minghao Gou, Yong-Lu Li, Cewu Lu",conf:"ICCV 2019",links:[["arXiv","https://arxiv.org/abs/1908.07801"],["PDF","https://arxiv.org/pdf/1908.07801.pdf"],["Code","https://github.com/GothicAi/Instaboost"],],github:"https://github.com/GothicAi/Instaboost"},{title:"Transferable Interactiveness Knowledge for Human-Object Interaction Detection",author:"Yong-Lu Li, Siyuan Zhou, Xijie Huang, Liang Xu, Ze Ma, Hao-Shu Fang, Yan-Feng Wang, Cewu Lu",conf:"CVPR 2019",shortname:"TIN",links:[["arXiv","https://arxiv.org/abs/1811.08264"],["PDF","https://arxiv.org/pdf/1811.08264.pdf"],["Code","https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network"],],github:"https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network"},{title:"SRDA: Generating Instance Segmentation Annotation via Scanning, Reasoning and Domain Adaptation",author:"Wenqiang Xu*, Yong-Lu Li*, Cewu Lu",conf:"ECCV 2018",links:[["arXiv","https://arxiv.org/abs/1801.08839"],["PDF","https://arxiv.org/pdf/1801.08839.pdf"],["Dataset (Instance-60k & 3D Object Models)","https://drive.google.com/drive/folders/1t941oiLk40XQX2Q9a2HPmiDRUpiwazJO?usp=sharing",],["Code","https://github.com/DirtyHarryLYL/SRDA-ECCV2018"],],github:"https://github.com/DirtyHarryLYL/SRDA-ECCV2018"},{title:"Beyond Holistic Object Recognition: Enriching Image Understanding with Part States",author:"Cewu Lu, Hao Su, Yong-Lu Li, Yongyi Lu, Li Yi, Chi-Keung Tang, Leonidas J. Guibas",conf:"CVPR 2018",links:[["PDF","http://ai.ucsd.edu/~haosu/papers/cvpr18_partstate.pdf"]]},{title:"Optimization of Radial Distortion Self-Calibration for Structure from Motion from Uncalibrated UAV Images",author:"Yong-Lu Li, Yinghao Cai, Dayong Wen, Yiping Yang",conf:"ICPR 2016",links:[["PDF","https://projet.liris.cnrs.fr/imagine/pub/proceedings/ICPR-2016/media/files/1010.pdf"]]},],S=[{name:"Cewu Lu",position:"Professor",url:"https://www.mvig.org/"},{name:"Yong-Lu Li",position:"Assistant Professor",url:"https://dirtyharrylyl.github.io/"},{name:"Xinpeng Liu",position:"PhD. Student",url:"https://foruck.github.io/"},{name:"Yue Xu",position:"PhD. Student",url:"https://silicx.github.io/"},{name:"Xiaoqian Wu",position:"PhD. Student",url:"https://scholar.google.com/citations?user=-PHR96oAAAAJ"},{name:"Siqi Liu",position:"PhD. Student",url:"https://github.com/MayuOshima/"},{name:"Xinyu Xu",position:"Master Student",url:"https://xuxinyu.website"},{name:"Yusong Qiu",position:"Master Student",altimage:"placeholder.png",url:""},],k=[{name:"Kaitong Cui",position:"HKU, Intern ",url:""},{name:"Junyi Zhang",position:"UC Merced, Intern",url:"https://www.junyi42.com/"},{name:"Yikun Ji",position:"",url:"http://github.com/Gennadiyev/"},{name:"Yiming Dou",position:"UMich, Ph.D.",url:"https://dou-yiming.github.io/"},{name:"Xiaohan Mao",position:"Shanghai AI Lab & SJTU, Ph.D.",url:"https://scholar.google.com/citations?user=-zT1NKwAAAAJ"},{name:"Zhemin Huang",position:"Stanford University, MS"},{name:"Yuzhe Wu",position:""},{name:"Shaopeng Guo",position:"UCSD, Ph.D.",url:"https://coeusguo.github.io/"},{name:"Xudong Lu",position:"CUHK, Ph.D."},{name:"Hongwei Fan",position:"Sensetime, Research Engineer"},{name:"Yuan Yao",position:"U of Rochester, Ph.D."},{name:"Zuoyu Qiu",position:"SJTU, MS"},{name:"Junqi Liu",position:""},{name:"Han Lu",position:"SJTU, Ph.D."},{name:"Shiyi Wang",position:""},{name:"Zhanke Zhou",position:"HKBU, Ph.D."},{name:"Mingyang Chen",position:"UCSD, MS"},{name:"Siyuan Zhou",position:"SJTU, MS"},{name:"Liang Xu",position:"EIAS & SJTU, Ph.D.",url:"https://liangxuy.github.io/"},{name:"Ze Ma",position:"Columbia University, MS",url:"https://maqingyang.github.io/"},{name:"Xijie Huang",url:"https://huangowen.github.io/",position:"HKUST, Ph.D."},]}}]);