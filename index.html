<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>RHOS</title><meta content="Homepage of RHOS" name="description"/><link href="https://http://mvig-rhos.com/" rel="canonical"/><link href="/favicon.ico" rel="icon" sizes="any"/><link href="/icon.svg" rel="icon" type="image/svg+xml"/><link href="/apple-touch-icon.png" rel="apple-touch-icon"/><link href="/site.webmanifest" rel="manifest"/><meta content="RHOS" property="og:title"/><meta content="Homepage of RHOS" property="og:description"/><meta content="https://http://mvig-rhos.com/" property="og:url"/><meta content="RHOS" name="twitter:title"/><meta content="Homepage of RHOS" name="twitter:description"/><link rel="preload" as="image" href="/_next/static/media/Robotics_Cyberpunk_2077_bg.f50d6e01.png"/><meta name="next-head-count" content="15"/><meta charSet="utf-8"/><meta content="notranslate" name="google"/><link rel="preload" href="/_next/static/css/3ba891e686b9e20a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/3ba891e686b9e20a.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8d2c45f9a1616a86.js" defer=""></script><script src="/_next/static/chunks/framework-7751730b10fa0f74.js" defer=""></script><script src="/_next/static/chunks/main-a9b50f256c2cfb57.js" defer=""></script><script src="/_next/static/chunks/pages/_app-7194e30148ab5f37.js" defer=""></script><script src="/_next/static/chunks/342-08dd1fd52b1d6335.js" defer=""></script><script src="/_next/static/chunks/675-717e3cc8fb67a947.js" defer=""></script><script src="/_next/static/chunks/238-4a0e50b92cdff6c7.js" defer=""></script><script src="/_next/static/chunks/pages/index-a7f87d3830f8c6eb.js" defer=""></script><script src="/_next/static/aCD78StrAO3V-az-E8ppv/_buildManifest.js" defer=""></script><script src="/_next/static/aCD78StrAO3V-az-E8ppv/_ssgManifest.js" defer=""></script></head><body class="bg-neutral-900"><div id="__next"><section class="" id="hero"><div class=""><div class="relative flex h-screen w-screen items-center justify-center"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:absolute;top:0;left:0;bottom:0;right:0"><img alt="RHOS-image" src="/_next/static/media/Robotics_Cyberpunk_2077_bg.f50d6e01.png" decoding="async" data-nimg="fill" class="absolute z-0" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAIAAAD38zoCAAAAhUlEQVR42mPw8DGPDLMz0ZMzs7OMTwpVNTAI8LayN1dikDXTUbUx9nDQD/K01jHSUzOyNrd1NjbUZVDQkbGw1zO1NXJ1srS2MjKysLK0tPEOiWCwMZVJ8FUvTbToKLSfUe9eHKXhbCrEIynL4GuvVJLiUp/jVZVqmxWqGuggpaciLCDACQABSR1UPIEuQgAAAABJRU5ErkJggg==&quot;)"/><noscript><img alt="RHOS-image" src="/_next/static/media/Robotics_Cyberpunk_2077_bg.f50d6e01.png" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="absolute z-0"/></noscript></span><div class="z-10 w-full max-w-screen-lg px-4 lg:px-0"><div class="flex flex-col items-center gap-y-6 rounded-xl bg-gray-800/40 p-6 text-center shadow-lg backdrop-blur-sm"><h1 class="text-4xl font-bold text-white sm:text-5xl lg:text-7xl">RHOS</h1><p class="prose-sm text-stone-200 sm:prose-base lg:prose-lg">Robot • Human • Object • Scene</p><div class="flex w-5/6 justify-center gap-x-4"><a class="flex gap-x-2 rounded-full border-2 bg-none py-2 px-4 text-sm font-medium text-white ring-offset-gray-700/80 hover:bg-gray-700/80 focus:outline-none focus:ring-2 focus:ring-offset-2 sm:text-base border-orange-500 ring-orange-500" href="#recruit">Recruit</a><a class="flex gap-x-2 rounded-full border-2 bg-none py-2 px-4 text-sm font-medium text-white ring-offset-gray-700/80 hover:bg-gray-700/80 focus:outline-none focus:ring-2 focus:ring-offset-2 sm:text-base border-white ring-white" href="https://github.com/mvig-rhos"><svg class="h-5 w-5 text-white sm:h-6 sm:w-6" fill="currentColor" viewBox="0 0 128 128" width="128" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M64 5.103c-33.347 0-60.388 27.035-60.388 60.388 0 26.682 17.303 49.317 41.297 57.303 3.017.56 4.125-1.31 4.125-2.905 0-1.44-.056-6.197-.082-11.243-16.8 3.653-20.345-7.125-20.345-7.125-2.747-6.98-6.705-8.836-6.705-8.836-5.48-3.748.413-3.67.413-3.67 6.063.425 9.257 6.223 9.257 6.223 5.386 9.23 14.127 6.562 17.573 5.02.542-3.903 2.107-6.568 3.834-8.076-13.413-1.525-27.514-6.704-27.514-29.843 0-6.593 2.36-11.98 6.223-16.21-.628-1.52-2.695-7.662.584-15.98 0 0 5.07-1.623 16.61 6.19C53.7 35 58.867 34.327 64 34.304c5.13.023 10.3.694 15.127 2.033 11.526-7.813 16.59-6.19 16.59-6.19 3.287 8.317 1.22 14.46.593 15.98 3.872 4.23 6.215 9.617 6.215 16.21 0 23.194-14.127 28.3-27.574 29.796 2.167 1.874 4.097 5.55 4.097 11.183 0 8.08-.07 14.583-.07 16.572 0 1.607 1.088 3.49 4.148 2.897 23.98-7.994 41.263-30.622 41.263-57.294C124.388 32.14 97.35 5.104 64 5.104z" fill-rule="evenodd"></path><path d="M26.484 91.806c-.133.3-.605.39-1.035.185-.44-.196-.685-.605-.543-.906.13-.31.603-.395 1.04-.188.44.197.69.61.537.91zm2.446 2.729c-.287.267-.85.143-1.232-.28-.396-.42-.47-.983-.177-1.254.298-.266.844-.14 1.24.28.394.426.472.984.17 1.255zM31.312 98.012c-.37.258-.976.017-1.35-.52-.37-.538-.37-1.183.01-1.44.373-.258.97-.025 1.35.507.368.545.368 1.19-.01 1.452zm3.261 3.361c-.33.365-1.036.267-1.552-.23-.527-.487-.674-1.18-.343-1.544.336-.366 1.045-.264 1.564.23.527.486.686 1.18.333 1.543zm4.5 1.951c-.147.473-.825.688-1.51.486-.683-.207-1.13-.76-.99-1.238.14-.477.823-.7 1.512-.485.683.206 1.13.756.988 1.237zm4.943.361c.017.498-.563.91-1.28.92-.723.017-1.308-.387-1.315-.877 0-.503.568-.91 1.29-.924.717-.013 1.306.387 1.306.88zm4.598-.782c.086.485-.413.984-1.126 1.117-.7.13-1.35-.172-1.44-.653-.086-.498.422-.997 1.122-1.126.714-.123 1.354.17 1.444.663zm0 0"></path></svg>Github</a><a class="flex gap-x-2 rounded-full border-2 bg-none py-2 px-4 text-sm font-medium text-white ring-offset-gray-700/80 hover:bg-gray-700/80 focus:outline-none focus:ring-2 focus:ring-offset-2 sm:text-base border-white ring-white" href="https://mvig-rhos.com/clean-demos">Demos</a></div></div></div><div class="absolute inset-x-0 bottom-6 flex justify-center"><a class="rounded-full bg-white p-1 ring-white ring-offset-2 ring-offset-gray-700/80 focus:outline-none focus:ring-2 sm:p-2" href="/#about"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="h-5 w-5 bg-transparent sm:h-6 sm:w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M19 9l-7 7-7-7"></path></svg></a></div></div></div></section><section class="bg-neutral-800 px-4 py-8 md:py-12 lg:px-8" id="about"><div class="mx-auto max-w-screen-lg"><div class="grid grid-cols-1 gap-y-4 md:grid-cols-4"><div class="col-span-1 flex justify-center md:justify-start"><h2 class="text-2xl font-bold text-white">RHOS</h2></div><div class="col-span-1 flex flex-col gap-y-6 md:col-span-3"><div class="flex flex-col gap-y-2"><h2 class="text-2xl font-bold text-white">About</h2><div class="prose prose-sm text-gray-300 sm:prose-base"><div><p>Hi, this is the website of the RHOS lab. We study<!-- --> <i><b>Embodied AI</b></i>,<!-- --> <i><b>Physical Reasoning</b></i>, and<!-- --> <i><b>Human Activity Understanding</b></i>. We are building a knowledge and reasoning-driven system that enables intelligent agents/robots to perceive human activities, reason human behavior logics, learn skills from human activities and interact with the environment.</p><p><b>Research Interests: </b></p><p>(S) <b>Embodied AI</b>: how to make agents learn skills from humans and interact with humans &amp; scenes &amp; objects.<br/>(S-1) <b>Human Activity Understanding</b>: how to learn and ground complex/ambiguous human activity concepts (body motion, human-object/human/scene interaction) and object concepts from multi-modal information (2D-3D-4D).<br/>(S-2) <b>Visual Reasoning</b>: how to mine, capture, and embed the logic and causal relations from human activities.<br/>(S-3) <b>General Multi-Modal Foundation Models</b>: especially for human-centric perception tasks.<br/>(S-4) <b>Activity Understanding from A Cognitive Perspective</b>: work with multidisciplinary researchers to study how the brain perceives activities.<br/>(E) <b>Human-Robot Interaction for hospital, home, factory, etc.</b>: work with experts in different domains to develop intelligent robots to help people.</p></div></div><h2 class="text-2xl font-bold text-white">Contact</h2><div class="prose prose-sm text-gray-300 sm:prose-base"><div><b>Yong-Lu Li</b><br/>Email: yonglu_li[at]sjtu[dot]edu[dot]cn Shanghai Jiao Tong University Shanghai Innovation Institute</div></div><div><a class="mr-2 font-bold text-sky-200 underline block" href="https://dirtyharrylyl.github.io/"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="inline h-5 w-5 text-white"><path stroke-linecap="round" stroke-linejoin="round" d="M5 3v4M3 5h4M6 17v4m-2-2h4m5-16l2.286 6.857L21 12l-5.714 2.143L13 21l-2.286-6.857L5 12l5.714-2.143L13 3z"></path></svg>Personal Website</a><a class="mr-2 font-bold text-sky-200 underline" href="https://scholar.google.com.hk/citations?user=UExAaVgAAAAJ"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="inline h-5 w-5 text-white"><path d="M12 14l9-5-9-5-9 5 9 5z"></path><path d="M12 14l6.16-3.422a12.083 12.083 0 01.665 6.479A11.952 11.952 0 0012 20.055a11.952 11.952 0 00-6.824-2.998 12.078 12.078 0 01.665-6.479L12 14z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M12 14l9-5-9-5-9 5 9 5zm0 0l6.16-3.422a12.083 12.083 0 01.665 6.479A11.952 11.952 0 0012 20.055a11.952 11.952 0 00-6.824-2.998 12.078 12.078 0 01.665-6.479L12 14zm-4 6v-7.5l4-2.222"></path></svg>Google Scholar </a><a class="mr-2 font-bold text-sky-200 underline" href="https://github.com/DirtyHarryLYL"><svg class="inline h-5 w-5 text-white" fill="currentColor" viewBox="0 0 128 128" width="128" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M64 5.103c-33.347 0-60.388 27.035-60.388 60.388 0 26.682 17.303 49.317 41.297 57.303 3.017.56 4.125-1.31 4.125-2.905 0-1.44-.056-6.197-.082-11.243-16.8 3.653-20.345-7.125-20.345-7.125-2.747-6.98-6.705-8.836-6.705-8.836-5.48-3.748.413-3.67.413-3.67 6.063.425 9.257 6.223 9.257 6.223 5.386 9.23 14.127 6.562 17.573 5.02.542-3.903 2.107-6.568 3.834-8.076-13.413-1.525-27.514-6.704-27.514-29.843 0-6.593 2.36-11.98 6.223-16.21-.628-1.52-2.695-7.662.584-15.98 0 0 5.07-1.623 16.61 6.19C53.7 35 58.867 34.327 64 34.304c5.13.023 10.3.694 15.127 2.033 11.526-7.813 16.59-6.19 16.59-6.19 3.287 8.317 1.22 14.46.593 15.98 3.872 4.23 6.215 9.617 6.215 16.21 0 23.194-14.127 28.3-27.574 29.796 2.167 1.874 4.097 5.55 4.097 11.183 0 8.08-.07 14.583-.07 16.572 0 1.607 1.088 3.49 4.148 2.897 23.98-7.994 41.263-30.622 41.263-57.294C124.388 32.14 97.35 5.104 64 5.104z" fill-rule="evenodd"></path><path d="M26.484 91.806c-.133.3-.605.39-1.035.185-.44-.196-.685-.605-.543-.906.13-.31.603-.395 1.04-.188.44.197.69.61.537.91zm2.446 2.729c-.287.267-.85.143-1.232-.28-.396-.42-.47-.983-.177-1.254.298-.266.844-.14 1.24.28.394.426.472.984.17 1.255zM31.312 98.012c-.37.258-.976.017-1.35-.52-.37-.538-.37-1.183.01-1.44.373-.258.97-.025 1.35.507.368.545.368 1.19-.01 1.452zm3.261 3.361c-.33.365-1.036.267-1.552-.23-.527-.487-.674-1.18-.343-1.544.336-.366 1.045-.264 1.564.23.527.486.686 1.18.333 1.543zm4.5 1.951c-.147.473-.825.688-1.51.486-.683-.207-1.13-.76-.99-1.238.14-.477.823-.7 1.512-.485.683.206 1.13.756.988 1.237zm4.943.361c.017.498-.563.91-1.28.92-.723.017-1.308-.387-1.315-.877 0-.503.568-.91 1.29-.924.717-.013 1.306.387 1.306.88zm4.598-.782c.086.485-.413.984-1.126 1.117-.7.13-1.35-.172-1.44-.653-.086-.498.422-.997 1.122-1.126.714-.123 1.354.17 1.444.663zm0 0"></path></svg>Github </a><a class="mr-2 font-bold text-sky-200 underline" href="https://www.linkedin.com/in/%E6%B0%B8%E9%9C%B2-%E6%9D%8E-991b99139/"><svg class="inline h-5 w-5 text-white" fill="currentColor" viewBox="0 0 128 128" width="128" xmlns="http://www.w3.org/2000/svg"><path d="M116 3H12a8.91 8.91 0 00-9 8.8v104.42a8.91 8.91 0 009 8.78h104a8.93 8.93 0 009-8.81V11.77A8.93 8.93 0 00116 3zM39.17 107H21.06V48.73h18.11zm-9-66.21a10.5 10.5 0 1110.49-10.5 10.5 10.5 0 01-10.54 10.48zM107 107H88.89V78.65c0-6.75-.12-15.44-9.41-15.44s-10.87 7.36-10.87 15V107H50.53V48.73h17.36v8h.24c2.42-4.58 8.32-9.41 17.13-9.41C103.6 47.28 107 59.35 107 75z" fill="currentColor"></path></svg>LinkedIn</a><a class="mr-2 font-bold text-sky-200 underline" href="https://dblp.org/pid/198/9345.html"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="inline h-5 w-5 text-white"><path stroke-linecap="round" stroke-linejoin="round" d="M9 20l-5.447-2.724A1 1 0 013 16.382V5.618a1 1 0 011.447-.894L9 7m0 13l6-3m-6 3V7m6 10l4.553 2.276A1 1 0 0021 18.382V7.618a1 1 0 00-.553-.894L15 4m0 13V4m0 0L9 7"></path></svg>dblp</a><a class="mr-2 font-bold text-sky-200 underline" href="https://www.semanticscholar.org/author/Yong-Lu-Li/10384643"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="inline h-5 w-5 text-white"><path stroke-linecap="round" stroke-linejoin="round" d="M9 20l-5.447-2.724A1 1 0 013 16.382V5.618a1 1 0 011.447-.894L9 7m0 13l6-3m-6 3V7m6 10l4.553 2.276A1 1 0 0021 18.382V7.618a1 1 0 00-.553-.894L15 4m0 13V4m0 0L9 7"></path></svg>Semantic Scholar</a><a class="mr-2 font-bold text-sky-200 underline block" href="https://mvig-rhos.com/clean-demos"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="inline h-5 w-5 text-white"><path stroke-linecap="round" stroke-linejoin="round" d="M5 3v4M3 5h4M6 17v4m-2-2h4m5-16l2.286 6.857L21 12l-5.714 2.143L13 21l-2.286-6.857L5 12l5.714-2.143L13 3z"></path></svg>Robot Demos</a></div></div></div></div></div></section><section class="bg-neutral-100 px-4 py-8 md:py-12 lg:px-8" id="recruit"><div class="mx-auto max-w-screen-lg"><div class="flex flex-col divide-y-2 divide-neutral-300"><div class="grid grid-cols-1 gap-y-4 py-8 first:pt-0 last:pb-0 md:grid-cols-4"><div class="col-span-1 flex justify-center md:justify-start"><div class="relative h-max"><h2 class="text-xl font-bold uppercase text-neutral-800">Recruitment</h2><span class="absolute inset-x-0 border-b-2 border-orange-400"></span><div></div></div></div><div class="col-span-1 flex flex-col md:col-span-3"><span><p>We are actively looking for self-motivated<!-- --> <strong>students (Master/PhD, 2026 spring &amp; fall), interns/engineers/visitors</strong> <!-- -->(CV/ML/ROB/NLP/Math/Phys background, always welcome) to join us in<!-- --> <a class="text-red600" href="https://www.mvig.org/">Machine Vision and Intelligence Group (MVIG)</a>. If you have the same/similar interests, feel free to email me your resume.</p><p>Click: <!-- --> <a class="text-red-600" href="https://dirtyharrylyl.github.io/recruit.html"><b>Eng</b></a> <!-- --> or<a class="text-red-600" href="https://mvig-rhos.com/recruit"><b> 中</b></a> <!-- -->for more details.</p></span></div></div></div></div></section><section class="bg-neutral-100 px-4 py-8 md:py-12 lg:px-8" id="news"><div class="mx-auto max-w-screen-lg"><div class="flex flex-col divide-y-2 divide-neutral-300"><div class="grid grid-cols-1 gap-y-4 py-8 first:pt-0 last:pb-0 md:grid-cols-4"><div class="col-span-1 flex justify-center md:justify-start"><div class="relative h-max"><h2 class="text-xl font-bold uppercase text-neutral-800">News and Olds</h2><span class="absolute inset-x-0 border-b-2 border-orange-400"></span><div></div></div></div><div class="col-span-1 flex flex-col md:col-span-3"><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2025.8<!-- -->] </span><span class="flex-1 sm:flex-none">Our GarmageNet will appear at SIGGRAPH Asia 2025!</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2025.8<!-- -->] </span><span class="flex-1 sm:flex-none">Our exUMI will appear at CoRL 2025!</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2025.6<!-- -->] </span><span class="flex-1 sm:flex-none">Our DensePolicy will appear at ICCV 2025!</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2025.5<!-- -->] </span><span class="flex-1 sm:flex-none">Our paper on human-robot joint learning has won the <a class="text-red-600">ICRA 2025 Best Paper Award on Human-Robot Interaction</a>!</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2025.4<!-- -->] </span><span class="flex-1 sm:flex-none">Our paper on human-robot joint learning has been selected as an ICRA 2025 Best Paper Award Finalist!</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2025.2<!-- -->] </span><span class="flex-1 sm:flex-none">Our work on 3D HOI reconstruction, motion dynamics, garment generation/reconstruction, and dynamic object segmentation will appear at CVPR 2025!</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2025.1<!-- -->] </span><span class="flex-1 sm:flex-none">Our work on efficient robot teleoperation will appear at ICRA 2025.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2025.1<!-- -->] </span><span class="flex-1 sm:flex-none">Two works on the association ability of LLM and human motion will appear at ICLR 2025.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2024.12<!-- -->] </span><span class="flex-1 sm:flex-none">Our work on video human-object interaction learning will appear at AAAI 2025.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2024.9<!-- -->] </span><span class="flex-1 sm:flex-none">Two works on articulated object image manipulation and humanoid-object interaction will appear at NeurIPS 2024!</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2024.7<!-- -->] </span><span class="flex-1 sm:flex-none">Five works on visual reasoning, 4D human motions, embodied AI, and dataset distillation will appear at ECCV 2024!</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2024.2<!-- -->] </span><span class="flex-1 sm:flex-none">Our work <b><a class="underline text-sky-600" href="https://mvig-rhos.com/pangea">Pangea</a></b> and <b><a class="underline text-sky-600" href="https://mvig-rhos.com/video-distill">Video Distillation</a></b> will appear at CVPR 2024.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2023.12<!-- -->] </span><span class="flex-1 sm:flex-none">Our work on <b>primitive-based HOI reconstruction (<a class="underline text-sky-600" href="https://mvig-rhos.com/p3haoi">P3HAOI</a>)</b> will appear at AAAI 2024!</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2023.9<!-- -->] </span><span class="flex-1 sm:flex-none">The advanced HAKE reasoning engine based on LLM (<a class="underline text-sky-600" href="https://mvig-rhos.com/symbol_llm">Symbol-LLM</a>) will appear at NeurIPS&#x27;23!</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2023.7<!-- -->] </span><span class="flex-1 sm:flex-none">Our works on <a class="underline text-sky-600" href="https://mvig-rhos.com/ego_pca">ego-centric video understanding</a> and <a class="underline text-sky-600" href="https://mvig-rhos.com/ocl">object concept learning</a> will appear at ICCV&#x27;23!</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2023.7<!-- -->] </span><span class="flex-1 sm:flex-none">The upgrade version of <a class="underline text-sky-600" href="https://github.com/AllenXuuu/DCR">DCR</a> will appear at IJCV!</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2022.12<!-- -->] </span><span class="flex-1 sm:flex-none"><a class="font-bold" href="https://arxiv.org/abs/2202.06851">HAKE 2.0</a> will appear at TPAMI!</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2022.12<!-- -->] </span><span class="flex-1 sm:flex-none">OCL (Object Concept Learning) is released on <a class="underline text-sky-600" href="https://arxiv.org/abs/2212.02710">arXiv</a>. Please visit the <a class="underline text-sky-600" href="/ocl">project page</a> for details.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2022.11<!-- -->] </span><span class="flex-1 sm:flex-none">We release the human body part states and interactive object bounding box annotations upon AVA (2.1 &amp; 2.2): <a class="font-bold" href="https://github.com/DirtyHarryLYL/HAKE-AVA">[HAKE-AVA]</a>, and a CLIP-based human part state &amp; verb recognizer: <a class="font-bold" href="https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/CLIP-Activity2Vec">[CLIP-Activity2Vec]</a>.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2022.11<!-- -->] </span><span class="flex-1 sm:flex-none"><a class="font-bold" href="https://github.com/MVIG-SJTU/AlphaPose">AlphaPose</a> will appear at TPAMI!</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2022.07<!-- -->] </span><span class="flex-1 sm:flex-none">Two papers on <b>longtailed learning, HOI detection</b> are accepted by ECCV&#x27;22, arXivs and code are coming soon</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2022.03<!-- -->] </span><span class="flex-1 sm:flex-none">Five papers on <b>HOI detection/prediction, trajection prediction, 3D detection/keypoints</b> are accepted by CVPR&#x27;22, papers and code are coming soon.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2022.02<!-- -->] </span><span class="flex-1 sm:flex-none">We release the human body part state labels based on AVA:<!-- --> <a class="font-bold" href="https://github.com/DirtyHarryLYL/HAKE-AVA">HAKE-AVA</a> and <a class="font-bold" href="https://arxiv.org/abs/2202.06851">HAKE 2.0</a>.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2021.12<!-- -->] </span><span class="flex-1 sm:flex-none">Our work on <b>HOI generalization</b> will appear at AAAI&#x27;22.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2021.10<!-- -->] </span><span class="flex-1 sm:flex-none"><b>Learning Single/Multi-Attribute of Object with Symmetry and Group</b> is accepted by TPAMI.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2021.09<!-- -->] </span><span class="flex-1 sm:flex-none">Our work <b>Localization with Sampling-Argmax</b> will appear at NeurIPS&#x27;21.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2021.02<!-- -->] </span><span class="flex-1 sm:flex-none">Upgraded<!-- --> <a class="font-bold" href="https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/Activity2Vec">HAKE-Activity2Vec</a> <!-- -->is released! Images/Videos --&gt; human box + ID + skeleton + part states + action + representation.<!-- --> <a class="underline" href="https://youtu.be/ty-bXDInLMQ">[Demo]</a> <a class="underline" href="https://drive.google.com/file/d/1iZ57hKjus2lKbv1MAB-TLFrChSoWGD5e/view?usp=sharing">[Description]</a></span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2021.01<!-- -->] </span><span class="flex-1 sm:flex-none"><b><a href="https://arxiv.org/abs/2101.10292">TIN</a> (Transferable Interactiveness Network)</b> <!-- -->is accepted by TPAMI.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2020.12<!-- -->] </span><span class="flex-1 sm:flex-none"><a class="font-bold underline" href="https://arxiv.org/abs/2010.01007">DecAug</a> <!-- -->is accepted by AAAI&#x27;21.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2020.09<!-- -->] </span><span class="flex-1 sm:flex-none">Our work <b>HOI Analysis</b> will appear at NeurIPS 2020.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2020.06<!-- -->] </span><span class="flex-1 sm:flex-none">The larger<!-- --> <a class="font-bold" href="https://github.com/DirtyHarryLYL/HAKE#hake-large-for-instance-level-hoi-detection">HAKE-Large</a> <!-- -->(&gt;120K images with activity and part state labels) is released.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2020.02<!-- -->] </span><span class="flex-1 sm:flex-none">Three papers <b>Image-based HAKE: PaSta-Net</b>, <b>2D-3D Joint HOI Learning</b>,<!-- --> <b>Symmetry-based Attribute-Object Learning</b> are accepted in <a href="http://cvpr2020.thecvf.com/">CVPR&#x27;20</a>! Papers and corresponding resources (code, data) will be released soon.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2019.07<!-- -->] </span><span class="flex-1 sm:flex-none">Our paper <b>InstaBoost</b> is accepted in <a href="http://iccv2019.thecvf.com/">ICCV&#x27;19</a>.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2019.06<!-- -->] </span><span class="flex-1 sm:flex-none">The Part I of our <a href="/hake"><b><span style="color:red">H</span><span style="color:blue">A</span><span style="color:red">KE</span></b></a>:<!-- --> <b><a href="http://hake-mvig.cn/download/">HAKE-HICO</a></b> <!-- -->which contains the image-level part-state annotations is released.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2019.04<!-- -->] </span><span class="flex-1 sm:flex-none">Our project <a href="/hake"><b><span style="color:red">H</span><span style="color:blue">A</span><span style="color:red">KE</span></b></a> (Human Activity Knowledge Engine) begins trial operation.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2019.02<!-- -->] </span><span class="flex-1 sm:flex-none">Our paper on<!-- --> <b><a href="https://arxiv.org/abs/1811.08264">Interactiveness</a></b> <!-- -->is accepted in <a href="http://cvpr2019.thecvf.com/">CVPR&#x27;19</a>.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2018.07<!-- -->] </span><span class="flex-1 sm:flex-none">Our paper on<!-- --> <b><a href="https://arxiv.org/abs/1801.08839">GAN &amp; Annotation Generation</a></b> <!-- -->is accepted in <a href="https://eccv2018.org/">ECCV&#x27;18</a>.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2018.05<!-- -->] </span><span class="flex-1 sm:flex-none">Presentation (Kaibot Team) in<!-- --> <a href="https://icra2018.org/tidy-up-my-room-challenge/">TIDY UP MY ROOM CHALLENGE | ICRA&#x27;18</a>.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2018.02<!-- -->] </span><span class="flex-1 sm:flex-none">Our paper on<!-- --> <b><a href="http://ai.ucsd.edu/~haosu/papers/cvpr18_partstate.pdf">Object Part States</a></b> <!-- -->is accepted in <a href="http://cvpr2018.thecvf.com/program/main_conference">CVPR&#x27;18</a>.</span></div></div></div></div></div></section><section class="bg-neutral-100 px-4 py-8 md:py-12 lg:px-8" id="projects"><div class="mx-auto max-w-screen-lg"><div class="flex flex-col divide-y-2 divide-neutral-300"><div class="grid grid-cols-1 gap-y-4 py-8 first:pt-0 last:pb-0 md:grid-cols-4"><div class="col-span-1 flex justify-center md:justify-start"><div class="relative h-max"><h2 class="text-xl font-bold uppercase text-neutral-800">Projects</h2><span class="absolute inset-x-0 border-b-2 border-orange-400"></span><div></div></div></div><div class="col-span-1 flex flex-col md:col-span-3"><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col pb-4"><h2 class="text-xl font-bold">HAKE</h2><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Human Activity Knowledge Engine</span><span>•</span><span class="flex-1 text-sm sm:flex-none">2018</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="http://hake-mvig.cn/home/">Project Page</a></div></div><p>Human Activity Knowledge Engine (<a href="/hake"><b><span style="color:red">H</span><span style="color:blue">A</span><span style="color:red">KE</span></b></a>) is a knowledge-driven system that enables intelligent agents to perceive human activities, reason human behavior logics, learn skills from human activities, and interact with objects and environments.</p></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col pb-4"><h2 class="text-xl font-bold">OCL</h2><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Object Concept Learning</span><span>•</span><span class="flex-1 text-sm sm:flex-none">2022</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="/ocl">Project Page</a></div></div><p>We propose a challenging Object Concept Learning (OCL) task to push the envelope of object understanding. It requires machines to reason out object affordances and simultaneously give the reason: what attributes make an object possess these affordances.</p></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col pb-4"><h2 class="text-xl font-bold">Pangea</h2><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Unified Action Semantic Space</span><span>•</span><span class="flex-1 text-sm sm:flex-none">2023</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="/pangea">Project Page</a></div></div><p>We design an action semantic space given a verb taxonomy hierarchy that covers a vast range of actions. Thus, we can gather multi-modal datasets into a unified database in a unified label system, i.e., bridging “isolated islands” into a “Pangea”. Therefore, we propose a bidirectional mapping model between physical and semantic spaces to utilize Pangea fully.</p></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col pb-4"><h2 class="text-xl font-bold">EgoPCA</h2><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">EgoPCA: A New Framework for EgoHOI</span><span>•</span><span class="flex-1 text-sm sm:flex-none">2023</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="/ego_pca">Project Page</a></div></div><p>We rethink and propose a new framework as an infrastructure to advance Ego-HOI recognition by Probing, Curation, and Adaptation (EgoPCA). We contribute comprehensive pre-train sets, balanced test sets, and a new baseline, which are complete with a training-finetuning strategy and several new and effective mechanisms and settings to advance further research.</p></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col pb-4"><h2 class="text-xl font-bold">Human-Robot Joint Learning</h2><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Robotic Teleoperation, Robot Manipulation, Imitation Learning</span><span>•</span><span class="flex-1 text-sm sm:flex-none">2023</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="/joint_learning">Project Page</a></div></div><p>[<a class="text-red-600">ICRA 2025 Best Paper Award on Human-Robot Interaction</a>] A human-robot joint learning teleoperation system for faster data collection, less human effort, and efficient robot manipulation skill acquisition.</p></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col pb-4"><h2 class="text-xl font-bold">Video-Distillation</h2><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Video Distillation via Static-Dynamic Disentanglement</span><span>•</span><span class="flex-1 text-sm sm:flex-none">2023</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="/video-distill">Project Page</a></div></div><p>We provide the first systematic study of video distillation and introduce a taxonomy to categorize temporal compression. This taxonomy motivates our unified framework for disentangling the dynamic and static information in videos. It first distills the videos into still images as static memory and then compensates the dynamic and motion information with a learnable dynamic memory block.</p></div></div></div></div></div></section><section class="bg-neutral-800 px-4 py-8 md:py-12 lg:px-8" id="portfolio"><div class="mx-auto max-w-screen-lg"><div class="flex flex-col gap-y-8"><h2 class="self-center text-xl font-bold text-white">Check out some of our work</h2><div class=" w-full columns-2 md:columns-3 lg:columns-4"><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:55.4140127388535%"></span><img alt="HAKE 2.0" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAQACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABwEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAALAI/8QAGhAAAgIDAAAAAAAAAAAAAAAAAQIAEQMTQf/aAAgBAQABPwAIyUNuR665sz//xAAWEQADAAAAAAAAAAAAAAAAAAAAATH/2gAIAQIBAT8AUP/EABQRAQAAAAAAAAAAAAAAAAAAAAD/2gAIAQMBAT8Af//Z&quot;)"/><noscript><img alt="HAKE 2.0" src="/_next/static/media/2022_hake2.0.38642608.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="http://hake-mvig.cn/home" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">HAKE 2.0</h2><p class="text-xs text-white opacity-100 sm:text-sm">The upgraded Human Activity Knowledge Engine</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:65.625%"></span><img alt="PartMap" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAUACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABgEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAL0R/8QAHRAAAAUFAAAAAAAAAAAAAAAAAQIDEVIABhQhMv/aAAgBAQABPwBW3xVM+UXltpjJ5V//xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAgEBPwCP/8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAwEBPwB//9k=&quot;)"/><noscript><img alt="PartMap" src="/_next/static/media/2022_ECCV_partmap.9885d210.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="https://github.com/enlighten0707/Body-Part-Map-for-Interactiveness" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">PartMap</h2><p class="text-xs text-white opacity-100 sm:text-sm">Interactiveness learning from the global, scene-level perspective</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:65.57377049180327%"></span><img alt="DLSA" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAUACAMBIgACEQEDEQH/xAAnAAEBAAAAAAAAAAAAAAAAAAAABwEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAAsIP/xAAXEAADAQAAAAAAAAAAAAAAAAAAERJB/9oACAEBAAE/AJen/8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAgEBPwB//8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAwEBPwB//9k=&quot;)"/><noscript><img alt="DLSA" src="/_next/static/media/2022_ECCV_longtail.3826d3d9.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="https://github.com/silicx/DLSA" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">DLSA</h2><p class="text-xs text-white opacity-100 sm:text-sm">Plug-and-play long-tail learning module by reorganizing label space</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:70.12987012987013%"></span><img alt="Interactiveness-Field" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAYACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABwEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAKMK/8QAHBAAAgEFAQAAAAAAAAAAAAAAAQMCAAQFERMS/9oACAEBAAE/ABh7B0oMZ3k3v72Wy0CDX//EABYRAQEBAAAAAAAAAAAAAAAAAAEAEv/aAAgBAgEBPwBDTf/EABcRAAMBAAAAAAAAAAAAAAAAAAABETH/2gAIAQMBAT8AxQ//2Q==&quot;)"/><noscript><img alt="Interactiveness-Field" src="/_next/static/media/2022_CVPR_InteractivenessField.f43a60ee.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="https://github.com/Foruck/Interactiveness-Field" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">Interactiveness-Field</h2><p class="text-xs text-white opacity-100 sm:text-sm">Model HOI with the interactiveness bimodal prior</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:63.70757180156657%"></span><img alt="DCR" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAUACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABwEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAKcI/8QAGxAAAgIDAQAAAAAAAAAAAAAAAQIDEgAEEyH/2gAIAQEAAT8Aoz7EZ6OKyMfGIz//xAAVEQEBAAAAAAAAAAAAAAAAAAAAAf/aAAgBAgEBPwCv/8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAwEBPwB//9k=&quot;)"/><noscript><img alt="DCR" src="/_next/static/media/2022_CVPR_anticipate.fc507045.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="https://github.com/AllenXuuu/DCR" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">DCR</h2><p class="text-xs text-white opacity-100 sm:text-sm">A training strategy on action predictions with a dynamic learning pipeline</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:88.9776357827476%"></span><img alt="OC-Immunity" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAcACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABwEBAQAAAAAAAAAAAAAAAAAAAwT/2gAMAwEAAhADEAAAAKKCo//EABwQAAEEAwEAAAAAAAAAAAAAAAMBAgQSABMUcv/aAAgBAQABPwAkyOQPYl2tGfX6VHVz/8QAFxEBAAMAAAAAAAAAAAAAAAAAAQASof/aAAgBAgEBPwC7gz//xAAYEQEAAwEAAAAAAAAAAAAAAAABAAIhEv/aAAgBAwEBPwAr0auKE//Z&quot;)"/><noscript><img alt="OC-Immunity" src="/_next/static/media/2022_AAAI_hoi.89abf76b.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="https://github.com/Foruck/OC-Immunity" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">OC-Immunity</h2><p class="text-xs text-white opacity-100 sm:text-sm">Advance HOI generalization by mitigating object category bias</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:72.3463687150838%"></span><img alt="TIN &amp; TIN++" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAYACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABAEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAKxX/8QAHBAAAQMFAAAAAAAAAAAAAAAAAQMREgAEBUGR/9oACAEBAAE/AE8EzgoW8tCRPTGv/8QAGREAAQUAAAAAAAAAAAAAAAAAAgABAxFx/9oACAECAQE/AJDK2xf/xAAWEQEBAQAAAAAAAAAAAAAAAAABAAL/2gAIAQMBAT8AwCX/2Q==&quot;)"/><noscript><img alt="TIN &amp; TIN++" src="/_next/static/media/2021_TPAMI_TIN.31f2fb93.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">TIN &amp; TIN++</h2><p class="text-xs text-white opacity-100 sm:text-sm">Interactiveness learning for Human-Object Interaction learning.</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:87.42138364779875%"></span><img alt="SymNet" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAcACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABwEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAALAI/8QAGRAAAQUAAAAAAAAAAAAAAAAAAQARMUJx/9oACAEBAAE/ALSSXxf/xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oACAECAQE/AH//xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oACAEDAQE/AH//2Q==&quot;)"/><noscript><img alt="SymNet" src="/_next/static/media/2021_TPAMI_SymNet.484f8bf2.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="https://github.com/DirtyHarryLYL/SymNet" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">SymNet</h2><p class="text-xs text-white opacity-100 sm:text-sm">Attribute detector based on symmetry and group.</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:95.70135746606336%"></span><img alt="HOI Analysis" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAgACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABwEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAKWK/8QAGxAAAwACAwAAAAAAAAAAAAAAAQIDESEABBP/2gAIAQEAAT8AhJZ4TysAnbooy5BYUO31z//EABYRAAMAAAAAAAAAAAAAAAAAAAARIf/aAAgBAgEBPwB0/8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAwEBPwB//9k=&quot;)"/><noscript><img alt="HOI Analysis" src="/_next/static/media/2020_NeurIPS_analysis.99cda008.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network)" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">HOI Analysis</h2><p class="text-xs text-white opacity-100 sm:text-sm">A way to decompose and integrate human-object interaction pairs.</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:69.5266272189349%"></span><img alt="PaStaNet" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAYACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABwEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAKoK/8QAGxAAAwACAwAAAAAAAAAAAAAAAQIDBBIAMXL/2gAIAQEAAT8AXErPMrUtMWaehdU7XYsqnzz/xAAVEQEBAAAAAAAAAAAAAAAAAAARAP/aAAgBAgEBPwBL/8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAwEBPwB//9k=&quot;)"/><noscript><img alt="PaStaNet" src="/_next/static/media/2020_CVPR_pastanet.f6b3b9f4.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="https://github.com/DirtyHarryLYL/HAKE-Action" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">PaStaNet</h2><p class="text-xs text-white opacity-100 sm:text-sm">A human body Part States library and learner.</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:62.824207492795395%"></span><img alt="DJ-RN" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAUACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABgEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAL8L/8QAHBAAAgICAwAAAAAAAAAAAAAAAgMBBQAEEiGR/9oACAEBAAE/AKWuCu1SSpk8JeJ+D1Gf/8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAgEBPwB//8QAFREBAQAAAAAAAAAAAAAAAAAAAAH/2gAIAQMBAT8Ar//Z&quot;)"/><noscript><img alt="DJ-RN" src="/_next/static/media/2020_CVPR_djrn.0056cd45.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="https://github.com/DirtyHarryLYL/DJ-RN" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">DJ-RN</h2><p class="text-xs text-white opacity-100 sm:text-sm">A 2D-3D joint learning framework for Human-Object Interaction detection.</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:90.61662198391421%"></span><img alt="HAKE 1.0" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAcACAMBIgACEQEDEQH/xAAnAAEBAAAAAAAAAAAAAAAAAAAABQEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAAug//xAAcEAABBAMBAAAAAAAAAAAAAAADAQIEBQAGIrL/2gAIAQEAAT8AsqXZLBkkK1sMRTBQfMxyt8Z//8QAFxEAAwEAAAAAAAAAAAAAAAAAAAEhMf/aAAgBAgEBPwB3T//EABcRAAMBAAAAAAAAAAAAAAAAAAABITH/2gAIAQMBAT8AUw//2Q==&quot;)"/><noscript><img alt="HAKE 1.0" src="/_next/static/media/2019_hake1.46ea1cb1.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="http://hake-mvig.cn/home" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">HAKE 1.0</h2><p class="text-xs text-white opacity-100 sm:text-sm">Human Activity Knowledge Engine.</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div></div></div></div></section><section class="bg-neutral-100 px-4 py-8 md:py-12 lg:px-8" id="publications"><div class="mx-auto max-w-screen-lg"><div class="flex flex-col divide-y-2 divide-neutral-300"><div class="grid grid-cols-1 gap-y-4 py-8 first:pt-0 last:pb-0 md:grid-cols-4"><div class="col-span-1 flex justify-center md:justify-start"><div class="relative h-max"><h2 class="text-xl font-bold uppercase text-neutral-800">Publications</h2><span class="absolute inset-x-0 border-b-2 border-orange-400"></span><div><br/>*=equal contribution<br/>#=corresponding author</div></div></div><div class="col-span-1 flex flex-col md:col-span-3"><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">---Robot Demos---</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Watch how robots from RHOS Lab complete real-world tasks — fully on their own.</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">All videos are played at normal speed (1x) and left unedited, allowing you to see exactly how the robots perform in real-time.</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://mvig-rhos.com/clean-demos">Demo Page</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Verb Mirage: Unveiling and Assessing Verb Concept Hallucinations in Multimodal Large Language Models</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Zehao Wang, Xinpeng Liu, Xiaoqian Wu, Yudonglin Zhang, Zhou Fang, Yifan Fang, Junfu Pu, Cewu Lu#, Yong-Lu Li#</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">AAAI 2026</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2412.04939">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2412.04939.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">RoboHiMan: A Hierarchical Evaluation Paradigm for Compositional Generalization in Long-Horizon Manipulation</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yangtao Chen, Zixuan Chen, Nga Teng Chan, Junting Chen, Junhui Yin, Jieqi Shi, Yang Gao, Yong-Lu Li#, Jing Huo#.</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">arXiv 2025</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2510.13149">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2510.13149.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://chenyt31.github.io/robo-himan.github.io/">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/chenyt31/RoboHiMan">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">GarmageNet: a Multimodal Generative Framework for Sewing Pattern Design and Generic Garment Modeling</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Siran Li, Chen Liu, Ruiyang Liu, Zhendong Wang, Gaofeng He, Yong-Lu Li, Xiaogang Jin, Huamin Wang.</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">TOG (SIGGRAPH Asia) 2025</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2504.01483">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2504.01483.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://style3d.github.io/garmagenet/">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/xx/xx">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">exUMI: Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yue Xu, Litao Wei, Pengyu An, Qingyu Zhang, Yong-Lu Li#</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CoRL 2025</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2509.14688">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2509.14688.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://silicx.github.io/exUMI/">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/silicx/exUMI">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Dense Policy: Bidirectional Autoregressive Learning of Actions</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yue Su, Xinyu Zhan, Hongjie Fang, Han Xue, Hao-Shu Fang, Yong-Lu Li, Cewu Lu, Lixin Yang</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ICCV 2025</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2503.13217">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2503.13217.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://selen-suyue.github.io/DspNet/">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/Selen-Suyue/DensePolicy">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">SIME: Enhancing Policy Self-Improvement with Modal-level Exploration</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yang Jin, Jun Lv, Wenye Yu, Hongjie Fang, Yong-Lu Li, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">IROS 2025</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2505.01396">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2505.01396.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://ericjin2002.github.io/SIME/">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/EricJin2002/SIME">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Motion Before Action: Diffusing Object Motion as Manipulation Condition</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yue Su, Xinyu Zhan, Hongjie Fang, Yong-Lu Li, Cewu Lu, Lixin Yang</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">RA-L 2025</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2411.09658">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2411.09658.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://selen-suyue.github.io/MBApage/">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/Selen-Suyue/MBA">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Reconstructing In-the-Wild Open-Vocabulary Human-Object Interactions</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Boran Wen, Dingbang Huang, Zichen Zhang, Jiahong Zhou, Jianbin Deng, Jingyu Gong, Yulong Chen#, Lizhuang Ma#, Yong-Lu Li#</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2025</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2503.15898">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2503.15898.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://wenboran2002.github.io/3dhoi/">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/wenboran2002/open-3dhoi">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Homogeneous Dynamics Space for Heterogeneous Humans</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Xinpeng Liu, Junxuan Liang, Chenshuo Zhang, Zixuan Cai, Cewu Lu#, Yong-Lu Li#</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2025</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2412.06146">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2412.06146.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://foruck.github.io/HDyS/">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="xx">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">GaPT-DAR: Category-level Garments Pose Tracking via Integrated 2D Deformation and 3D Reconstruction</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Li Zhang, Mingliang Xu, Jianan Wang, Qiaojun Yu, Lixin Yang, Yong-Lu Li, Cewu Lu, RujingWang, Liu Liu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2025</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/xx">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/xx.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://sites.google.com/view/gapt-dar">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="xx">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">M3-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object Segmentation</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Zixuan Chen*, Jiaxin Li*, Liming Tan, Yejie Guo, Junxuan Liang, Cewu Lu, Yong-Lu Li#</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2025</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2412.13803">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2412.13803.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://zixuan-chen.github.io/M-cube-VOS.github.io/">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/zixuan-chen/DeformVOS">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Design2GarmentCode: Turning Design Concepts to Tangible Garments Through Program Synthesis</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Feng Zhou, Ruiyang Liu, Chen Liu, Gaofeng He, Yong-Lu Li, Xiaogang Jin, Huamin Wang</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2025</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2412.08603">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2412.08603.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://style3d.github.io/design2garmentcode/">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/Style3D/SXDGarmentCode">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Human-Agent Joint Learning for Efficient Robot Manipulation Skill Acquisition</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Shengcheng Luo*, Quanquan Peng*, Jun Lv*, Kaiwen Hong, Katherine Rose Driggs-Campbell, Cewu Lu, Yong-Lu Li#</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ICRA 2025, Best Paper Award on Human-Robot Interaction</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://www.researchgate.net/publication/381882833_Human-Agent_Joint_Learning_for_Efficient_Robot_Manipulation_Skill_Acquisition">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://www.researchgate.net/publication/381882833_Human-Agent_Joint_Learning_for_Efficient_Robot_Manipulation_Skill_Acquisition">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://mvig-rhos.com/joint_learning">Project</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">ImDy: Human Inverse Dynamics from Imitated Observations</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Xinpeng Liu, Junxuan Liang, Zili Lin, Haowen Hou, Yong-Lu Li#, Cewu Lu#</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ICLR 2025</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2410.17610">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2410.17610.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://foruck.github.io/ImDy/">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/Foruck/ImDy">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">The Labyrinth of Links: Navigating the Associative Maze of Multi-modal LLMs</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Hong Li, Nanxi Li, Yuanjie Chen, Jianbin Zhu, Qinlu Guo, Cewu Lu, Yong-Lu Li#</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ICLR 2025</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2410.01417">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2410.01417.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://mvig-rhos.com/llm_inception">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/lihong2303/LLM_Inception">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Interacted Object Grounding in Spatio-Temporal Human-Object Interactions</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Xiaoyang Liu*, Boran Wen*, Xinpeng Liu*, Zizheng Zhou, Hongwei Fan, Cewu Lu, Lizhuang Ma, Yulong Chen#, Yong-Lu Li#</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">AAAI 2025</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2412.19542">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2412.19542.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/HAKE-AVA/tree/DIO">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">General Articulated Objects Manipulation in Real Images via Part-Aware Diffusion Process</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Zhou Fang, Yong-Lu Li#, Lixin Yang, Cewu Lu#</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">NeurIPS 2024</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/xx">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://openreview.net/pdf?id=WRd9LCbvxN">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://mvig-rhos.com/pa_diffusion">Project</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">HumanVLA: Towards Vision-Language Directed Object Rearrangement by Physical Humanoid</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Xinyu Xu, Yizheng Zhang, Yong-Lu Li, Lei Han, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">NeurIPS 2024</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2406.19972">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2406.19972.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/AllenXuuu/HumanVLA">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Take A Step Back: Rethinking the Two Stages in Visual Reasoning</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Mingyu Zhang*, Jiting Cai*, Mingyu Liu, Yue Xu, Cewu Lu, Yong-Lu Li#</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ECCV 2024</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2407.19666">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2407.19666.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://mybearyzhang.github.io/projects/TwoStageReason/">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/mybearyZhang/TwoStageReason">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Revisit Human-Scene Interaction via Space Occupancy</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Xinpeng Liu*, Haowen Hou*, Yanchao Yang, Yong-Lu Li#, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ECCV 2024</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2312.02700">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2312.02700.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://foruck.github.io/occu-page/">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/HaowenHou/Motion-Occupancy-Base">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Bridging the Gap between Human Motion and Action Semantics via Kinematic Phrases</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Xinpeng Liu, Yong-Lu Li#, Ailing Zeng, Zizheng Zhou, Yang You, Cewu Lu#</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ECCV 2024</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2310.04189">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2310.04189.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://foruck.github.io/KP/">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="xx">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yue Xu, Yong-Lu Li#, Kaitong Cui, Ziyu Wang, Cewu Lu, Yu-Wing Tai, Chi-Keung Tang</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ECCV 2024</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2305.18381">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2305.18381.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/silicx/GoldFromOres">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">DISCO: Embodied Navigation and Interaction via Differentiable Scene Semantics and Dual-level Control</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Xinyu Xu, Shengcheng Luo, Yanchao Yang, Yong-Lu Li#, Cewu Lu#</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ECCV 2024</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2407.14758">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2407.14758.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/AllenXuuu/DISCO">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Low-Rank Similarity Mining for Multimodal Dataset Distillation</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yue Xu, Zhilin Lin, Yusong Qiu, Cewu Lu, Yong-Lu Li#</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ICML 2024</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2406.03793">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2406.03793.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://mvig-rhos.com/xx">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/silicx/LoRS_Distill">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Dancing with Still Images: Video Distillation via Static-Dynamic Disentanglement</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Ziyu Wang*, Yue Xu*, Cewu Lu, Yong-Lu Li#</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2024</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2312.00362">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2312.00362.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://mvig-rhos.com/video-distill">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/yuz1wan/video_distillation">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li*, Xiaoqian Wu*, Xinpeng Liu, Yiming Dou, Yikun Ji, Junyi Zhang, Yixing Li, Xudong Lu, Jingru Tan, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2024, Highlight</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2304.00553">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2304.00553.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://mvig-rhos.com/pangea">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/Sandwich">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Primitive-based 3D Human-Object Interaction Modelling and Programming</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Siqi Liu, Yong-Lu Li#, Zhou Fang, Xinpeng Liu, Yang You, Cewu Lu#</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">AAAI 2024</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2312.10714">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2312.10714.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://mvig-rhos.com/p3haoi">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/MayuOshima/P3HAOI">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human Activity Reasoning</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Xiaoqian Wu, Yong-Lu Li#, Jianhua Sun, Cewu Lu#</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">NeurIPS 2023</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2311.17365">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2311.17365.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://mvig-rhos.com/symbol_llm">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/enlighten0707/Symbol-LLM">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Beyond Object Recognition: A New Benchmark towards Object Concept Learning</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li, Yue Xu, Xinyu Xu, Xiaohan Mao, Yuan Yao, Siqi Liu, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ICCV 2023</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2212.02710">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2212.02710.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://mvig-rhos.com/ocl">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/silicx/ObjectConceptLearning">Code &amp; Data</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yue Xu, Yong-Lu Li#, Zhemin Huang, Michael Xu LIU, Cewu Lu, Yu-Wing Tai, Chi Keung Tang.</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ICCV 2023</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2309.02423">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2309.02423.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://mvig-rhos.com/ego_pca">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/silicx/EgoPCA">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Dynamic Context Removal: A General Training Strategy for Robust Models on Video Action Predictive Tasks</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Xinyu Xu, Yong-Lu Li#, Cewu Lu#.</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">IJCV 2023</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/AllenXuuu/DCR">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Discovering A Variety of Objects in Spatio-Temporal Human-Object Interactions</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li*, Hongwei Fan*, Zuoyu Qiu, Yiming Dou, Liang Xu, Hao-Shu Fang, Peiyang Guo, Haisheng Su, Dongliang Wang, Wei Wu, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">Tech Report</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2211.07501">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2211.07501.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/HAKE-AVA">Code &amp; Data</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/HAKE-AVA">Star</a></span></div></div>A part of the HAKE Project</div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">HAKE: Human Activity Knowledge Engine</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu, Mingyang Chen, Ze Ma, Shiyi Wang, Hao-Shu Fang, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">Tech Report</span><span>•</span><span class="flex-1 text-sm sm:flex-none"><span><a href="/hake"><b><span style="color:red">H</span><span style="color:blue">A</span><span style="color:red">KE</span></b></a>1.0</span></span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/1904.06539">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/1904.06539.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="http://hake-mvig.cn">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/HAKE">Code</a></div></div><table><tbody><tr><td>Main Repo: </td><td>HAKE <span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/HAKE">Star</a></span></td><td></td></tr><tr><td>Sub-repos: </td><td>Torch <span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/HAKE-Action-Torch">Star</a></span></td><td>TF <span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/HAKE-Action">Star</a></span></td><td>HAKE-AVA <span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/HAKE-AVA">Star</a></span></td></tr><tr><td></td><td>Halpe <span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/Fang-Haoshu/Halpe-FullBody">Star</a></span></td><td>HOI List <span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/HOI-Learning-List">Star</a></span></td></tr></tbody></table></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">HAKE: A Knowledge Engine Foundation for Human Activity Understanding</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Yizhuo Li, Zuoyu Qiu, Liang Xu, Yue Xu, Hao-Shu Fang, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">TPAMI 2023</span><span>•</span><span class="flex-1 text-sm sm:flex-none"><span><a href="/hake"><b><span style="color:red">H</span><span style="color:blue">A</span><span style="color:red">KE</span></b></a>2.0</span></span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2202.06851">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2202.06851.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="http://hake-mvig.cn">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/HAKE">Code</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://mp.weixin.qq.com/s/0KoPD7SAaaFKycmTUDBPOg">Press</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">AlphaPose: Whole-Body Regional Multi-Person Pose Estimation and Tracking in Real-Time</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Hao-Shu Fang*, Jiefeng Li*, Hongyang Tang, Chao Xu, Haoyi Zhu, Yuliang Xiu, Yong-Lu Li, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">TPAMI 2022</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2211.03375">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2211.03375.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/MVIG-SJTU/AlphaPose">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/MVIG-SJTU/AlphaPose">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Constructing Balance from Imbalance for Long-tailed Image Recognition</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yue Xu*, Yong-Lu Li*, Jiefeng Li, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ECCV 2022</span><span>•</span><span class="flex-1 text-sm sm:flex-none">DLSA</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2208.02567">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2208.02567.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/silicx/DLSA">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/silicx/DLSA">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Mining Cross-Person Cues for Body-Part Interactiveness Learning in HOI Detection</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Xiaoqian Wu*, Yong-Lu Li*, Xinpeng Liu, Junyi Zhang, Yuzhe Wu, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ECCV 2022</span><span>•</span><span class="flex-1 text-sm sm:flex-none">PartMap</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2207.14192">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2207.14192v1.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/enlighten0707/Body-Part-Map-for-Interactiveness">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/enlighten0707/Body-Part-Map-for-Interactiveness">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Interactiveness Field of Human-Object Interactions</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Xinpeng Liu*, Yong-Lu Li*, Xiaoqian Wu, Yu-Wing Tai, Cewu Lu, Chi Keung Tang</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2022</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2204.07718">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2204.07718.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/Foruck/Interactiveness-Field">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/Foruck/Interactiveness-Field">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Human Trajectory Prediction with Momentary Observation</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Jianhua Sun, Yuxuan Li, Liang Chai, Hao-Shu Fang, Yong-Lu Li, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2022</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_Human_Trajectory_Prediction_With_Momentary_Observation_CVPR_2022_paper.pdf">PDF</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Learn to Anticipate Future with Dynamic Context Removal</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Xinyu Xu, Yong-Lu Li, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2022</span><span>•</span><span class="flex-1 text-sm sm:flex-none">DCR</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2204.02587">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2204.02587.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/AllenXuuu/DCR">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/AllenXuuu/DCR">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Canonical Voting: Towards Robust Oriented Bounding Box Detection in 3D Scenes</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yang You, Zelin Ye, Yujing Lou, Chengkun Li, Yong-Lu Li, Lizhuang Ma, Weiming Wang, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2022</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2011.12001">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2011.12001.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/qq456cvb/CanonicalVoting">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/qq456cvb/CanonicalVoting">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">UKPGAN: Unsupervised KeyPoint GANeration</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yang You, Wenhai Liu, Yong-Lu Li, Weiming Wang, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2022</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2011.11974">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2011.11974.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/qq456cvb/UKPGAN">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/qq456cvb/UKPGAN">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Highlighting Object Category Immunity for the Generalization of Human-Object Interaction Detection</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Xinpeng Liu*, Yong-Lu Li*, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">AAAI 2022</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2202.09492">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2202.09492.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/Foruck/OC-Immunity">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/Foruck/OC-Immunity">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Learning Single/Multi-Attribute of Object with Symmetry and Group</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li, Yue Xu, Xinyu Xu, Xiaohan Mao, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">TPAMI 2021</span><span>•</span><span class="flex-1 text-sm sm:flex-none">SymNet</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2110.04603">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2110.04603.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/SymNet">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/SymNet">Star</a></span></div></div>An extension of our CVPR 2020 work (Symmetry and Group in Attribute-Object Compositions, SymNet).</div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Localization with Sampling-Argmax</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Jiefeng Li, Tong Chen, Ruiqi Shi, Yujing Lou, Yong-Lu Li, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">NeurIPS 2021</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2110.08825">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2110.08825.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/Jeff-sjtu/sampling-argmax">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/Jeff-sjtu/sampling-argmax">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Transferable Interactiveness Knowledge for Human-Object Interaction Detection</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Xijie Huang, Liang Xu, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">TPAMI 2021</span><span>•</span><span class="flex-1 text-sm sm:flex-none">TIN++</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2101.10292">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2101.10292.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network">Star</a></span></div></div>An extension of our CVPR 2019 work (Transferable Interactiveness Network, TIN).</div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">DecAug: Augmenting HOI Detection via Decomposition</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yichen Xie, Hao-Shu Fang, Dian Shao, Yong-Lu Li, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">AAAI 2021</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2010.01007">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2010.01007.pdf">PDF</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">HOI Analysis: Integrating and Decomposing Human-Object Interaction</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li*, Xinpeng Liu*, Xiaoqian Wu, Yizhuo Li, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">NeurIPS 2020</span><span>•</span><span class="flex-1 text-sm sm:flex-none">IDN</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2010.16219">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2010.16219.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network)">Code</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/master">Project: HAKE-Action-Torch</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network)">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">PaStaNet: Toward Human Activity Knowledge Engine</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu, Shiyi Wang, Hao-Shu Fang, Ze Ma, Mingyang Chen, Cewu Lu.</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2020</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2004.00945">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2004.00945.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://drive.google.com/file/d/16PCCK_flK2qW4QJVWoYXQwG3wgXd6yvT/view?usp=sharing">Video</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://drive.google.com/file/d/19J9uz3epBo3o9CIU85mzgLblRVNawl87/view?usp=sharing">Slides</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/HAKE">Data</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/HAKE-Action">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/HAKE-Action">Star</a></span></div></div><p><strong class="text-red-600">Oral Talk:</strong> <a href="http://ai.stanford.edu/~jingweij/cicv/#schedule">Compositionality in Computer Vision</a> in CVPR 2020</p></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Detailed 2D-3D Joint Representation for Human-Object Interaction</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li, Xinpeng Liu, Han Lu, Shiyi Wang, Junqi Liu, Jiefeng Li, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2020</span><span>•</span><span class="flex-1 text-sm sm:flex-none">DJ-RN</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2004.08154">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2004.08154.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://drive.google.com/file/d/14dK1tBLe3xHXyO_5WsRO2JcjJ95meaDB/view?usp=sharing">Video</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://drive.google.com/file/d/1A5bQZFsBOahj7dJgSnWR7jdyvMG58NkT/view?usp=sharing">Slides</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/DJ-RN#ambiguous-hoi">Benchmark: Ambiguous-HOI</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/DJ-RN">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/DJ-RN">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Symmetry and Group in Attribute-Object Compositions</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li, Yue Xu, Xiaohan Mao, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2020</span><span>•</span><span class="flex-1 text-sm sm:flex-none">SymNet</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2004.00587">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2004.00587.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://drive.google.com/file/d/1ZTSB2lJbDTH7D-7GdEJQGmszvEc2Vuwd/view?usp=sharing">Video</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://drive.google.com/file/d/1aqYeSIQkoTp1hYOJokDgoucdZcufN2iG/view?usp=sharing">Slides</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/SymNet">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/SymNet">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">InstaBoost: Boosting Instance Segmentation Via Probability Map Guided Copy-Pasting</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Hao-Shu Fang*, Jianhua Sun*, Runzhong Wang*, Minghao Gou, Yong-Lu Li, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ICCV 2019</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/1908.07801">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/1908.07801.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/GothicAi/Instaboost">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/GothicAi/Instaboost">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Transferable Interactiveness Knowledge for Human-Object Interaction Detection</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li, Siyuan Zhou, Xijie Huang, Liang Xu, Ze Ma, Hao-Shu Fang, Yan-Feng Wang, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2019</span><span>•</span><span class="flex-1 text-sm sm:flex-none">TIN</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/1811.08264">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/1811.08264.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">SRDA: Generating Instance Segmentation Annotation via Scanning, Reasoning and Domain Adaptation</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Wenqiang Xu*, Yong-Lu Li*, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ECCV 2018</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/1801.08839">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/1801.08839.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://drive.google.com/drive/folders/1t941oiLk40XQX2Q9a2HPmiDRUpiwazJO?usp=sharing">Dataset (Instance-60k &amp; 3D Object Models)</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/SRDA-ECCV2018">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/SRDA-ECCV2018">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Beyond Holistic Object Recognition: Enriching Image Understanding with Part States</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Cewu Lu, Hao Su, Yong-Lu Li, Yongyi Lu, Li Yi, Chi-Keung Tang, Leonidas J. Guibas</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2018</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="http://ai.ucsd.edu/~haosu/papers/cvpr18_partstate.pdf">PDF</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Optimization of Radial Distortion Self-Calibration for Structure from Motion from Uncalibrated UAV Images</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li, Yinghao Cai, Dayong Wen, Yiping Yang</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ICPR 2016</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://projet.liris.cnrs.fr/imagine/pub/proceedings/ICPR-2016/media/files/1010.pdf">PDF</a></div></div></div></div></div></div></div></section><section class="bg-neutral-100 px-4 py-8 md:py-12 lg:px-8" id="people"><div class="mx-auto max-w-screen-lg"><div class="flex flex-col"><div class="grid grid-cols-1 gap-y-4 py-8 first:pt-0 last:pb-0 md:grid-cols-4"><div class="col-span-1 flex justify-center md:justify-start"><div class="relative h-max"><h2 class="text-xl font-bold uppercase text-neutral-800">People</h2><span class="absolute inset-x-0 border-b-2 border-orange-400"></span><div></div></div></div><div class="col-span-1 flex flex-col md:col-span-3"><div class="grid grid-cols-4 gap-4 pb-4"><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href="https://dirtyharrylyl.github.io/"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27586%27%20height=%27820%27/%3e"/></span><img alt="Yong-Lu Li" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Yong-Lu Li" src="/_next/static/media/Yong-Lu_Li.9b3f9166.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="https://dirtyharrylyl.github.io/">Yong-Lu Li</a></div><div class="text-sm font-medium">PI, Associate Professor</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href="https://foruck.github.io/"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27500%27%20height=%27500%27/%3e"/></span><img alt="Xinpeng Liu" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Xinpeng Liu" src="/_next/static/media/Xinpeng_Liu.48d03ebb.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="https://foruck.github.io/">Xinpeng Liu</a></div><div class="text-sm font-medium">PhD. Student</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href="https://silicx.github.io/"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27800%27%20height=%271029%27/%3e"/></span><img alt="Yue Xu" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Yue Xu" src="/_next/static/media/Yue_Xu.4a0113e2.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="https://silicx.github.io/">Yue Xu</a></div><div class="text-sm font-medium">PhD. Student</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href="https://scholar.google.com/citations?user=-PHR96oAAAAJ"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27433%27%20height=%27632%27/%3e"/></span><img alt="Xiaoqian Wu" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Xiaoqian Wu" src="/_next/static/media/Xiaoqian_Wu.394c81fd.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="https://scholar.google.com/citations?user=-PHR96oAAAAJ">Xiaoqian Wu</a></div><div class="text-sm font-medium">PhD. Student</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href="https://github.com/MayuOshima/"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27512%27%20height=%27512%27/%3e"/></span><img alt="Siqi Liu" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Siqi Liu" src="/_next/static/media/Siqi_Liu.0472512f.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="https://github.com/MayuOshima/">Siqi Liu</a></div><div class="text-sm font-medium">PhD. Student</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href="https://scholar.google.com/citations?user=fhDk2-wAAAAJ&amp;hl=zh-CN"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27827%27%20height=%271063%27/%3e"/></span><img alt="Hong Li" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Hong Li" src="/_next/static/media/Hong_Li.ef37f080.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="https://scholar.google.com/citations?user=fhDk2-wAAAAJ&amp;hl=zh-CN">Hong Li</a></div><div class="text-sm font-medium">PhD. Student</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href=""><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27167%27%20height=%27167%27/%3e"/></span><img alt="Zehao Wang" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Zehao Wang" src="/_next/static/media/placeholder.5f16935b.png" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="">Zehao Wang</a></div><div class="text-sm font-medium">Ph.D. Student</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href=""><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27386%27%20height=%27540%27/%3e"/></span><img alt="Zixuan Chen" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Zixuan Chen" src="/_next/static/media/Zixuan_Chen.4fa92817.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="">Zixuan Chen</a></div><div class="text-sm font-medium">Ph.D. Student</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href="https://scholar.google.com/citations?user=8KnzXWUAAAAJ&amp;hl=zh-CN"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%271530%27%20height=%271530%27/%3e"/></span><img alt="Boran Wen" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Boran Wen" src="/_next/static/media/Boran_Wen.6676549b.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="https://scholar.google.com/citations?user=8KnzXWUAAAAJ&amp;hl=zh-CN">Boran Wen</a></div><div class="text-sm font-medium">Ph.D. Student</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href="https://scholar.google.com/citations?user=YyWddPsAAAAJ&amp;hl=en"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27554%27%20height=%27760%27/%3e"/></span><img alt="Mingyu Zhang" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Mingyu Zhang" src="/_next/static/media/Mingyu_Zhang.98cdec63.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="https://scholar.google.com/citations?user=YyWddPsAAAAJ&amp;hl=en">Mingyu Zhang</a></div><div class="text-sm font-medium">Ph.D. Student</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href=""><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27301%27%20height=%27394%27/%3e"/></span><img alt="Jiaxin Li" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Jiaxin Li" src="/_next/static/media/Jiaxin_Li.6d8f3908.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="">Jiaxin Li</a></div><div class="text-sm font-medium">Ph.D. Student</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href=""><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272654%27%20height=%273715%27/%3e"/></span><img alt="Xianchao Zeng" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Xianchao Zeng" src="/_next/static/media/Xianchao_Zeng.d1f23174.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="">Xianchao Zeng</a></div><div class="text-sm font-medium">Ph.D. Student (SII)</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href="https://scholar.google.com.hk/citations?user=fvP8SGcAAAAJ&amp;hl=zh-CN"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27984%27%20height=%271378%27/%3e"/></span><img alt="Weixi Song" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Weixi Song" src="/_next/static/media/Weixi_Song.e33363a6.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="https://scholar.google.com.hk/citations?user=fvP8SGcAAAAJ&amp;hl=zh-CN">Weixi Song</a></div><div class="text-sm font-medium">Ph.D. Student (SII)</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href=""><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27715%27%20height=%27953%27/%3e"/></span><img alt="Xinyu Zhou" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Xinyu Zhou" src="/_next/static/media/Xinyu_Zhou.0043deb9.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="">Xinyu Zhou</a></div><div class="text-sm font-medium">Ph.D. Student (SII)</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href=""><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27167%27%20height=%27167%27/%3e"/></span><img alt="Yusong Qiu" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Yusong Qiu" src="/_next/static/media/placeholder.5f16935b.png" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="">Yusong Qiu</a></div><div class="text-sm font-medium">Master Student</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href="https://yushunxiang.github.io"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%271026%27%20height=%271450%27/%3e"/></span><img alt="Yushun Xiang" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Yushun Xiang" src="/_next/static/media/Yushun_Xiang.c2c1bbdb.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="https://yushunxiang.github.io">Yushun Xiang</a></div><div class="text-sm font-medium">Master Student</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27167%27%20height=%27167%27/%3e"/></span><img alt="Chenyang Yu" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Chenyang Yu" src="/_next/static/media/placeholder.5f16935b.png" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a>Chenyang Yu</a></div><div class="text-sm font-medium">Master Student</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href="https://scholar.google.com/citations?user=BsM3ggwAAAAJ&amp;hl=en"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%271280%27%20height=%271489%27/%3e"/></span><img alt="Ziyu Wang" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Ziyu Wang" src="/_next/static/media/Ziyu_Wang.6cd2fffc.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="https://scholar.google.com/citations?user=BsM3ggwAAAAJ&amp;hl=en">Ziyu Wang</a></div><div class="text-sm font-medium">Master Student</div></div></div></div></div><div class="py-4 gap-y-2"><h3 class="text-xl font-bold">Alumni:</h3><div><a class="font-bold underline" href="">Junxuan Liang</a>: <!-- -->CMU, Intern</div><div><a class="font-bold underline" href="">Dingbang Huang</a>: <!-- -->UT Austin, Intern</div><div><a class="font-bold underline" href="">Yixing Chen</a>: <!-- -->UIUC, Intern</div><div><a class="font-bold underline" href="">Xin Li</a>: <!-- -->Shanghai AI Lab, Researcher</div><div><a class="font-bold underline" href="https://scholar.google.com/citations?user=l18d7kcAAAAJ&amp;hl=en">Jingru Tan</a>: <!-- -->Central South University, Associate Professor</div><div><a class="font-bold underline" href="">Yude Zou</a>: <!-- -->Westlake University, Ph.D.</div><div><a class="font-bold underline" href="">Yejie Guo</a>: <!-- -->UCMerced, Intern</div><div><a class="font-bold underline" href="https://bariona.github.io/">Quanquan Peng</a>: <!-- -->UW, Intern -&gt; USCD, Ph.D.</div><div><a class="font-bold underline" href="https://scholar.google.com/citations?user=h_UN0qUAAAAJ&amp;hl=en">Zizheng Zhou</a>: <!-- -->UC Merced, Intern -&gt; CMU, MS</div><div><a class="font-bold underline" href="https://scholar.google.com/citations?user=vhagIp4AAAAJ&amp;hl=zh-CN">Jiting Cai</a>: <!-- -->UMass, Intern -&gt; CMU, MS</div><div><a class="font-bold underline" href="">Yuyang Zhang</a>: <!-- -->EIAS &amp; SJTU, Ph.D.</div><div><a class="font-bold underline" href="">Zhilin Lin</a>: <!-- -->SJTU, Ph.D.</div><div><a class="font-bold underline" href="">Zili Lin</a>: <!-- -->EIAS &amp; SJTU, Ph.D.</div><div><a class="font-bold underline" href="">Yifan Shi</a>: <!-- -->EIAS &amp; SJTU, Ph.D.</div><div><a class="font-bold underline" href="https://haowenhou.github.io/">Haowen Hou</a>: <!-- -->UCSD, Intern -&gt; SJTU, Ph.D.</div><div><a class="font-bold underline" href="https://scholar.google.com/citations?user=V8FcspEAAAAJ&amp;hl=en">Yixing Li</a>: <!-- -->CUHK, Ph.D.</div><div><a class="font-bold underline" href="https://mingyulau.github.io/">Mingyu Liu</a>: <!-- -->ZJU, Ph.D.</div><div><a class="font-bold underline" href="">Kaitong Cui</a>: <!-- -->HKU, Intern</div><div><a class="font-bold underline" href="https://www.junyi42.com/">Junyi Zhang</a>: <!-- -->UC Merced+Google Intern -&gt; UCB, Ph.D.</div><div><a class="font-bold underline" href="https://dou-yiming.github.io/">Yiming Dou</a>: <!-- -->Stanford, Intern -&gt; UMich, Ph.D. -&gt; Cornell, Ph.D.</div><div><a class="font-bold underline" href="https://scholar.google.com/citations?user=-zT1NKwAAAAJ">Xiaohan Mao</a>: <!-- -->Shanghai AI Lab &amp; SJTU, Ph.D.</div><div><a class="font-bold">Zhemin Huang</a>: <!-- -->Stanford, MS</div><div><a class="font-bold">Yuzhe Wu</a>: </div><div><a class="font-bold underline" href="https://coeusguo.github.io/">Shaopeng Guo</a>: <!-- -->HKUST, Intern -&gt; UCSD, Ph.D.</div><div><a class="font-bold underline" href="https://lucky-lance.github.io">Xudong Lu</a>: <!-- -->CUHK, Ph.D.</div><div><a class="font-bold underline" href="https://hwfan.io/about-me/">Hongwei Fan</a>: <!-- -->SenseTime, Research Engineer -&gt; Peking University, Ph.D.</div><div><a class="font-bold underline" href="https://www.cs.rochester.edu/u/yyao39/#research">Yuan Yao</a>: <!-- -->Johns Hopkins, Intern -&gt; U of Rochester, Ph.D.</div><div><a class="font-bold">Zuoyu Qiu</a>: <!-- -->SJTU, MS</div><div><a class="font-bold">Shiyi Wang</a>: <!-- -->Flexiv</div><div><a class="font-bold">Junqi Liu</a>: </div><div><a class="font-bold">Han Lu</a>: <!-- -->SJTU, Ph.D.</div><div><a class="font-bold">Zhanke Zhou</a>: <!-- -->HKBU, Ph.D.</div><div><a class="font-bold">Mingyang Chen</a>: <!-- -->UCSD, MS</div><div><a class="font-bold underline" href="https://liangxuy.github.io/">Liang Xu</a>: <!-- -->EIAS &amp; SJTU, Ph.D.</div><div><a class="font-bold underline" href="https://maqingyang.github.io/">Ze Ma</a>: <!-- -->UCB, Intern -&gt; Columbia University, MS</div><div><a class="font-bold underline" href="https://huangowen.github.io/">Xijie Huang</a>: <!-- -->HKUST, Ph.D. -&gt; Meta Reality Lab</div></div></div></div></div></div></section><div class="relative bg-neutral-900 px-4 pb-6 pt-12 sm:px-8 sm:pt-14 sm:pb-8"><div class="absolute inset-x-0 -top-4 flex justify-center sm:-top-6"><a class="rounded-full bg-neutral-100 p-1 ring-white ring-offset-2 ring-offset-gray-700/80 focus:outline-none focus:ring-2 sm:p-2" href="/#hero"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="h-6 w-6 bg-transparent sm:h-8 sm:w-8"><path stroke-linecap="round" stroke-linejoin="round" d="M5 15l7-7 7 7"></path></svg></a></div><div class="flex flex-col items-center gap-y-6"><div id="pageview-script" class="text-sm text-neutral-700"><a href="https://www.revolvermaps.com/livestats/5r1om30zfoi/"><img src="//rf.revolvermaps.com/h/m/a/0/ff0000/128/0/5r1om30zfoi.png" width="256" height="128" alt="Map" style="border:0"/></a><script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=ko7teOw_sX7QKyWbHLxkMdyOA6BYkSEu0Fo1wnSs9QE"></script></div><span class="text-sm text-neutral-700">© Copyright 2022 MVIG-RHOS • Based on<!-- --> <a href="https://github.com/tbakerx/react-resume-template">tbakerx</a></span></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{}},"page":"/","query":{},"buildId":"aCD78StrAO3V-az-E8ppv","nextExport":true,"autoExport":true,"isFallback":false,"scriptLoader":[]}</script></body></html>