<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>RHOS</title><meta content="Homepage of RHOS" name="description"/><link href="https://http://mvig-rhos.com/" rel="canonical"/><link href="/favicon.ico" rel="icon" sizes="any"/><link href="/icon.svg" rel="icon" type="image/svg+xml"/><link href="/apple-touch-icon.png" rel="apple-touch-icon"/><link href="/site.webmanifest" rel="manifest"/><meta content="RHOS" property="og:title"/><meta content="Homepage of RHOS" property="og:description"/><meta content="https://http://mvig-rhos.com/" property="og:url"/><meta content="RHOS" name="twitter:title"/><meta content="Homepage of RHOS" name="twitter:description"/><link rel="preload" as="image" href="/_next/static/media/Robotics_Cyberpunk_2077_bg.f50d6e01.png"/><meta name="next-head-count" content="15"/><meta charSet="utf-8"/><meta content="notranslate" name="google"/><link rel="preload" href="/_next/static/css/cd001c8d894663d3.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cd001c8d894663d3.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-dcc8c4e5e5864e6f.js" defer=""></script><script src="/_next/static/chunks/framework-7751730b10fa0f74.js" defer=""></script><script src="/_next/static/chunks/main-a9b50f256c2cfb57.js" defer=""></script><script src="/_next/static/chunks/pages/_app-18e1e0f85fa3b58e.js" defer=""></script><script src="/_next/static/chunks/505-cae962cf7f0df22a.js" defer=""></script><script src="/_next/static/chunks/345-974c0170f2cf59d0.js" defer=""></script><script src="/_next/static/chunks/pages/index-76f17fa72d23fb5f.js" defer=""></script><script src="/_next/static/tQG3OeW69wWFNfhoMTK4O/_buildManifest.js" defer=""></script><script src="/_next/static/tQG3OeW69wWFNfhoMTK4O/_ssgManifest.js" defer=""></script></head><body class="bg-neutral-900"><div id="__next"><section class="" id="hero"><div class=""><div class="relative flex h-screen w-screen items-center justify-center"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:absolute;top:0;left:0;bottom:0;right:0"><img alt="RHOS-image" src="/_next/static/media/Robotics_Cyberpunk_2077_bg.f50d6e01.png" decoding="async" data-nimg="fill" class="absolute z-0" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAIAAAD38zoCAAAAhUlEQVR42mPw8DGPDLMz0ZMzs7OMTwpVNTAI8LayN1dikDXTUbUx9nDQD/K01jHSUzOyNrd1NjbUZVDQkbGw1zO1NXJ1srS2MjKysLK0tPEOiWCwMZVJ8FUvTbToKLSfUe9eHKXhbCrEIynL4GuvVJLiUp/jVZVqmxWqGuggpaciLCDACQABSR1UPIEuQgAAAABJRU5ErkJggg==&quot;)"/><noscript><img alt="RHOS-image" src="/_next/static/media/Robotics_Cyberpunk_2077_bg.f50d6e01.png" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="absolute z-0"/></noscript></span><div class="z-10 w-full max-w-screen-lg px-4 lg:px-0"><div class="flex flex-col items-center gap-y-6 rounded-xl bg-gray-800/40 p-6 text-center shadow-lg backdrop-blur-sm"><h1 class="text-4xl font-bold text-white sm:text-5xl lg:text-7xl">RHOS</h1><p class="prose-sm text-stone-200 sm:prose-base lg:prose-lg">Robot • Human • Object • Scene</p><div class="flex w-5/6 justify-center gap-x-4"><a class="flex gap-x-2 rounded-full border-2 bg-none py-2 px-4 text-sm font-medium text-white ring-offset-gray-700/80 hover:bg-gray-700/80 focus:outline-none focus:ring-2 focus:ring-offset-2 sm:text-base border-orange-500 ring-orange-500" href="#recruit">Recruit</a><a class="flex gap-x-2 rounded-full border-2 bg-none py-2 px-4 text-sm font-medium text-white ring-offset-gray-700/80 hover:bg-gray-700/80 focus:outline-none focus:ring-2 focus:ring-offset-2 sm:text-base border-white ring-white" href="https://github.com/mvig-rhos"><svg class="h-5 w-5 text-white sm:h-6 sm:w-6" fill="currentColor" viewBox="0 0 128 128" width="128" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M64 5.103c-33.347 0-60.388 27.035-60.388 60.388 0 26.682 17.303 49.317 41.297 57.303 3.017.56 4.125-1.31 4.125-2.905 0-1.44-.056-6.197-.082-11.243-16.8 3.653-20.345-7.125-20.345-7.125-2.747-6.98-6.705-8.836-6.705-8.836-5.48-3.748.413-3.67.413-3.67 6.063.425 9.257 6.223 9.257 6.223 5.386 9.23 14.127 6.562 17.573 5.02.542-3.903 2.107-6.568 3.834-8.076-13.413-1.525-27.514-6.704-27.514-29.843 0-6.593 2.36-11.98 6.223-16.21-.628-1.52-2.695-7.662.584-15.98 0 0 5.07-1.623 16.61 6.19C53.7 35 58.867 34.327 64 34.304c5.13.023 10.3.694 15.127 2.033 11.526-7.813 16.59-6.19 16.59-6.19 3.287 8.317 1.22 14.46.593 15.98 3.872 4.23 6.215 9.617 6.215 16.21 0 23.194-14.127 28.3-27.574 29.796 2.167 1.874 4.097 5.55 4.097 11.183 0 8.08-.07 14.583-.07 16.572 0 1.607 1.088 3.49 4.148 2.897 23.98-7.994 41.263-30.622 41.263-57.294C124.388 32.14 97.35 5.104 64 5.104z" fill-rule="evenodd"></path><path d="M26.484 91.806c-.133.3-.605.39-1.035.185-.44-.196-.685-.605-.543-.906.13-.31.603-.395 1.04-.188.44.197.69.61.537.91zm2.446 2.729c-.287.267-.85.143-1.232-.28-.396-.42-.47-.983-.177-1.254.298-.266.844-.14 1.24.28.394.426.472.984.17 1.255zM31.312 98.012c-.37.258-.976.017-1.35-.52-.37-.538-.37-1.183.01-1.44.373-.258.97-.025 1.35.507.368.545.368 1.19-.01 1.452zm3.261 3.361c-.33.365-1.036.267-1.552-.23-.527-.487-.674-1.18-.343-1.544.336-.366 1.045-.264 1.564.23.527.486.686 1.18.333 1.543zm4.5 1.951c-.147.473-.825.688-1.51.486-.683-.207-1.13-.76-.99-1.238.14-.477.823-.7 1.512-.485.683.206 1.13.756.988 1.237zm4.943.361c.017.498-.563.91-1.28.92-.723.017-1.308-.387-1.315-.877 0-.503.568-.91 1.29-.924.717-.013 1.306.387 1.306.88zm4.598-.782c.086.485-.413.984-1.126 1.117-.7.13-1.35-.172-1.44-.653-.086-.498.422-.997 1.122-1.126.714-.123 1.354.17 1.444.663zm0 0"></path></svg>Github</a></div></div></div><div class="absolute inset-x-0 bottom-6 flex justify-center"><a class="rounded-full bg-white p-1 ring-white ring-offset-2 ring-offset-gray-700/80 focus:outline-none focus:ring-2 sm:p-2" href="/#about"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="h-5 w-5 bg-transparent sm:h-6 sm:w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M19 9l-7 7-7-7"></path></svg></a></div></div></div></section><section class="bg-neutral-800 px-4 py-8 md:py-12 lg:px-8" id="about"><div class="mx-auto max-w-screen-lg"><div class="grid grid-cols-1 gap-y-4 md:grid-cols-4"><div class="col-span-1 flex justify-center md:justify-start"><h2 class="text-2xl font-bold text-white">RHOS</h2></div><div class="col-span-1 flex flex-col gap-y-6 md:col-span-3"><div class="flex flex-col gap-y-2"><h2 class="text-2xl font-bold text-white">About</h2><div class="prose prose-sm text-gray-300 sm:prose-base"><div><p>Hi, this is the website of RHOS team at<!-- --> <a class="font-bold text-white" href="https://www.mvig.org/">MVIG</a>. We study<!-- --> <i><b>Human Activity Understanding</b></i>,<!-- --> <i><b>Visual Reasoning</b></i>, and<!-- --> <i><b>Embodied AI</b></i>. We are building a knowledge-driven system that enables intelligent agents to perceive human activities, reason human behavior logics, learn skills from human activities, and interact with environment.</p><p><b>Research Interests: </b></p><p>(S) <b>Embodied AI</b>: how to make agents learn skills from humans and interact with human &amp; scene &amp; object.<br/>(S-1) <b>Human Activity Understanding</b>: how to learn and ground complex/ambiguous human activity concepts (body motion, human-object/human/scene interaction) and object concepts from multi-modal information (2D-3D-4D).<br/>(S-2) <b>Visual Reasoning</b>: how to mine, capture, and embed the logics and causal relations from human activities.<br/>(S-3) <b>General Multi-Modal Foundation Models</b>: especially for human-centric perception tasks.<br/>(S-4) <b>Activity Understanding from A Cognitive Perspective</b>: work with multidisciplinary researchers to study how the brain perceives activities.<br/>(E) <b>Human-Robot Interaction (e.g. for Smart Hospital)</b>: work with the healthcare team (doctors and engineers) in SJTU to develop intelligent robots to help people.</p></div></div><h2 class="text-2xl font-bold text-white">Contact</h2><div class="prose prose-sm text-gray-300 sm:prose-base"><div><b>Yong-Lu Li</b><br/>Email: yonglu_li[at]sjtu[dot]edu[dot]cn<br/>Office: SEIEE-3-301<br/>Shanghai Jiao Tong University</div></div><div><a class="mr-2 font-bold text-sky-200 underline block" href="https://dirtyharrylyl.github.io/"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="inline h-5 w-5 text-white"><path stroke-linecap="round" stroke-linejoin="round" d="M5 3v4M3 5h4M6 17v4m-2-2h4m5-16l2.286 6.857L21 12l-5.714 2.143L13 21l-2.286-6.857L5 12l5.714-2.143L13 3z"></path></svg>Personal Website</a><a class="mr-2 font-bold text-sky-200 underline" href="https://scholar.google.com.hk/citations?user=UExAaVgAAAAJ"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="inline h-5 w-5 text-white"><path d="M12 14l9-5-9-5-9 5 9 5z"></path><path d="M12 14l6.16-3.422a12.083 12.083 0 01.665 6.479A11.952 11.952 0 0012 20.055a11.952 11.952 0 00-6.824-2.998 12.078 12.078 0 01.665-6.479L12 14z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M12 14l9-5-9-5-9 5 9 5zm0 0l6.16-3.422a12.083 12.083 0 01.665 6.479A11.952 11.952 0 0012 20.055a11.952 11.952 0 00-6.824-2.998 12.078 12.078 0 01.665-6.479L12 14zm-4 6v-7.5l4-2.222"></path></svg>Google Scholar </a><a class="mr-2 font-bold text-sky-200 underline" href="https://github.com/DirtyHarryLYL"><svg class="inline h-5 w-5 text-white" fill="currentColor" viewBox="0 0 128 128" width="128" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M64 5.103c-33.347 0-60.388 27.035-60.388 60.388 0 26.682 17.303 49.317 41.297 57.303 3.017.56 4.125-1.31 4.125-2.905 0-1.44-.056-6.197-.082-11.243-16.8 3.653-20.345-7.125-20.345-7.125-2.747-6.98-6.705-8.836-6.705-8.836-5.48-3.748.413-3.67.413-3.67 6.063.425 9.257 6.223 9.257 6.223 5.386 9.23 14.127 6.562 17.573 5.02.542-3.903 2.107-6.568 3.834-8.076-13.413-1.525-27.514-6.704-27.514-29.843 0-6.593 2.36-11.98 6.223-16.21-.628-1.52-2.695-7.662.584-15.98 0 0 5.07-1.623 16.61 6.19C53.7 35 58.867 34.327 64 34.304c5.13.023 10.3.694 15.127 2.033 11.526-7.813 16.59-6.19 16.59-6.19 3.287 8.317 1.22 14.46.593 15.98 3.872 4.23 6.215 9.617 6.215 16.21 0 23.194-14.127 28.3-27.574 29.796 2.167 1.874 4.097 5.55 4.097 11.183 0 8.08-.07 14.583-.07 16.572 0 1.607 1.088 3.49 4.148 2.897 23.98-7.994 41.263-30.622 41.263-57.294C124.388 32.14 97.35 5.104 64 5.104z" fill-rule="evenodd"></path><path d="M26.484 91.806c-.133.3-.605.39-1.035.185-.44-.196-.685-.605-.543-.906.13-.31.603-.395 1.04-.188.44.197.69.61.537.91zm2.446 2.729c-.287.267-.85.143-1.232-.28-.396-.42-.47-.983-.177-1.254.298-.266.844-.14 1.24.28.394.426.472.984.17 1.255zM31.312 98.012c-.37.258-.976.017-1.35-.52-.37-.538-.37-1.183.01-1.44.373-.258.97-.025 1.35.507.368.545.368 1.19-.01 1.452zm3.261 3.361c-.33.365-1.036.267-1.552-.23-.527-.487-.674-1.18-.343-1.544.336-.366 1.045-.264 1.564.23.527.486.686 1.18.333 1.543zm4.5 1.951c-.147.473-.825.688-1.51.486-.683-.207-1.13-.76-.99-1.238.14-.477.823-.7 1.512-.485.683.206 1.13.756.988 1.237zm4.943.361c.017.498-.563.91-1.28.92-.723.017-1.308-.387-1.315-.877 0-.503.568-.91 1.29-.924.717-.013 1.306.387 1.306.88zm4.598-.782c.086.485-.413.984-1.126 1.117-.7.13-1.35-.172-1.44-.653-.086-.498.422-.997 1.122-1.126.714-.123 1.354.17 1.444.663zm0 0"></path></svg>Github </a><a class="mr-2 font-bold text-sky-200 underline" href="https://www.linkedin.com/in/%E6%B0%B8%E9%9C%B2-%E6%9D%8E-991b99139/"><svg class="inline h-5 w-5 text-white" fill="currentColor" viewBox="0 0 128 128" width="128" xmlns="http://www.w3.org/2000/svg"><path d="M116 3H12a8.91 8.91 0 00-9 8.8v104.42a8.91 8.91 0 009 8.78h104a8.93 8.93 0 009-8.81V11.77A8.93 8.93 0 00116 3zM39.17 107H21.06V48.73h18.11zm-9-66.21a10.5 10.5 0 1110.49-10.5 10.5 10.5 0 01-10.54 10.48zM107 107H88.89V78.65c0-6.75-.12-15.44-9.41-15.44s-10.87 7.36-10.87 15V107H50.53V48.73h17.36v8h.24c2.42-4.58 8.32-9.41 17.13-9.41C103.6 47.28 107 59.35 107 75z" fill="currentColor"></path></svg>LinkedIn</a><a class="mr-2 font-bold text-sky-200 underline" href="https://dblp.org/pid/198/9345.html"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="inline h-5 w-5 text-white"><path stroke-linecap="round" stroke-linejoin="round" d="M9 20l-5.447-2.724A1 1 0 013 16.382V5.618a1 1 0 011.447-.894L9 7m0 13l6-3m-6 3V7m6 10l4.553 2.276A1 1 0 0021 18.382V7.618a1 1 0 00-.553-.894L15 4m0 13V4m0 0L9 7"></path></svg>dblp</a><a class="mr-2 font-bold text-sky-200 underline" href="https://www.semanticscholar.org/author/Yong-Lu-Li/10384643"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="inline h-5 w-5 text-white"><path stroke-linecap="round" stroke-linejoin="round" d="M9 20l-5.447-2.724A1 1 0 013 16.382V5.618a1 1 0 011.447-.894L9 7m0 13l6-3m-6 3V7m6 10l4.553 2.276A1 1 0 0021 18.382V7.618a1 1 0 00-.553-.894L15 4m0 13V4m0 0L9 7"></path></svg>Semantic Scholar</a></div></div></div></div></div></section><section class="bg-neutral-100 px-4 py-8 md:py-12 lg:px-8" id="recruit"><div class="mx-auto max-w-screen-lg"><div class="flex flex-col divide-y-2 divide-neutral-300"><div class="grid grid-cols-1 gap-y-4 py-8 first:pt-0 last:pb-0 md:grid-cols-4"><div class="col-span-1 flex justify-center md:justify-start"><div class="relative h-max"><h2 class="text-xl font-bold uppercase text-neutral-800">Recruitment</h2><span class="absolute inset-x-0 border-b-2 border-orange-400"></span><div></div></div></div><div class="col-span-1 flex flex-col md:col-span-3"><span><p>We are actively looking for self-motivated<!-- --> <strong>students (Master/PhD, 2024 spring &amp; fall), interns / engineers / visitors</strong> <!-- -->(CV/ML/ROB/NLP/Math/Phys background, always welcome) to join us in<!-- --> <a class="text-red600" href="https://www.mvig.org/">Machine Vision and Intelligence Group (MVIG)</a>. If you share same/similar interests, feel free to drop me an email with your resume.</p><p>Click<!-- --> <a class="text-red-600" href="https://dirtyharrylyl.github.io/recruit.html"><b>here</b></a> <!-- -->for more details.</p></span></div></div></div></div></section><section class="bg-neutral-100 px-4 py-8 md:py-12 lg:px-8" id="news"><div class="mx-auto max-w-screen-lg"><div class="flex flex-col divide-y-2 divide-neutral-300"><div class="grid grid-cols-1 gap-y-4 py-8 first:pt-0 last:pb-0 md:grid-cols-4"><div class="col-span-1 flex justify-center md:justify-start"><div class="relative h-max"><h2 class="text-xl font-bold uppercase text-neutral-800">News and Olds</h2><span class="absolute inset-x-0 border-b-2 border-orange-400"></span><div></div></div></div><div class="col-span-1 flex flex-col md:col-span-3"><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2023.7<!-- -->] </span><span class="flex-1 sm:flex-none">Our works on ego-centric video understanding and <a class="underline text-sky-600" href="https://mvig-rhos.com/ocl">object concept learning</a> will appear at ICCV 2023!</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2023.7<!-- -->] </span><span class="flex-1 sm:flex-none">The upgrade version of <a class="underline text-sky-600" href="https://github.com/AllenXuuu/DCR">DCR</a> will appear at IJCV!</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2022.12<!-- -->] </span><span class="flex-1 sm:flex-none"><a class="font-bold" href="https://arxiv.org/abs/2202.06851">HAKE 2.0</a> will appear at TPAMI!</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2022.12<!-- -->] </span><span class="flex-1 sm:flex-none">OCL (Object Concept Leanring) is released on <a class="underline text-sky-600" href="https://arxiv.org/abs/2212.02710">arXiv</a>. Please visit the <a class="underline text-sky-600" href="/ocl">project page</a> for details.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2022.11<!-- -->] </span><span class="flex-1 sm:flex-none">We release the human body part states and interactive object bounding box annotations upon AVA (2.1 &amp; 2.2): <a class="font-bold" href="https://github.com/DirtyHarryLYL/HAKE-AVA">[HAKE-AVA]</a>, and a CLIP-based human part state &amp; verb recognizer: <a class="font-bold" href="https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/CLIP-Activity2Vec">[CLIP-Activity2Vec]</a>.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2022.11<!-- -->] </span><span class="flex-1 sm:flex-none"><a class="font-bold" href="https://github.com/MVIG-SJTU/AlphaPose">AlphaPose</a> will appear at TPAMI!</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2022.07<!-- -->] </span><span class="flex-1 sm:flex-none">Two papers on <b>longtailed learning, HOI detection</b> are accepted by ECCV&#x27;22, arXivs and code are coming soon</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2022.03<!-- -->] </span><span class="flex-1 sm:flex-none">Five papers on <b>HOI detection/prediction, trajection prediction, 3D detection/keypoints</b> are accepted by CVPR&#x27;22, papers and code are coming soon.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2022.02<!-- -->] </span><span class="flex-1 sm:flex-none">We release the human body part state labels based on AVA:<!-- --> <a class="font-bold" href="https://github.com/DirtyHarryLYL/HAKE-AVA">HAKE-AVA</a> and <a class="font-bold" href="https://arxiv.org/abs/2202.06851">HAKE 2.0</a>.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2021.12<!-- -->] </span><span class="flex-1 sm:flex-none">Our work on <b>HOI generalization</b> will appear at AAAI&#x27;22.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2021.10<!-- -->] </span><span class="flex-1 sm:flex-none"><b>Learning Single/Multi-Attribute of Object with Symmetry and Group</b> is accepted by TPAMI.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2021.09<!-- -->] </span><span class="flex-1 sm:flex-none">Our work <b>Localization with Sampling-Argmax</b> will appear at NeurIPS&#x27;21.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2021.02<!-- -->] </span><span class="flex-1 sm:flex-none">Upgraded<!-- --> <a class="font-bold" href="https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/Activity2Vec">HAKE-Activity2Vec</a> <!-- -->is released! Images/Videos --&gt; human box + ID + skeleton + part states + action + representation.<!-- --> <a class="underline" href="https://youtu.be/ty-bXDInLMQ">[Demo]</a> <a class="underline" href="https://drive.google.com/file/d/1iZ57hKjus2lKbv1MAB-TLFrChSoWGD5e/view?usp=sharing">[Description]</a></span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2021.01<!-- -->] </span><span class="flex-1 sm:flex-none"><b><a href="https://arxiv.org/abs/2101.10292">TIN</a> (Transferable Interactiveness Network)</b> <!-- -->is accepted by TPAMI.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2020.12<!-- -->] </span><span class="flex-1 sm:flex-none"><a class="font-bold underline" href="https://arxiv.org/abs/2010.01007">DecAug</a> <!-- -->is accepted by AAAI&#x27;21.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2020.09<!-- -->] </span><span class="flex-1 sm:flex-none">Our work <b>HOI Analysis</b> will appear at NeurIPS 2020.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2020.06<!-- -->] </span><span class="flex-1 sm:flex-none">The larger<!-- --> <a class="font-bold" href="https://github.com/DirtyHarryLYL/HAKE#hake-large-for-instance-level-hoi-detection">HAKE-Large</a> <!-- -->(&gt;120K images with activity and part state labels) is released.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2020.02<!-- -->] </span><span class="flex-1 sm:flex-none">Three papers <b>Image-based HAKE: PaSta-Net</b>, <b>2D-3D Joint HOI Learning</b>,<!-- --> <b>Symmetry-based Attribute-Object Learning</b> are accepted in <a href="http://cvpr2020.thecvf.com/">CVPR&#x27;20</a>! Papers and corresponding resources (code, data) will be released soon.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2019.07<!-- -->] </span><span class="flex-1 sm:flex-none">Our paper <b>InstaBoost</b> is accepted in <a href="http://iccv2019.thecvf.com/">ICCV&#x27;19</a>.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2019.06<!-- -->] </span><span class="flex-1 sm:flex-none">The Part I of our <a href="/hake"><b><span style="color:red">H</span><span style="color:blue">A</span><span style="color:red">KE</span></b></a>:<!-- --> <b><a href="http://hake-mvig.cn/download/">HAKE-HICO</a></b> <!-- -->which contains the image-level part-state annotations is released.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2019.04<!-- -->] </span><span class="flex-1 sm:flex-none">Our project <a href="/hake"><b><span style="color:red">H</span><span style="color:blue">A</span><span style="color:red">KE</span></b></a> (Human Activity Knowledge Engine) begins trial operation.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2019.02<!-- -->] </span><span class="flex-1 sm:flex-none">Our paper on<!-- --> <b><a href="https://arxiv.org/abs/1811.08264">Interactiveness</a></b> <!-- -->is accepted in <a href="http://cvpr2019.thecvf.com/">CVPR&#x27;19</a>.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2018.07<!-- -->] </span><span class="flex-1 sm:flex-none">Our paper on<!-- --> <b><a href="https://arxiv.org/abs/1801.08839">GAN &amp; Annotation Generation</a></b> <!-- -->is accepted in <a href="https://eccv2018.org/">ECCV&#x27;18</a>.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2018.05<!-- -->] </span><span class="flex-1 sm:flex-none">Presentation (Kaibot Team) in<!-- --> <a href="https://icra2018.org/tidy-up-my-room-challenge/">TIDY UP MY ROOM CHALLENGE | ICRA&#x27;18</a>.</span></div><div class="pb-2"><span class="flex-1 font-bold sm:flex-none">[<!-- -->2018.02<!-- -->] </span><span class="flex-1 sm:flex-none">Our paper on<!-- --> <b><a href="http://ai.ucsd.edu/~haosu/papers/cvpr18_partstate.pdf">Object Part States</a></b> <!-- -->is accepted in <a href="http://cvpr2018.thecvf.com/program/main_conference">CVPR&#x27;18</a>.</span></div></div></div></div></div></section><section class="bg-neutral-100 px-4 py-8 md:py-12 lg:px-8" id="projects"><div class="mx-auto max-w-screen-lg"><div class="flex flex-col divide-y-2 divide-neutral-300"><div class="grid grid-cols-1 gap-y-4 py-8 first:pt-0 last:pb-0 md:grid-cols-4"><div class="col-span-1 flex justify-center md:justify-start"><div class="relative h-max"><h2 class="text-xl font-bold uppercase text-neutral-800">Projects</h2><span class="absolute inset-x-0 border-b-2 border-orange-400"></span><div></div></div></div><div class="col-span-1 flex flex-col md:col-span-3"><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col pb-4"><h2 class="text-xl font-bold">HAKE</h2><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Human Activity Knowledge Engine</span><span>•</span><span class="flex-1 text-sm sm:flex-none">2018</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="http://hake-mvig.cn/home/">Homepage</a></div></div><p>Human Activity Knowledge Engine (<a href="/hake"><b><span style="color:red">H</span><span style="color:blue">A</span><span style="color:red">KE</span></b></a>) is a knowledge-driven system that aims at enabling intelligent agents to perceive human activities, reason human behavior logics, learn skills from human activities, and interact with objects and environments.</p></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col pb-4"><h2 class="text-xl font-bold">OCL</h2><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Object Concept Learning</span><span>•</span><span class="flex-1 text-sm sm:flex-none">2022</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="/ocl">Homepage</a></div></div><p>We propose a challenging Object Concept Learning (OCL) task to push the envelope of object understanding. It requires machines to reason out object affordances and simultaneously give the reason: what attributes make an object possesses these affordances.</p></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col pb-4"><h2 class="text-xl font-bold">Pangea</h2><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Unified Action Semantic Space</span><span>•</span><span class="flex-1 text-sm sm:flex-none">2023</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="/pangea">Homepage</a></div></div><p>We design an action semantic space in view of verb taxonomy hierarchy and covering massive actions. Thus, we can gather multi-modal datasets into a unified database in a unified label system, i.e., bridging “isolated islands” into a “Pangea”. Accordingly, we propose a bidirectional mapping model between physical and semantic space to fully use Pangea.</p></div></div></div></div></div></section><section class="bg-neutral-800 px-4 py-8 md:py-12 lg:px-8" id="portfolio"><div class="mx-auto max-w-screen-lg"><div class="flex flex-col gap-y-8"><h2 class="self-center text-xl font-bold text-white">Check out some of our work</h2><div class=" w-full columns-2 md:columns-3 lg:columns-4"><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:55.4140127388535%"></span><img alt="HAKE 2.0" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAQACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABwEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAALAI/8QAGhAAAgIDAAAAAAAAAAAAAAAAAQIAEQMTQf/aAAgBAQABPwAIyUNuR665sz//xAAWEQADAAAAAAAAAAAAAAAAAAAAATH/2gAIAQIBAT8AUP/EABQRAQAAAAAAAAAAAAAAAAAAAAD/2gAIAQMBAT8Af//Z&quot;)"/><noscript><img alt="HAKE 2.0" src="/_next/static/media/2022_hake2.0.38642608.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="http://hake-mvig.cn/home" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">HAKE 2.0</h2><p class="text-xs text-white opacity-100 sm:text-sm">The upgraded Human Activity Knowledge Engine</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:65.625%"></span><img alt="PartMap" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAUACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABgEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAL0R/8QAHRAAAAUFAAAAAAAAAAAAAAAAAQIDEVIABhQhMv/aAAgBAQABPwBW3xVM+UXltpjJ5V//xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAgEBPwCP/8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAwEBPwB//9k=&quot;)"/><noscript><img alt="PartMap" src="/_next/static/media/2022_ECCV_partmap.9885d210.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="https://github.com/enlighten0707/Body-Part-Map-for-Interactiveness" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">PartMap</h2><p class="text-xs text-white opacity-100 sm:text-sm">Interactiveness learning from the global, scene-level perspective</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:65.57377049180327%"></span><img alt="DLSA" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAUACAMBIgACEQEDEQH/xAAnAAEBAAAAAAAAAAAAAAAAAAAABwEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAAsIP/xAAXEAADAQAAAAAAAAAAAAAAAAAAERJB/9oACAEBAAE/AJen/8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAgEBPwB//8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAwEBPwB//9k=&quot;)"/><noscript><img alt="DLSA" src="/_next/static/media/2022_ECCV_longtail.3826d3d9.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="https://github.com/silicx/DLSA" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">DLSA</h2><p class="text-xs text-white opacity-100 sm:text-sm">Plug-and-play long-tail learning module by reorganizing label space</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:70.12987012987013%"></span><img alt="Interactiveness-Field" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAYACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABwEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAKMK/8QAHBAAAgEFAQAAAAAAAAAAAAAAAQMCAAQFERMS/9oACAEBAAE/ABh7B0oMZ3k3v72Wy0CDX//EABYRAQEBAAAAAAAAAAAAAAAAAAEAEv/aAAgBAgEBPwBDTf/EABcRAAMBAAAAAAAAAAAAAAAAAAABETH/2gAIAQMBAT8AxQ//2Q==&quot;)"/><noscript><img alt="Interactiveness-Field" src="/_next/static/media/2022_CVPR_InteractivenessField.f43a60ee.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="https://github.com/Foruck/Interactiveness-Field" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">Interactiveness-Field</h2><p class="text-xs text-white opacity-100 sm:text-sm">Model HOI with the interactiveness bimodal prior</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:63.70757180156657%"></span><img alt="DCR" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAUACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABwEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAKcI/8QAGxAAAgIDAQAAAAAAAAAAAAAAAQIDEgAEEyH/2gAIAQEAAT8Aoz7EZ6OKyMfGIz//xAAVEQEBAAAAAAAAAAAAAAAAAAAAAf/aAAgBAgEBPwCv/8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAwEBPwB//9k=&quot;)"/><noscript><img alt="DCR" src="/_next/static/media/2022_CVPR_anticipate.fc507045.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="https://github.com/AllenXuuu/DCR" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">DCR</h2><p class="text-xs text-white opacity-100 sm:text-sm">A training strategy on action predictions with a dynamic learning pipeline</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:88.9776357827476%"></span><img alt="OC-Immunity" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAcACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABwEBAQAAAAAAAAAAAAAAAAAAAwT/2gAMAwEAAhADEAAAAKKCo//EABwQAAEEAwEAAAAAAAAAAAAAAAMBAgQSABMUcv/aAAgBAQABPwAkyOQPYl2tGfX6VHVz/8QAFxEBAAMAAAAAAAAAAAAAAAAAAQASof/aAAgBAgEBPwC7gz//xAAYEQEAAwEAAAAAAAAAAAAAAAABAAIhEv/aAAgBAwEBPwAr0auKE//Z&quot;)"/><noscript><img alt="OC-Immunity" src="/_next/static/media/2022_AAAI_hoi.89abf76b.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="https://github.com/Foruck/OC-Immunity" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">OC-Immunity</h2><p class="text-xs text-white opacity-100 sm:text-sm">Advance HOI generalization by mitigating object category bias</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:72.3463687150838%"></span><img alt="TIN &amp; TIN++" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAYACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABAEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAKxX/8QAHBAAAQMFAAAAAAAAAAAAAAAAAQMREgAEBUGR/9oACAEBAAE/AE8EzgoW8tCRPTGv/8QAGREAAQUAAAAAAAAAAAAAAAAAAgABAxFx/9oACAECAQE/AJDK2xf/xAAWEQEBAQAAAAAAAAAAAAAAAAABAAL/2gAIAQMBAT8AwCX/2Q==&quot;)"/><noscript><img alt="TIN &amp; TIN++" src="/_next/static/media/2021_TPAMI_TIN.31f2fb93.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">TIN &amp; TIN++</h2><p class="text-xs text-white opacity-100 sm:text-sm">Interactiveness learning for Human-Object Interaction learning.</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:87.42138364779875%"></span><img alt="SymNet" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAcACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABwEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAALAI/8QAGRAAAQUAAAAAAAAAAAAAAAAAAQARMUJx/9oACAEBAAE/ALSSXxf/xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oACAECAQE/AH//xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oACAEDAQE/AH//2Q==&quot;)"/><noscript><img alt="SymNet" src="/_next/static/media/2021_TPAMI_SymNet.484f8bf2.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="https://github.com/DirtyHarryLYL/SymNet" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">SymNet</h2><p class="text-xs text-white opacity-100 sm:text-sm">Attribute detector based on symmetry and group.</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:95.70135746606336%"></span><img alt="HOI Analysis" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAgACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABwEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAKWK/8QAGxAAAwACAwAAAAAAAAAAAAAAAQIDESEABBP/2gAIAQEAAT8AhJZ4TysAnbooy5BYUO31z//EABYRAAMAAAAAAAAAAAAAAAAAAAARIf/aAAgBAgEBPwB0/8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAwEBPwB//9k=&quot;)"/><noscript><img alt="HOI Analysis" src="/_next/static/media/2020_NeurIPS_analysis.99cda008.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network)" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">HOI Analysis</h2><p class="text-xs text-white opacity-100 sm:text-sm">A way to decompse and integrate human-object interaction pairs.</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:69.5266272189349%"></span><img alt="PaStaNet" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAYACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABwEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAKoK/8QAGxAAAwACAwAAAAAAAAAAAAAAAQIDBBIAMXL/2gAIAQEAAT8AXErPMrUtMWaehdU7XYsqnzz/xAAVEQEBAAAAAAAAAAAAAAAAAAARAP/aAAgBAgEBPwBL/8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAwEBPwB//9k=&quot;)"/><noscript><img alt="PaStaNet" src="/_next/static/media/2020_CVPR_pastanet.f6b3b9f4.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="https://github.com/DirtyHarryLYL/HAKE-Action" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">PaStaNet</h2><p class="text-xs text-white opacity-100 sm:text-sm">A human body Part States library and learner.</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:62.824207492795395%"></span><img alt="DJ-RN" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAUACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABgEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAL8L/8QAHBAAAgICAwAAAAAAAAAAAAAAAgMBBQAEEiGR/9oACAEBAAE/AKWuCu1SSpk8JeJ+D1Gf/8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAgEBPwB//8QAFREBAQAAAAAAAAAAAAAAAAAAAAH/2gAIAQMBAT8Ar//Z&quot;)"/><noscript><img alt="DJ-RN" src="/_next/static/media/2020_CVPR_djrn.0056cd45.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="https://github.com/DirtyHarryLYL/DJ-RN" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">DJ-RN</h2><p class="text-xs text-white opacity-100 sm:text-sm">A 2D-3D jointlt learning framework for Human-Object Interaction detection.</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div><div class="pb-6"><div class="relative h-max w-full overflow-hidden rounded-lg shadow-lg shadow-black/30 lg:shadow-xl"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:90.61662198391421%"></span><img alt="HAKE 1.0" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAcACAMBIgACEQEDEQH/xAAnAAEBAAAAAAAAAAAAAAAAAAAABQEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAAug//xAAcEAABBAMBAAAAAAAAAAAAAAADAQIEBQAGIrL/2gAIAQEAAT8AsqXZLBkkK1sMRTBQfMxyt8Z//8QAFxEAAwEAAAAAAAAAAAAAAAAAAAEhMf/aAAgBAgEBPwB3T//EABcRAAMBAAAAAAAAAAAAAAAAAAABITH/2gAIAQMBAT8AUw//2Q==&quot;)"/><noscript><img alt="HAKE 1.0" src="/_next/static/media/2019_hake1.46ea1cb1.jpg" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><a class="absolute inset-0 h-full w-full  bg-gray-900 transition-all duration-300 opacity-0 hover:opacity-80 opacity-0" href="http://hake-mvig.cn/home" target="_blank"><div class="relative h-full w-full p-4"><div class="flex h-full w-full flex-col gap-y-2 overflow-y-auto"><h2 class="text-center font-bold text-white opacity-100">HAKE 1.0</h2><p class="text-xs text-white opacity-100 sm:text-sm">Human Activity Knowledge Engine.</p></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="absolute bottom-1 right-1 h-4 w-4 shrink-0 text-white sm:bottom-2 sm:right-2"><path stroke-linecap="round" stroke-linejoin="round" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg></div></a></div></div></div></div></div></section><section class="bg-neutral-100 px-4 py-8 md:py-12 lg:px-8" id="publications"><div class="mx-auto max-w-screen-lg"><div class="flex flex-col divide-y-2 divide-neutral-300"><div class="grid grid-cols-1 gap-y-4 py-8 first:pt-0 last:pb-0 md:grid-cols-4"><div class="col-span-1 flex justify-center md:justify-start"><div class="relative h-max"><h2 class="text-xl font-bold uppercase text-neutral-800">Publications</h2><span class="absolute inset-x-0 border-b-2 border-orange-400"></span><div><br/>*=equal contribution<br/>#=corresponding author</div></div></div><div class="col-span-1 flex flex-col md:col-span-3"><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Beyond Object Recognition: A New Benchmark towards Object Concept Learning</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li, Yue Xu, Xinyu Xu, Xiaohan Mao, Yuan Yao, Siqi Liu, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ICCV 2023</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2212.02710">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2212.02710.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://mvig-rhos.com/ocl">Project</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yue Xu, Yong-Lu Li#, Zhemin Huang, Michael Xu LIU, Cewu Lu, Yu-Wing Tai, Chi Keung Tang.</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ICCV 2023</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://mvig-rhos.com/">Project</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Dynamic Context Removal: A General Training Strategy for Robust Models on Video Action Predictive Tasks</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Xinyu Xu, Yong-Lu Li#, Cewu Lu#.</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">IJCV 2023</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/AllenXuuu/DCR">Code</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yue Xu, Yong-Lu Li#, Kaitong Cui, Ziyu Wang, Cewu Lu, Yu-Wing Tai, Chi-Keung Tang</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">Preprint</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2305.18381">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2305.18381.pdf">PDF</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li*, Xiaoqian Wu*, Xinpeng Liu, Yiming Dou, Yikun Ji, Junyi Zhang, Yixing Li, Jingru Tan, Xudong Lu, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">Preprint</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2304.00553">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2304.00553.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://mvig-rhos.com/pangea">Project</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Discovering A Variety of Objects in Spatio-Temporal Human-Object Interactions</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li*, Hongwei Fan*, Zuoyu Qiu, Yiming Dou, Liang Xu, Hao-Shu Fang, Peiyang Guo, Haisheng Su, Dongliang Wang, Wei Wu, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">Tech Report</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2211.07501">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2211.07501.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/HAKE-AVA">Code &amp; Data</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/HAKE-AVA">Star</a></span></div></div>A part of the HAKE Project</div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">HAKE: Human Activity Knowledge Engine</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu, Mingyang Chen, Ze Ma, Shiyi Wang, Hao-Shu Fang, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">Tech Report</span><span>•</span><span class="flex-1 text-sm sm:flex-none"><span><a href="/hake"><b><span style="color:red">H</span><span style="color:blue">A</span><span style="color:red">KE</span></b></a>1.0</span></span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/1904.06539">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/1904.06539.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="http://hake-mvig.cn">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/HAKE">Code</a></div></div><table><tbody><tr><td>Main Repo: </td><td>HAKE <span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/HAKE">Star</a></span></td><td></td></tr><tr><td>Sub-repos: </td><td>Torch <span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/HAKE-Action-Torch">Star</a></span></td><td>TF <span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/HAKE-Action">Star</a></span></td><td>HAKE-AVA <span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/HAKE-AVA">Star</a></span></td></tr><tr><td></td><td>Halpe <span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/Fang-Haoshu/Halpe-FullBody">Star</a></span></td><td>HOI List <span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/HOI-Learning-List">Star</a></span></td></tr></tbody></table></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">HAKE: A Knowledge Engine Foundation for Human Activity Understanding</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Yizhuo Li, Zuoyu Qiu, Liang Xu, Yue Xu, Hao-Shu Fang, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">TPAMI 2023</span><span>•</span><span class="flex-1 text-sm sm:flex-none"><span><a href="/hake"><b><span style="color:red">H</span><span style="color:blue">A</span><span style="color:red">KE</span></b></a>2.0</span></span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2202.06851">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2202.06851.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="http://hake-mvig.cn">Project</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://mp.weixin.qq.com/s/0KoPD7SAaaFKycmTUDBPOg">Press</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">AlphaPose: Whole-Body Regional Multi-Person Pose Estimation and Tracking in Real-Time</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Hao-Shu Fang*, Jiefeng Li*, Hongyang Tang, Chao Xu, Haoyi Zhu, Yuliang Xiu, Yong-Lu Li, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">TPAMI 2022</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2211.03375">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2211.03375.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/MVIG-SJTU/AlphaPose">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/MVIG-SJTU/AlphaPose">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Constructing Balance from Imbalance for Long-tailed Image Recognition</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yue Xu*, Yong-Lu Li*, Jiefeng Li, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ECCV 2022</span><span>•</span><span class="flex-1 text-sm sm:flex-none">DLSA</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2208.02567">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2208.02567.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/silicx/DLSA">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/silicx/DLSA">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Mining Cross-Person Cues for Body-Part Interactiveness Learning in HOI Detection</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Xiaoqian Wu*, Yong-Lu Li*, Xinpeng Liu, Junyi Zhang, Yuzhe Wu, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ECCV 2022</span><span>•</span><span class="flex-1 text-sm sm:flex-none">PartMap</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2207.14192">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2207.14192v1.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/enlighten0707/Body-Part-Map-for-Interactiveness">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/enlighten0707/Body-Part-Map-for-Interactiveness">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Interactiveness Field of Human-Object Interactions</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Xinpeng Liu*, Yong-Lu Li*, Xiaoqian Wu, Yu-Wing Tai, Cewu Lu, Chi Keung Tang</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2022</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2204.07718">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2204.07718.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/Foruck/Interactiveness-Field">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/Foruck/Interactiveness-Field">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Human Trajectory Prediction with Momentary Observation</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Jianhua Sun, Yuxuan Li, Liang Chai, Hao-Shu Fang, Yong-Lu Li, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2022</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_Human_Trajectory_Prediction_With_Momentary_Observation_CVPR_2022_paper.pdf">PDF</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Learn to Anticipate Future with Dynamic Context Removal</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Xinyu Xu, Yong-Lu Li, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2022</span><span>•</span><span class="flex-1 text-sm sm:flex-none">DCR</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2204.02587">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2204.02587.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/AllenXuuu/DCR">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/AllenXuuu/DCR">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Canonical Voting: Towards Robust Oriented Bounding Box Detection in 3D Scenes</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yang You, Zelin Ye, Yujing Lou, Chengkun Li, Yong-Lu Li, Lizhuang Ma, Weiming Wang, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2022</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2011.12001">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2011.12001.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/qq456cvb/CanonicalVoting">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/qq456cvb/CanonicalVoting">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">UKPGAN: Unsupervised KeyPoint GANeration</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yang You, Wenhai Liu, Yong-Lu Li, Weiming Wang, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2022</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2011.11974">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2011.11974.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/qq456cvb/UKPGAN">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/qq456cvb/UKPGAN">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Highlighting Object Category Immunity for the Generalization of Human-Object Interaction Detection</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Xinpeng Liu*, Yong-Lu Li*, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">AAAI 2022</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2202.09492">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2202.09492.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/Foruck/OC-Immunity">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/Foruck/OC-Immunity">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Learning Single/Multi-Attribute of Object with Symmetry and Group</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li, Yue Xu, Xinyu Xu, Xiaohan Mao, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">TPAMI 2021</span><span>•</span><span class="flex-1 text-sm sm:flex-none">SymNet</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2110.04603">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2110.04603.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/SymNet">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/SymNet">Star</a></span></div></div>An extension of our CVPR 2020 work (Symmetry and Group in Attribute-Object Compositions, SymNet).</div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Localization with Sampling-Argmax</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Jiefeng Li, Tong Chen, Ruiqi Shi, Yujing Lou, Yong-Lu Li, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">NeurIPS 2021</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2110.08825">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2110.08825.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/Jeff-sjtu/sampling-argmax">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/Jeff-sjtu/sampling-argmax">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Transferable Interactiveness Knowledge for Human-Object Interaction Detection</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Xijie Huang, Liang Xu, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">TPAMI 2021</span><span>•</span><span class="flex-1 text-sm sm:flex-none">TIN++</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2101.10292">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2101.10292.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network">Star</a></span></div></div>An extension of our CVPR 2019 work (Transferable Interactiveness Network, TIN).</div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">DecAug: Augmenting HOI Detection via Decomposition</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yichen Xie, Hao-Shu Fang, Dian Shao, Yong-Lu Li, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">AAAI 2021</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2010.01007">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2010.01007.pdf">PDF</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">HOI Analysis: Integrating and Decomposing Human-Object Interaction</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li*, Xinpeng Liu*, Xiaoqian Wu, Yizhuo Li, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">NeurIPS 2020</span><span>•</span><span class="flex-1 text-sm sm:flex-none">IDN</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2010.16219">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2010.16219.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network)">Code</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/master">Project: HAKE-Action-Torch</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network)">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">PaStaNet: Toward Human Activity Knowledge Engine</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu, Shiyi Wang, Hao-Shu Fang, Ze Ma, Mingyang Chen, Cewu Lu.</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2020</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2004.00945">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2004.00945.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://drive.google.com/file/d/16PCCK_flK2qW4QJVWoYXQwG3wgXd6yvT/view?usp=sharing">Video</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://drive.google.com/file/d/19J9uz3epBo3o9CIU85mzgLblRVNawl87/view?usp=sharing">Slides</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/HAKE">Data</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/HAKE-Action">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/HAKE-Action">Star</a></span></div></div><p><strong class="text-red-600">Oral Talk:</strong> <a href="http://ai.stanford.edu/~jingweij/cicv/#schedule">Compositionality in Computer Vision</a> in CVPR 2020</p></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Detailed 2D-3D Joint Representation for Human-Object Interaction</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li, Xinpeng Liu, Han Lu, Shiyi Wang, Junqi Liu, Jiefeng Li, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2020</span><span>•</span><span class="flex-1 text-sm sm:flex-none">DJ-RN</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2004.08154">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2004.08154.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://drive.google.com/file/d/14dK1tBLe3xHXyO_5WsRO2JcjJ95meaDB/view?usp=sharing">Video</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://drive.google.com/file/d/1A5bQZFsBOahj7dJgSnWR7jdyvMG58NkT/view?usp=sharing">Slides</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/DJ-RN#ambiguous-hoi">Benchmark: Ambiguous-HOI</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/DJ-RN">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/DJ-RN">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Symmetry and Group in Attribute-Object Compositions</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li, Yue Xu, Xiaohan Mao, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2020</span><span>•</span><span class="flex-1 text-sm sm:flex-none">SymNet</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/2004.00587">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/2004.00587.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://drive.google.com/file/d/1ZTSB2lJbDTH7D-7GdEJQGmszvEc2Vuwd/view?usp=sharing">Video</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://drive.google.com/file/d/1aqYeSIQkoTp1hYOJokDgoucdZcufN2iG/view?usp=sharing">Slides</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/SymNet">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/SymNet">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">InstaBoost: Boosting Instance Segmentation Via Probability Map Guided Copy-Pasting</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Hao-Shu Fang*, Jianhua Sun*, Runzhong Wang*, Minghao Gou, Yong-Lu Li, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ICCV 2019</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/1908.07801">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/1908.07801.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/GothicAi/Instaboost">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/GothicAi/Instaboost">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Transferable Interactiveness Knowledge for Human-Object Interaction Detection</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li, Siyuan Zhou, Xijie Huang, Liang Xu, Ze Ma, Hao-Shu Fang, Yan-Feng Wang, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2019</span><span>•</span><span class="flex-1 text-sm sm:flex-none">TIN</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/1811.08264">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/1811.08264.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">SRDA: Generating Instance Segmentation Annotation via Scanning, Reasoning and Domain Adaptation</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Wenqiang Xu*, Yong-Lu Li*, Cewu Lu</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ECCV 2018</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/abs/1801.08839">arXiv</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://arxiv.org/pdf/1801.08839.pdf">PDF</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://drive.google.com/drive/folders/1t941oiLk40XQX2Q9a2HPmiDRUpiwazJO?usp=sharing">Dataset (Instance-60k &amp; 3D Object Models)</a><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://github.com/DirtyHarryLYL/SRDA-ECCV2018">Code</a><span><a aria-label="Star buttons/github-buttons on GitHub" data-show-count="true" href="https://github.com/DirtyHarryLYL/SRDA-ECCV2018">Star</a></span></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Beyond Holistic Object Recognition: Enriching Image Understanding with Part States</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Cewu Lu, Hao Su, Yong-Lu Li, Yongyi Lu, Li Yi, Chi-Keung Tang, Leonidas J. Guibas</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">CVPR 2018</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="http://ai.ucsd.edu/~haosu/papers/cvpr18_partstate.pdf">PDF</a></div></div></div><div class="flex flex-col pb-8 text-center last:pb-0 md:text-left"><div class="flex flex-col"><h3 class="text-xl font-bold">Optimization of Radial Distortion Self-Calibration for Structure from Motion from Uncalibrated UAV Images</h3><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium italic sm:flex-none">Yong-Lu Li, Yinghao Cai, Dayong Wen, Yiping Yang</span></div><div class="flex items-center justify-center gap-x-2 md:justify-start"><span class="flex-1 text-sm font-medium sm:flex-none">ICPR 2016</span><span>•</span><a class="flex-1 text-sm font-bold text-sky-600 underline sm:flex-none" href="https://projet.liris.cnrs.fr/imagine/pub/proceedings/ICPR-2016/media/files/1010.pdf">PDF</a></div></div></div></div></div></div></div></section><section class="bg-neutral-100 px-4 py-8 md:py-12 lg:px-8" id="people"><div class="mx-auto max-w-screen-lg"><div class="flex flex-col"><div class="grid grid-cols-1 gap-y-4 py-8 first:pt-0 last:pb-0 md:grid-cols-4"><div class="col-span-1 flex justify-center md:justify-start"><div class="relative h-max"><h2 class="text-xl font-bold uppercase text-neutral-800">People</h2><span class="absolute inset-x-0 border-b-2 border-orange-400"></span><div></div></div></div><div class="col-span-1 flex flex-col md:col-span-3"><div class="grid grid-cols-4 gap-4 pb-4"><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href="https://www.mvig.org/"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27500%27%20height=%27500%27/%3e"/></span><img alt="Cewu Lu" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Cewu Lu" src="/_next/static/media/Cewu_Lu.2a4b87af.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="https://www.mvig.org/">Cewu Lu</a></div><div class="text-sm font-medium">Professor</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href="https://dirtyharrylyl.github.io/"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27586%27%20height=%27820%27/%3e"/></span><img alt="Yong-Lu Li" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Yong-Lu Li" src="/_next/static/media/Yong-Lu_Li.9b3f9166.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="https://dirtyharrylyl.github.io/">Yong-Lu Li</a></div><div class="text-sm font-medium">Assistant Professor</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href="https://foruck.github.io/"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27500%27%20height=%27500%27/%3e"/></span><img alt="Xinpeng Liu" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Xinpeng Liu" src="/_next/static/media/Xinpeng_Liu.48d03ebb.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="https://foruck.github.io/">Xinpeng Liu</a></div><div class="text-sm font-medium">PhD. Student</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href="https://silicx.github.io/"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27421%27%20height=%27526%27/%3e"/></span><img alt="Yue Xu" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Yue Xu" src="/_next/static/media/Yue_Xu.aff57491.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="https://silicx.github.io/">Yue Xu</a></div><div class="text-sm font-medium">PhD. Student</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href="https://scholar.google.com/citations?user=-PHR96oAAAAJ"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27433%27%20height=%27632%27/%3e"/></span><img alt="Xiaoqian Wu" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Xiaoqian Wu" src="/_next/static/media/Xiaoqian_Wu.394c81fd.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="https://scholar.google.com/citations?user=-PHR96oAAAAJ">Xiaoqian Wu</a></div><div class="text-sm font-medium">PhD. Student</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href="https://github.com/MayuOshima/"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27512%27%20height=%27512%27/%3e"/></span><img alt="Siqi Liu" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Siqi Liu" src="/_next/static/media/Siqi_Liu.0472512f.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="https://github.com/MayuOshima/">Siqi Liu</a></div><div class="text-sm font-medium">PhD. Student</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href="https://xuxinyu.website"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27437%27%20height=%27598%27/%3e"/></span><img alt="Xinyu Xu" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Xinyu Xu" src="/_next/static/media/Xinyu_Xu.d39c7b22.jpg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="https://xuxinyu.website">Xinyu Xu</a></div><div class="text-sm font-medium">Master Student</div></div></div></div><div><div class="flex flex-col bg-white divide-slate-100 divide-y-2"><div class="flex items-center justify-center aspect-[7/10] p-1"><a href=""><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27167%27%20height=%27167%27/%3e"/></span><img alt="Yusong Qiu" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Yusong Qiu" src="/_next/static/media/placeholder.5f16935b.png" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></a></div><div class="flex flex-col items-center justify-center gap-x-2"><div class="font-bold"><a href="">Yusong Qiu</a></div><div class="text-sm font-medium">Master Student</div></div></div></div></div><div class="py-4 gap-y-2"><h3 class="text-xl font-bold">Alumni:</h3><div><a class="font-bold underline" href="">Kaitong Cui</a>: <!-- -->HKU, Intern </div><div><a class="font-bold underline" href="https://www.junyi42.com/">Junyi Zhang</a>: <!-- -->UC Merced, Intern</div><div><a class="font-bold underline" href="http://github.com/Gennadiyev/">Yikun Ji</a>: </div><div><a class="font-bold underline" href="https://dou-yiming.github.io/">Yiming Dou</a>: <!-- -->UMich, Ph.D.</div><div><a class="font-bold underline" href="https://scholar.google.com/citations?user=-zT1NKwAAAAJ">Xiaohan Mao</a>: <!-- -->Shanghai AI Lab &amp; SJTU, Ph.D.</div><div><a class="font-bold">Zhemin Huang</a>: <!-- -->Stanford University, MS</div><div><a class="font-bold">Yuzhe Wu</a>: </div><div><a class="font-bold underline" href="https://coeusguo.github.io/">Shaopeng Guo</a>: <!-- -->UCSD, Ph.D.</div><div><a class="font-bold">Xudong Lu</a>: <!-- -->CUHK, Ph.D.</div><div><a class="font-bold">Hongwei Fan</a>: <!-- -->Sensetime, Research Engineer</div><div><a class="font-bold">Yuan Yao</a>: <!-- -->U of Rochester, Ph.D.</div><div><a class="font-bold">Zuoyu Qiu</a>: <!-- -->SJTU, MS</div><div><a class="font-bold">Junqi Liu</a>: </div><div><a class="font-bold">Han Lu</a>: <!-- -->SJTU, Ph.D.</div><div><a class="font-bold">Shiyi Wang</a>: </div><div><a class="font-bold">Zhanke Zhou</a>: <!-- -->HKBU, Ph.D.</div><div><a class="font-bold">Mingyang Chen</a>: <!-- -->UCSD, MS</div><div><a class="font-bold">Siyuan Zhou</a>: <!-- -->SJTU, MS</div><div><a class="font-bold underline" href="https://liangxuy.github.io/">Liang Xu</a>: <!-- -->EIAS &amp; SJTU, Ph.D.</div><div><a class="font-bold underline" href="https://maqingyang.github.io/">Ze Ma</a>: <!-- -->Columbia University, MS</div><div><a class="font-bold underline" href="https://huangowen.github.io/">Xijie Huang</a>: <!-- -->HKUST, Ph.D.</div></div></div></div></div></div></section><div class="relative bg-neutral-900 px-4 pb-6 pt-12 sm:px-8 sm:pt-14 sm:pb-8"><div class="absolute inset-x-0 -top-4 flex justify-center sm:-top-6"><a class="rounded-full bg-neutral-100 p-1 ring-white ring-offset-2 ring-offset-gray-700/80 focus:outline-none focus:ring-2 sm:p-2" href="/#hero"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="h-6 w-6 bg-transparent sm:h-8 sm:w-8"><path stroke-linecap="round" stroke-linejoin="round" d="M5 15l7-7 7 7"></path></svg></a></div><div class="flex flex-col items-center gap-y-6"><div id="pageview-script" class="text-sm text-neutral-700"></div><span class="text-sm text-neutral-700">© Copyright 2022 MVIG-RHOS • Based on<!-- --> <a href="https://github.com/tbakerx/react-resume-template">tbakerx</a></span></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{}},"page":"/","query":{},"buildId":"tQG3OeW69wWFNfhoMTK4O","nextExport":true,"autoExport":true,"isFallback":false,"scriptLoader":[]}</script></body></html>